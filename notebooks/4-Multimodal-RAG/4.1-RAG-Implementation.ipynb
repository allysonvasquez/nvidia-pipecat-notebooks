{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70308b81",
   "metadata": {},
   "source": [
    "# Module 4.2 - NVIDIA RAG Implementation\n",
    "\n",
    "## Module Introduction\n",
    "\n",
    "Welcome to Module 4.2 of the NVIDIA Digital Humans Teaching Kit! Building on the foundational understanding of RAG from Module 4.1, this notebook guides you through practical implementation steps. We will start by creating a simplified, CPU-friendly RAG pipeline to solidify the core concepts of retrieval and generation. Following this, we will dive into the specifics of integrating with the powerful NVIDIA RAG blueprint, leveraging the `NvidiaRAGService` from `nvidia-pipecat`'s framework to connect your digital human to a production-grade knowledge base. This module will also cover crucial aspects like real-time performance, context management in multi-turn conversations, and integration patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765ee89",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "Upon completing this notebook, you will be able to:\n",
    "\n",
    "*   Prepare and process data for ingestion into a knowledge base suitable for RAG.\n",
    "*   Implement a simplified, conceptual RAG pipeline to solidify core principles without GPU dependencies.\n",
    "*   Understand the process of data ingestion into the NVIDIA RAG blueprint.\n",
    "*   Detail the conceptual steps for querying the NVIDIA RAG blueprint.\n",
    "*   Explain the role and parameters of the `NvidiaRAGService` from `nvidia-pipecat` for connecting to a deployed RAG blueprint.\n",
    "*   Grasp how RAG contributes to multi-turn conversation context management within a digital human.\n",
    "*   Understand how real-time citation support enhances trust and transparency.\n",
    "*   Recognize key performance and latency considerations for RAG in conversational AI applications.\n",
    "*   Identify common integration patterns for RAG services within broader digital human frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09d698",
   "metadata": {},
   "source": [
    "## Required Prerequisites and Setup\n",
    "\n",
    "To get the most out of this notebook, ensure you have:\n",
    "\n",
    "*   **Python Proficiency:** Familiarity with Python programming, including object-oriented concepts and common data structures.\n",
    "*   **Jupyter Notebooks / VS Code Experience:** Comfort with navigating and executing code within a Jupyter environment.\n",
    "*   **Foundational AI Knowledge:** Basic understanding of LLMs, embeddings, and intelligent systems.\n",
    "*   **Module 0-4.1 Completion:** A stable environment set up from Module 0, and a grasp of the digital human pipeline, speech services, dialogue management, and fundamental RAG concepts covered in Modules 1, 2, 3, and 4.0.\n",
    "\n",
    "### Module-Specific Setup\n",
    "\n",
    "This module will primarily involve understanding and interacting with components relevant to the NVIDIA RAG blueprint.\n",
    "\n",
    "1.  **NVIDIA RAG Blueprint Access:** The core RAG implementation discussed will refer to the [NVIDIA-AI-Blueprints/rag](https://github.com/NVIDIA-AI-Blueprints/rag) repository. While we won't fully deploy it within this notebook, understanding its structure is key. We expect you to deploy this and run through the workflow in parallel with this module.\n",
    "    *   **RA Note:** Provide detailed markdown instructions here for students to *clone* the `NVIDIA-AI-Blueprints/rag` repository locally and outline any initial setup steps (`conda env create`, `pip install -r requirements.txt`) they would need to get familiar with it. Emphasize that the full blueprint typically requires GPU resources, which might not be covered by standard local setups.\n",
    "2.  **Required Python Libraries:** We will use standard libraries for text processing (`scikit-learn` for TF-IDF) and potentially a basic `transformers` installation (for a conceptual RAG example that can run on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ba7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block Placeholder: Install necessary libraries\n",
    "# RA Note: Ensure all dependencies for the *dummy* RAG example are covered in the setup instructions above (scikit-learn, nltk, transformers).\n",
    "# Example:\n",
    "# !pip install -q scikit-learn nltk transformers\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681bdf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Preparation for RAG\n",
    "\n",
    "The effectiveness of any RAG system heavily relies on the quality and structure of its underlying knowledge base. This section covers the crucial steps in preparing your data for optimal retrieval and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febcfe7b",
   "metadata": {},
   "source": [
    "### 1.1 Document Collection and Types\n",
    "\n",
    "Your RAG knowledge base can consist of a wide variety of document types, forming the authoritative source for your digital human's factual responses. Common sources include:\n",
    "\n",
    "*   **Text files:** `.txt`, `.md`, `.docx`, `.jsonl`\n",
    "*   **PDFs:** Manuals, reports, research papers\n",
    "*   **Web pages:** `.html` content from corporate websites, wikis, or product pages\n",
    "*   **Structured data:** Databases, APIs, or structured JSON/XML files\n",
    "\n",
    "**[RA Note: Expand this section with markdown by suggesting specific examples of data sources that would be highly relevant for typical digital human applications (customer support agent manuals, product FAQs, company policy documents, historical conversation logs, technical specifications). Discuss how different data sources might require different initial preprocessing steps.]**\n",
    "\n",
    "### 1.2 Text Extraction and Cleaning\n",
    "\n",
    "Before documents can be processed for RAG, their raw content needs to be extracted and cleaned. This often involves:\n",
    "\n",
    "*   **Parsing:** Handling different file formats (extracting text from a PDF, parsing HTML).\n",
    "*   **Layout Analysis:** For complex documents, understanding the structure (identifying headings, paragraphs, tables) to preserve context.\n",
    "*   **Noise Removal:** Removing boilerplate content like headers, footers, page numbers, or irrelevant navigation elements.\n",
    "*   **Formatting Cleanup:** Normalizing whitespace, removing special characters, and correcting encoding issues.\n",
    "\n",
    "### 1.3 Document Chunking\n",
    "\n",
    "Chunking is the process of splitting large documents into smaller, semantically meaningful segments. This is a vital step because:\n",
    "\n",
    "*   **LLM Context Window Limits:** Large Language Models have a limited input token capacity (their \"context window\"). Individual document sections need to fit within this limit when passed to the LLM.\n",
    "*   **Relevance:** Smaller, focused chunks are more likely to contain highly relevant information without diluting the query with unnecessary surrounding context.\n",
    "*   **Efficiency:** Searching and retrieving smaller chunks is significantly faster in a vector database.\n",
    "\n",
    "Common chunking strategies include:\n",
    "\n",
    "*   **Fixed-size chunks:** Splitting documents into segments of a predetermined token or character count.\n",
    "*   **Sentence-based chunks:** Dividing documents at sentence boundaries.\n",
    "*   **Paragraph/Section-based chunks:** Splitting at logical breaks like paragraphs, headings, or distinct sections, often preserving semantic coherence.\n",
    "*   **Overlapping chunks:** Adding a small overlap between consecutive chunks to ensure continuity of context, especially important when a relevant answer spans a chunk boundary.\n",
    "\n",
    "**[RA Note: Provide concrete markdown examples of different chunking strategies with a sample long text. Discuss the trade-offs of different chunk sizes and how they impact retrieval accuracy and LLM comprehension. Include suggestions for when to use each strategy (fixed-size for very long, unstructured text; semantic for well-structured documents).]**\n",
    "\n",
    "### 1.4 Embedding Models\n",
    "\n",
    "Once chunked, each text chunk is converted into a numerical vector, known as an **embedding**, using an embedding model. These embeddings capture the semantic meaning of the text, such that chunks with similar meanings are represented by vectors that are \"close\" to each other in a multi-dimensional vector space. User queries are also converted into embeddings.\n",
    "\n",
    "The retrieval process then involves finding document chunks whose embeddings are most \"similar\" (measured by cosine similarity) to the query embedding. Different types of embedding models exist:\n",
    "\n",
    "*   **Dense Embeddings:** Generated by deep neural networks (Sentence Transformers). They capture nuanced semantic relationships and are very effective for finding conceptually similar information.\n",
    "*   **Sparse Embeddings:** Based on lexical overlap (TF-IDF, BM25). They are good for keyword matching and can complement dense retrievers in hybrid systems.\n",
    "\n",
    "The choice of embedding model heavily influences the quality of retrieval and, consequently, the RAG system's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436388c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Implementing a Simple Non-GPU RAG (Dummy Example)\n",
    "\n",
    "To solidify your understanding of the RAG workflow, we will now build a very basic, CPU-friendly RAG system from scratch. This example will illustrate the core concepts of retrieval and generation without requiring a complex setup or GPU resources. This is a pedagogical tool to understand the flow, not a production-ready solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16673c",
   "metadata": {},
   "source": [
    "### 2.1 Define a Knowledge Base\n",
    "\n",
    "These documents will serve as a miniature, in-memory knowledge base for our simple Q&A system. Imagine these are small snippets of information about NVIDIA products or AI concepts that our digital human needs to access for providing accurate responses. The diversity of topics here will allow us to test the retrieval mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7875871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block: Dummy Knowledge Base\n",
    "documents = [\n",
    "    \"NVIDIA is a technology company known for designing graphics processing units (GPUs).\",\n",
    "    \"GPUs are essential for accelerating AI workloads, including deep learning and scientific computing.\",\n",
    "    \"CUDA is a parallel computing platform and programming model developed by NVIDIA for general purpose computing on GPUs.\",\n",
    "    \"RAG stands for Retrieval-Augmented Generation, a technique that enhances LLMs with external knowledge.\",\n",
    "    \"Digital humans are interactive AI characters that can communicate via speech, animation, and sometimes even visual perception.\",\n",
    "    \"The NVIDIA Digital Humans Teaching Kit provides comprehensive modules on building and deploying realistic digital humans using various NVIDIA technologies.\",\n",
    "    \"NVIDIA Riva is a GPU-accelerated SDK for building real-time AI applications, including speech AI services like ASR and TTS.\",\n",
    "    \"NVIDIA NIM (NVIDIA Inference Microservices) provides optimized AI models as microservices for easy deployment.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8bda4",
   "metadata": {},
   "source": [
    "### 2.2 Simple Retrieval Mechanism\n",
    "\n",
    "For this dummy example, we'll use a very basic keyword-based or TF-IDF (Term Frequency-Inverse Document Frequency) approach instead of complex, computationally intensive dense embeddings. TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This helps to illustrate the core concept of finding relevant information based on lexical similarity without requiring heavy computational resources.\n",
    "\n",
    "The `retrieve_documents` function will take a user query and our defined knowledge base, then return the `top_k` most relevant documents by calculating a similarity score between the query and each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f97243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block: Simple Retrieval\n",
    "# Example implementation notes (code to be added by Allyson):\n",
    "\n",
    "#\n",
    "# def retrieve_documents(query, documents, top_k=2):\n",
    "#     # 1. Initialize TF-IDF Vectorizer and fit to documents\n",
    "#     # vectorizer = TfidfVectorizer().fit(documents)\n",
    "#\n",
    "#     # 2. Transform documents and query into TF-IDF vectors\n",
    "#     # doc_vectors = vectorizer.transform(documents)\n",
    "#\n",
    "#     # 3. Calculate cosine similarity between query and document vectors\n",
    "#     # similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "#\n",
    "#     # 4. Get indices of top_k most similar documents\n",
    "#     # top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "#\n",
    "#     # 5. Return the actual document texts\n",
    "#     # return [documents[i] for i in top_indices]\n",
    "#     pass # Technical Lead will implement\n",
    "\n",
    "# Example usage (uncomment and run after implementation):\n",
    "# query = \"What is RAG?\"\n",
    "# retrieved_docs = retrieve_documents(query, documents)\n",
    "# print(f\"Retrieved documents: {retrieved_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe964222",
   "metadata": {},
   "source": [
    "### 2.3 Dummy Generation Logic\n",
    "\n",
    "In a real RAG system, the retrieved documents and the original query would be sent as a combined prompt to an actual Large Language Model (LLM) (via NIM). For our dummy example, we'll simply concatenate the retrieved information with the user's query into a structured string. This simulates the LLM's input and its subsequent role in synthesizing an answer from the provided context.\n",
    "\n",
    "The `generate_response` function will create this combined input, allowing us to visualize what a real LLM would receive and use for its response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3848f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block: Dummy Generation\n",
    "# Example implementation notes (code to be added by Technical Lead):\n",
    "# def generate_response(query, retrieved_docs):\n",
    "#     # Combine retrieved documents into a single context string\n",
    "#     context = \"\\n\".join(retrieved_docs)\n",
    "#\n",
    "#     # This structured string would be the prompt sent to a real LLM API\n",
    "#     prompt_for_llm = f\"\"\"Based on the following information:\n",
    "# {context}\n",
    "#\n",
    "# Answer the question: {query}\n",
    "# \"\"\"\n",
    "#     # For this dummy example, we simulate the LLM's response\n",
    "#     return f\"**Mock LLM Response (based on retrieved context):** I have processed your query based on the following context:\\n'{context}'\\n\\nMy answer to '{query}' would be formulated using this information.\"\n",
    "#\n",
    "# # Example usage (uncomment and run after implementation):\n",
    "# # query = \"What is RAG?\"\n",
    "# # response = generate_response(query, retrieved_docs)\n",
    "# # print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d121a7",
   "metadata": {},
   "source": [
    "### 2.4 Putting It All Together: Simple RAG Pipeline\n",
    "\n",
    "Let's combine our retrieval and dummy generation components into a complete, albeit simplified, RAG pipeline. This will demonstrate the end-to-end flow from a user query, through retrieval, to a contextualized (mock) response. This provides a clear picture of how RAG operates at a fundamental level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block: Simple RAG Pipeline Execution\n",
    "# Example implementation notes (code to be added by Technical Lead):\n",
    "# def run_simple_rag(query, knowledge_base_docs):\n",
    "#     print(f\"Processing query: '{query}'\")\n",
    "#     retrieved = retrieve_documents(query, knowledge_base_docs) # Call the retrieval function\n",
    "#     print(f\"Retrieved documents: {retrieved}\")\n",
    "#     response = generate_response(query, retrieved) # Call the generation function\n",
    "#     return response\n",
    "#\n",
    "# print(\"\\n--- Testing Simple RAG Pipeline ---\")\n",
    "# test_query_1 = \"What is NVIDIA known for?\"\n",
    "# # print(run_simple_rag(test_query_1, documents)) # Uncomment and run after implementation\n",
    "#\n",
    "# print(\"\\n---\")\n",
    "# test_query_2 = \"What does CUDA do?\"\n",
    "# # print(run_simple_rag(test_query_2, documents)) # Uncomment and run after implementation\n",
    "#\n",
    "# # Add more test queries here to showcase various scenarios, including queries that might not have direct answers\n",
    "# # in the dummy knowledge base, to illustrate limitations and the effect of grounding.\n",
    "# print(\"\\n---\")\n",
    "# test_query_3 = \"Tell me about NVIDIA Riva.\"\n",
    "# # print(run_simple_rag(test_query_3, documents)) # Uncomment and run after implementation\n",
    "#\n",
    "# print(\"\\n---\")\n",
    "# test_query_4 = \"Who invented the telephone?\" # Query not directly in knowledge base\n",
    "# # print(run_simple_rag(test_query_4, documents)) # This should ideally show that it lacks information on non-NVIDIA topics, demonstrating the grounding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea70931",
   "metadata": {},
   "source": [
    "**Reflection Question:** How does this simple RAG demonstrate the core concept of grounding an LLM's response, even without a real LLM, and what are its inherent limitations compared to a full production system?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058c51e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Integrating with the NVIDIA RAG Blueprint\n",
    "\n",
    "While our dummy example illustrates the RAG principles, the NVIDIA RAG blueprint provides a production-ready, scalable, and highly optimized solution. This section focuses on how you would conceptually interact with and leverage its capabilities, particularly within the `nvidia-pipecat` framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d4698",
   "metadata": {},
   "source": [
    "### 3.1 Setting Up the NVIDIA RAG Blueprint\n",
    "\n",
    "As discussed in Module 4.0, the NVIDIA RAG blueprint is typically deployed as a set of interconnected microservices (using Docker Compose for local development or Kubernetes for production). To utilize it, you would follow the detailed instructions provided in the [NVIDIA-AI-Blueprints/rag](https://github.com/NVIDIA-AI-Blueprints/rag) repository's README. This involves setting up the necessary infrastructure, which includes a vector database, ingestion service, and query service.\n",
    "\n",
    "**[RA Note: Provide a high-level conceptual markdown walk-through of how one would typically set up and run the NVIDIA RAG blueprint locally (mention `docker compose up` or `kubectl apply` commands as found in the blueprint's README). Emphasize that full deployment and GPU requirements are outside the scope of *this* notebook's runnable code, but understanding the steps to get the blueprint running is important for real-world application. Include a link to the relevant setup section in the blueprint's README.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be026b",
   "metadata": {},
   "source": [
    "### 3.2 Ingesting Data into the Blueprint\n",
    "\n",
    "Once the NVIDIA RAG blueprint services are deployed, the next step is to populate its knowledge base. The blueprint provides an API for ingesting your prepared documents. This process involves sending your text chunks (and potentially associated metadata) to the ingestion service. The ingestion service then handles the heavy lifting of generating embeddings using a high-quality model and storing them efficiently in the configured vector database. This is a critical step to build your digital human's specialized and up-to-date knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block Placeholder: Conceptual RAG Ingestion (Python client interaction)\n",
    "# RA Note: Provide conceptual Python code snippets (as comments) demonstrating how one would\n",
    "# make an API call to the RAG blueprint's ingestion service. Focus on the structure of the request\n",
    "# payload (document ID, text content, optional metadata).\n",
    "# This should *not* be runnable if the blueprint isn't set up, but demonstrate the expected client interaction.\n",
    "# Example:\n",
    "# import requests\n",
    "#\n",
    "# def ingest_document_to_rag(doc_id: str, text_content: str, rag_ingestion_url: str = \"http://localhost:8000/ingest\"):\n",
    "#     payload = {\"id\": doc_id, \"text\": text_content}\n",
    "#     try:\n",
    "#         response = requests.post(rag_ingestion_url, json=payload)\n",
    "#         response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "#         print(f\"Document {doc_id} ingested successfully: {response.json()}\")\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error ingesting document {doc_id}: {e}\")\n",
    "#\n",
    "# # Dummy usage (won't run without the service):\n",
    "# # ingest_document_to_rag(\"doc_1\", \"This is a sample document about NVIDIA's latest AI advancements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3347f0",
   "metadata": {},
   "source": [
    "### 3.3 Querying the RAG Blueprint and `nvidia-pipecat` Integration\n",
    "\n",
    "Once data is ingested into the blueprint, your digital human can query the RAG system to retrieve relevant information and generate answers. This involves sending a user query to the blueprint's query service, which then orchestrates the retrieval and LLM generation based on the ingested knowledge base.\n",
    "\n",
    "**Seamless Integration with NVIDIA ACE Controller (`nvidia-pipecat`):**\n",
    "\n",
    "For `nvidia-pipecat` based digital human applications, the `NvidiaRAGService` (`nvidia_rag.py` in the ACE Controller codebase) provides a direct and production-ready integration point for the NVIDIA RAG blueprint. This service extends standard LLM services within `pipecat`, allowing RAG capabilities to be seamlessly incorporated into your digital human pipeline without complex custom code.\n",
    "\n",
    "The `NvidiaRAGService` requires a deployed NVIDIA RAG server (your blueprint endpoint) and exposes several configurable parameters that are crucial for fine-tuning the retrieval and generation process. Understanding these parameters is key to optimizing your digital human's responses:\n",
    "\n",
    "*   `collection_name`: Specifies the name of the document collection within your vector database that the RAG service should query.\n",
    "*   `rag_server_url`: The URL of your deployed NVIDIA RAG blueprint's query endpoint (`http://localhost:8081`).\n",
    "*   `vdb_top_k`: (Vector Database Top-K) The number of top relevant document chunks to retrieve from the vector database. A higher value retrieves more candidates, but may increase processing time.\n",
    "*   `reranker_top_k`: (Reranker Top-K) From the `vdb_top_k` results, this parameter specifies how many should be further re-ranked for even higher relevance before being passed to the LLM. Typically `reranker_top_k` <= `vdb_top_k`.\n",
    "*   `enable_citations`: A boolean flag to enable or disable the inclusion of source citations in the LLM's response. Essential for transparency.\n",
    "*   `temperature`: A parameter for the LLM that controls the randomness of the generated responses. Lower temperatures (0.2) result in more deterministic and factual responses, ideal for RAG applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee0aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block Placeholder: Conceptual API Call to NVIDIA RAG Blueprint and `NvidiaRAGService` instantiation\n",
    "# RA Note: Provide conceptual Python code snippets (as comments) demonstrating how one would\n",
    "# make a direct API call to the RAG blueprint's query service, and then show how `NvidiaRAGService`\n",
    "# would be instantiated within a pipecat application context. This helps bridge the gap between\n",
    "# conceptual API calls and pipecat integration.\n",
    "# This code should *not* be runnable if the blueprint isn't set up.\n",
    "\n",
    "# Example Conceptual API Call to RAG Blueprint (direct HTTP request):\n",
    "# import requests\n",
    "#\n",
    "# def query_rag_blueprint(user_query: str, rag_query_url: str = \"http://localhost:8000/query\"):\n",
    "#     payload = {\"query\": user_query}\n",
    "#     try:\n",
    "#         response = requests.post(rag_query_url, json=payload)\n",
    "#         response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "#         result = response.json()\n",
    "#         print(f\"RAG Generated Answer: {result.get('answer', 'No answer received.')}\")\n",
    "#         print(f\"Retrieved Sources: {result.get('sources', 'No sources found.')}\")\n",
    "#         return result\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error querying RAG blueprint: {e}\")\n",
    "#         return None\n",
    "#\n",
    "# # Dummy usage (won't run without the service):\n",
    "# # query_rag_blueprint(\"What are the key benefits of using NVIDIA GPUs for AI?\")\n",
    "\n",
    "# Real-world RAG service instantiation (from ACE Controller's `nvidia_rag.py`):\n",
    "# This class would be imported and used within a pipecat pipeline definition.\n",
    "# from pipecat.services.nvidia_rag import NvidiaRAGService\n",
    "\n",
    "# rag_service = NvidiaRAGService(\n",
    "#     collection_name=\"your_knowledge_base\",\n",
    "#     rag_server_url=\"http://localhost:8081\",  # URL of your deployed RAG blueprint endpoint\n",
    "#     vdb_top_k=20,  # Retrieve top 20 chunks from vector database for initial candidates\n",
    "#     reranker_top_k=4,  # Re-rank these 20 to select the top 4 most relevant for the LLM\n",
    "#     enable_citations=True,  # Set to True to receive source citations in the response\n",
    "#     temperature=0.2  # Set to a low temperature for more factual and less creative LLM responses\n",
    "# )\n",
    "\n",
    "# RA Note: Add a markdown explanation for configurable RAG parameters like `vdb_top_k`, `reranker_top_k`, `enable_citations`, and `temperature`. Explain how adjusting these parameters can impact the relevance, quality, and style of the digital human's responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118d001",
   "metadata": {},
   "source": [
    "### 3.4 Hands-on Lab: Interacting with a Deployed RAG Blueprint\n",
    "\n",
    "**[RA Note: Develop a detailed hands-on lab section in markdown here. This section should assume the student has successfully set up the NVIDIA RAG blueprint (using Docker Compose as per setup instructions provided in Module 4.0 or this module's setup). The lab should guide them through conceptual steps or provide instructions for external execution, covering:]**\n",
    "\n",
    "1.  **Ingesting sample documents:** Provide a small set of conceptual documents (outline content for `.txt` files related to a specific domain, like NVIDIA products or AI topics) for them to ingest using the blueprint's provided client or API. Emphasize the *process* of ingestion and the expected outcome (documents vectorized and stored) rather than requiring runnable code within this notebook itself.\n",
    "\n",
    "2.  **Querying the RAG system:** Guide them to run several queries against their ingested knowledge base and observe the responses, including the retrieved source passages. Discuss what kind of output to expect from a successful RAG query.\n",
    "\n",
    "3.  **Experimenting with queries:** Encourage them to ask questions that directly require knowledge from their ingested documents, as well as general knowledge questions or questions outside the scope of the knowledge base. This will help them observe the RAG's behavior and the impact of grounding.\n",
    "\n",
    "4.  **Try This!** Include a \"Try This!\" section with suggestions for further exploration (experiment with ingesting documents on different topics and observing how the RAG response changes; try queries designed to test guardrails or limitations if such features are enabled in the blueprint; observe how changing `vdb_top_k` or `reranker_top_k` might affect retrieval results if they can access direct API calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d2330",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Advanced RAG Considerations for Digital Humans\n",
    "\n",
    "Beyond the basic integration, several advanced considerations are crucial for deploying robust and highly responsive digital human agents that leverage RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223eab32",
   "metadata": {},
   "source": [
    "### 4.1 Context Management for Multi-Turn Conversations\n",
    "\n",
    "Digital humans engaging in natural, multi-turn conversations require robust context management. RAG plays a vital role here by enabling the system to retrieve information relevant not just to the current query, but also to the ongoing dialogue. The NVIDIA RAG blueprint, designed for conversational AI, supports maintaining conversational context across multiple turns. This allows the digital human to answer follow-up questions accurately and coherently, building upon previous statements or shared context.\n",
    "\n",
    "This is typically achieved by feeding previous conversational turns (or a summary thereof) as part of the query to the RAG system, ensuring that relevant historical information influences both the retrieval and generation processes. This creates a more seamless and intelligent conversational experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed62e1",
   "metadata": {},
   "source": [
    "### 4.2 Real-time Citation Support\n",
    "\n",
    "A critical aspect of trustworthy AI, especially in factual domains, is the ability to provide verifiable sources for generated information. The `NvidiaRAGService` within the ACE Controller automatically processes citation data returned by the NVIDIA RAG server. It creates `NvidiaRAGCitation` objects (`nvidia_rag.py:17-34`), which contain metadata about the retrieved source documents or specific passages that informed the LLM's response.\n",
    "\n",
    "This feature allows the digital human to present these citations to the user, either directly in the spoken response or via a visual interface, enhancing transparency, accountability, and user trust. For example, a digital human might say, \"According to the product manual on page 5... [CITATION 1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e40041",
   "metadata": {},
   "source": [
    "### 4.3 Performance and Latency Considerations for RAG\n",
    "\n",
    "For truly responsive and natural real-time digital human interactions, optimizing RAG system performance and minimizing latency are crucial. This involves not only the efficient underlying RAG architecture (as provided by NVIDIA's blueprint) but also careful considerations in data preparation, model selection, and deployment strategies. Factors significantly influencing end-to-end latency include:\n",
    "\n",
    "*   **Retrieval Speed:** How quickly the vector database can return relevant chunks given a query embedding.\n",
    "*   **Reranking Efficiency:** The speed of the reranker model in refining the initial retrieved candidates.\n",
    "*   **LLM Inference Time:** How fast the LLM can generate a response given the augmented prompt, which includes the retrieved context.\n",
    "*   **Network Latency:** The communication time between different microservices (between the `nvidia-pipecat` agent, the RAG query service, and the LLM service).\n",
    "\n",
    "Designing for low-latency means minimizing each of these steps, leveraging techniques like efficient indexing (for the vector database), model quantization (for smaller, faster models), and optimized hardware (like NVIDIA GPUs).\n",
    "\n",
    "### 4.4 Integration Patterns for External Systems\n",
    "\n",
    "Integrating RAG APIs with external systems is vital for building comprehensive and extensible digital human frameworks. This often involves standard software architecture patterns:\n",
    "\n",
    "*   **RESTful APIs:** The NVIDIA RAG blueprint exposes standard HTTP/REST endpoints for both data ingestion and querying, making it accessible from virtually any application or service regardless of programming language.\n",
    "*   **Microservices Architecture:** As seen, the RAG blueprint itself is a microservices application. This modularity allows RAG components to be deployed independently, enabling flexible scaling of each service based on demand and facilitating independent development and updates.\n",
    "*   **Queueing Systems:** For asynchronous ingestion of large datasets or handling high volumes of batch updates to the knowledge base, integrating with message queues (Kafka, RabbitMQ) ensures robustness, prevents data loss, and enables eventual consistency.\n",
    "*   **Dynamic Configuration:** The `NvidiaRAGService` in `nvidia-pipecat` allows for dynamic updates to RAG settings (`vdb_top_k`, `reranker_top_k`, etc.) at runtime (`nvidia_rag.py:12-14`). This enables real-time adjustment of retrieval parameters without redeploying the entire service, crucial for adaptive digital human behavior based on changing conversational needs or performance requirements.\n",
    "\n",
    "These integration patterns ensure that the RAG capabilities can be seamlessly embedded into diverse digital human platforms and applications, from web-based interfaces to game engines or custom conversational agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbc015",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've delved into the practicalities of Retrieval-Augmented Generation. You've implemented a simple RAG pipeline to understand its core mechanics and gained a deep understanding of how to conceptually integrate with NVIDIA's robust RAG blueprint using the `NvidiaRAGService` within `nvidia-pipecat`. Furthermore, you've explored crucial advanced topics such as real-time performance, multimodal document understanding, multi-turn context management, and citation support.\n",
    "\n",
    "RAG fundamentally empowers your digital human to act as a reliable and knowledgeable source of information, drawing from a dynamically updateable knowledge base rather than relying solely on static, pre-trained LLM data.\n",
    "\n",
    "### Reflection Exercises\n",
    "\n",
    "*   Consider a real-world application where RAG would be critical for a digital human (a medical assistant, a legal advisor). What kind of knowledge base would it need? How would real-time performance and multi-turn context impact its effectiveness in such a sensitive domain?\n",
    "*   How does the concept of \"chunking\" influence the quality of retrieved information? What are the challenges, especially when dealing with complex, visually rich documents (which we'll touch on more in the next module)?\n",
    "*   Research how dynamic RAG parameters (like `top_k` values or reranker thresholds) could be exposed and tuned in a live digital human application based on user feedback or conversation state.\n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "Proceed to **Module 4.2 – Multimodal LLM Integration** to explore how the NVIDIA RAG blueprint's document understanding capabilities can be extended to process and utilize information from various modalities, such as images, tables, and charts, to further enrich your digital human's knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf1c6d-b451-41d0-b517-ce67c4adb2f7",
   "metadata": {},
   "source": [
    "TODO --- turn discussion now towards Tokkio being needed for multimodal integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfa710-0364-4af1-8833-b13dc5ba75b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
