{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "module-title-welcome",
   "metadata": {},
   "source": [
    "# Module 3.3: Retrieval-Augmented Generation (RAG) for Knowledgeable Digital Humans\n",
    "\n",
    "In previous modules, you've mastered the fundamentals of LLM integration and refined digital human behavior through prompt engineering (Module 3.1), and established safety boundaries with various guardrails (Module 3.2). While powerful, even the most advanced LLMs have limitations: their knowledge is static (limited to their training data) and they can sometimes hallucinate and provide incorrect information.\n",
    "\n",
    "This module introduces **Retrieval-Augmented Generation (RAG)**, a transformative technique that empowers your digital human agent to access, interpret, and cite up-to-date, factual information from external knowledge bases. We'll leverage NVIDIA's RAG blueprint and `nvidia-pipecat` to see how `NvidiaRAGService` integrates seamlessly within the Pipecat framework.\n",
    "\n",
    "## Learning Objectives\n",
    "- Explain the core concepts of Retrieval-Augmented Generation (RAG) and its importance for digital human development.\n",
    "- Understand the three main stages of a RAG pipeline: retrieval, ranking, and generation.\n",
    "- Identify how `NvidiaRAGService` integrates into the `nvidia-pipecat` architecture for knowledge retrieval.\n",
    "- Conceptualize the flow of data and context injection within a RAG-enabled digital human pipeline.\n",
    "- Explore practical use cases for RAG in domain-specific applications like a museum guide.\n",
    "- Discuss how RAG complements LLM authoring and guardrails for a multi-layered, robust AI system.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "code-imports-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA API key loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import getpass\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from pipecat.frames.frames import Frame, TextFrame, EndFrame, StartFrame, TranscriptionFrame, TTSSpeakFrame\n",
    "from pipecat.observers.base_observer import BaseObserver\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineTask, PipelineParams, FrameDirection\n",
    "from pipecat.processors.frame_processor import FrameProcessor\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "# NVIDIA specific services from nvidia-pipecat\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService # Conceptually used\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # For running asyncio in Jupyter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"NVIDIA API key not found or invalid in .env file.\")\n",
    "    api_key = getpass.getpass(\"ðŸ” Enter your NVIDIA API key: \").strip()\n",
    "    assert api_key.startswith(\"nvapi-\"), f\"{api_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = api_key\n",
    "else:\n",
    "    print(\"NVIDIA API key loaded from .env file.\")\n",
    "\n",
    "# Initialize a dummy OpenAI client for demonstration (for direct NIM calls if needed)\n",
    "nim_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ.get(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "class SimpleChatObserver(BaseObserver):\n",
    "    \"\"\"A simple observer to print streamed responses from LLM-like services.\"\"\"\n",
    "    async def on_push_frame(self, src: FrameProcessor, dst: FrameProcessor, frame: Frame, direction: FrameDirection, timestamp: int):\n",
    "        if isinstance(frame, TextFrame):\n",
    "            print(frame.text, end=\"\", flush=True)\n",
    "        elif isinstance(frame, EndFrame):\n",
    "            print() # Newline after response completes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-rag",
   "metadata": {},
   "source": [
    "# Introduction: What is RAG and Why It Matters for Digital Humans\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge during the generation process. In the context of building digital humans using NVIDIA's ACE Controller framework, RAG serves as a critical component that bridges the gap between general language understanding and domain-specific expertise.\n",
    "\n",
    "For your digital human, such as a museum guide agent, RAG transforms a general-purpose conversational AI into a knowledgeable curator. While your `NvidiaLLMService` provides natural language capabilities and your `GuardrailProcessor` ensures safe interactions, RAG adds the crucial ability to access and incorporate factual information from your database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-rag-works",
   "metadata": {},
   "source": [
    "## How RAG Works: The Three-Stage Pipeline\n",
    "\n",
    "The RAG process can be broken down into three main stages, which work in concert to deliver accurate and relevant responses:\n",
    "\n",
    "### 1. Retrieval Stage\n",
    "When a user asks a question, for instance, \"Tell me about the Renaissance exhibit in Gallery 2?\" the RAG system first searches through your configured document collection to find relevant information. The `NvidiaRAGService` (part of the ACE Controller SDK) typically uses a vector database (VDB) approach where documents are converted into numerical representations called embeddings. The user's query is also converted into an embedding, and then semantically similar (relevant) content is retrieved from the VDB.\n",
    "\n",
    "The retrieval process is controlled by parameters like `vdb_top_k`, which determines how many top-ranked document chunks to initially retrieve from the knowledge base.\n",
    "\n",
    "### 2. Ranking/Reranking Stage\n",
    "Not all retrieved documents are equally relevant or may contain redundant information. To optimize the quality of the context provided to the LLM, the system uses a reranker. This component scores and prioritizes the most pertinent information from the initial retrieval set. This stage is controlled by the `reranker_top_k` parameter, which typically selects a smaller number of higher-quality chunks from the initial `vdb_top_k` results.\n",
    "\n",
    "### 3. Generation with Context Injection\n",
    "The selected, high-quality documents from the reranking stage are then injected into the LLM prompt as context. The LLM receives this augmented prompt, which now contains the user's original question plus the retrieved factual information. The LLM then generates a response that incorporates this retrieved knowledge, creating answers that are both conversational and factually grounded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-integration-architecture",
   "metadata": {},
   "source": [
    "## RAG Integration in the `nvidia-pipecat` Architecture\n",
    "\n",
    "The `NvidiaRAGService` seamlessly integrates into the modular pipeline architecture you're familiar with from previous modules. It extends pipecats `OpenAILLMService` or works alongside it, meaning it processes similar context frames and maintains compatibility with your existing pipeline components.\n",
    "\n",
    "The service communicates with a separate RAG server that handles the heavy lifting of document indexing and retrieval. This RAG server typically follows architectures defined by NVIDIA's Generative AI Examples (like the [NVIDIA RAG Blueprint](https://github.com/NVIDIA-AI-Blueprints/rag)), which can be deployed as NVIDIA NIMs or on your infrastructure.\n",
    "\n",
    "### Frame Processing Flow (Conceptual)\n",
    "Here's a conceptual flow demonstrating where RAG fits into the digital human pipeline:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"User Query (Voice/Text)\"] --> B[\"ASR (TranscriptionFrame)\"]\n",
    "    B --> C[\"GuardRailProcessor (Keyword/Semantic)\"]\n",
    "    C --> D[\"OpenAILLMContext (Manage History)\"]\n",
    "    D -- User Question --> E[\"NvidiaRAGService (Retrieval/Ranking)\"]\n",
    "    E -- Query to RAG Server --> F[\"RAG Server (Vector DB, Documents)\"]\n",
    "    F --> E -- Retrieved Chunks --> G[\"LLM (NvidiaLLMService) with Augmented Context\"]\n",
    "    G --> H[\"TTS (TTSSpeakFrame)\"]\n",
    "    H --> I[\"Animation (Avatar Motion)\"]\n",
    "    I --> J[\"Digital Human Response\"]\n",
    "```\n",
    "\n",
    "The RAG service processes `OpenAILLMContextFrame` objects, extracts the conversation history, and enriches the context with retrieved documents before sending it to the LLM for final generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-ragservice",
   "metadata": {},
   "source": [
    "## Implementation: Setting Up `NvidiaRAGService` (Conceptual)\n",
    "\n",
    "While we cannot run a full RAG server deployment within this notebook, understanding how `NvidiaRAGService` is configured and used conceptually is vital. This service simplifies interaction with a deployed RAG backend.\n",
    "\n",
    "### Basic Configuration\n",
    "\n",
    "The `NvidiaRAGService` requires several key parameters for our museum guide application:\n",
    "\n",
    "-   `collection_name`: Identifies your specific museum's document collection within the RAG server (\"modern_art_exhibits\").\n",
    "-   `rag_server_url`: Points to your deployed RAG server endpoint ( a local URL if self-hosted, or an NVIDIA cloud service endpoint).\n",
    "-   `use_knowledge_base`: A boolean flag to enable or disable RAG functionality.\n",
    "-   `enable_citations`: A boolean to return source references (citations) with responses.\n",
    "\n",
    "```python\n",
    "# Conceptual instantiation of NvidiaRAGService\n",
    "# (Requires a running RAG server, which is external to this notebook)\n",
    "# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService\n",
    "\n",
    "# rag_service = NvidiaRAGService(\n",
    "#     collection_name=\"modern_art_museum_collection\",\n",
    "#     rag_server_url=\"http://your-rag-server-ip:port\", # Replace with your RAG server's actual URL\n",
    "#     use_knowledge_base=True,\n",
    "#     enable_citations=True,\n",
    "#     vdb_top_k=5,        # Retrieve top 5 documents\n",
    "#     reranker_top_k=2    # Re-rank and select top 2 most relevant\n",
    "# )\n",
    "```\n",
    "\n",
    "### Dynamic Configuration\n",
    "\n",
    "The service supports runtime configuration changes through specialized frames like `NvidiaRAGSettingsFrame`. This allows you to dynamically adjust parametersâ€”for instance, switching between different museum collections ( \"ancient_history\" vs. \"renaissance_art\") or modifying retrieval settings based on the user's current context or explicit preferences.\n",
    "\n",
    "### Citation Handling\n",
    "\n",
    "One of RAG's most valuable features for museum applications is citation support. The service can return `NvidiaRAGCitationsFrame` objects containing detailed source information.\n",
    "\n",
    "Each citation typically includes:\n",
    "-   Document type and identification ( \"Exhibit Label: The Starry Night\")\n",
    "-   Content excerpts from the source document\n",
    "-   Metadata ( creation date, curator notes, exhibit number, author)\n",
    "-   Relevance scores from the retrieval and ranking stages\n",
    "\n",
    "This allows your museum guide to not only provide accurate information but also direct visitors to specific artifacts, display cases, or additional online resources, enhancing the visitor experience and trustworthiness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "museum-domain-use-cases",
   "metadata": {},
   "source": [
    "## Museum Domain Use Cases: RAG in Action\n",
    "\n",
    "Let's explore practical scenarios where RAG transforms a generic digital human into an expert museum guide:\n",
    "\n",
    "### 1. Artifact Information Queries\n",
    "When visitors ask about specific pieces (\"Tell me about the blue vase in Gallery 7\"), RAG retrieves detailed curatorial descriptions, historical context, acquisition details, and related artifacts from your collection management system. The LLM then synthesizes this information into a natural, conversational response.\n",
    "\n",
    "### 2. Contextual Recommendations\n",
    "A visitor might ask, \"What else should I see if I'm interested in Roman sculpture?\" RAG can access thematic connections across your collection, identifying related artists, periods, or geographical origins, providing personalized tour suggestions that a general LLM would struggle with.\n",
    "\n",
    "### 3. Historical Context\n",
    "RAG enables your guide to draw from extensive historical databases, academic papers, and curatorial research to provide rich contextual information about time periods, artistic movements, or cultural significance that might be too vast or too niche for an LLM's pre-trained knowledge.\n",
    "\n",
    "### 4. Multilingual Support\n",
    "By configuring different document collections (or multilingual embeddings) for different languages, your museum guide can serve international visitors in their preferred language while maintaining factual accuracy across all stored information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-complements",
   "metadata": {},
   "source": [
    "## How RAG Complements LLM and Guardrails: A Multi-Layered Approach\n",
    "\n",
    "A truly robust digital human system relies on the synergy of multiple AI components. RAG doesn't replace LLMs or guardrails, it enhances them to form a powerful, multi-layered defense and capability system.\n",
    "\n",
    "### The Three-Layer Defense (Revisited with RAG)\n",
    "\n",
    "1.  **`GuardrailProcessor` (Keyword-Based)**: Acts as the first line of defense, quickly blocking explicit inappropriate queries before they consume resources or reach more complex systems.\n",
    "2.  **NeMo Guardrails NIMs (Semantic Safety & Topical)**: Provides a deeper, semantic layer of safety and topical control, ensuring queries align with acceptable conversation boundaries.\n",
    "3.  **RAG Knowledge Filtering**: Ensures responses are grounded in authoritative museum content rather than potentially unreliable general knowledge from the LLM, directly addressing the hallucination problem.\n",
    "4.  **LLM Safety**: The underlying language model still provides inherent safety measures and maintains conversational appropriateness based on its training, acting as a final check.\n",
    "\n",
    "### Scalability and Maintenance\n",
    "\n",
    "Unlike fine-tuning approaches (which require retraining the entire model for knowledge updates), RAG allows you to update your digital human's knowledge simply by updating the document collection in your vector database. New acquisitions, updated attributions, or revised interpretations can be incorporated without retraining the entire system, making it highly scalable and easier to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animation-integration",
   "metadata": {},
   "source": [
    "## Animation Integration for Dynamic Digital Humans (Building on Module 3.1)\n",
    "\n",
    "While RAG provides factual depth, a digital human's expressiveness and believability are significantly enhanced by its **animation system**. The NVIDIA ACE Controller SDK provides a comprehensive animation system specifically designed for avatar interactions, allowing for dynamic and context-aware visual responses.\n",
    "\n",
    "How RAG interacts with animation is the *retrieved content* can directly influence the avatar's non-verbal communication. For instance:\n",
    "\n",
    "-   If RAG retrieves information about a specific location, the avatar could use **pointing gestures** to direct attention (\"The sculpture you asked about is in Gallery 3, over there.\").\n",
    "-   When presenting detailed historical facts retrieved via RAG, the avatar might employ **presentation gestures** to emphasize key points, adding to the informative experience.\n",
    "-   The avatar's overall state (\"Attentive\" when listening, \"Thinking\" during retrieval, \"Talking\" when responding) enhances the natural interaction, providing visual cues that complement the verbal and factual information.\n",
    "\n",
    "This synergy between factual knowledge (RAG) and expressive animation creates an immersive and natural user experience, making the digital human come alive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-considerations",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "The RAG service includes configurable parameters for balancing response quality with latency. The `vdb_top_k` (number of initial documents retrieved) and `reranker_top_k` (number of documents selected after re-ranking) parameters allow you to tune the trade-off between comprehensive knowledge retrieval and response speed. Higher values for `top_k` parameters can lead to more accurate answers but also increased latency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-exploration",
   "metadata": {},
   "source": [
    "## Assignment: Designing a RAG-Enhanced Digital Human for a Specific Domain\n",
    "\n",
    "This assignment challenges you to design a digital human application that leverages RAG to provide expert, factual information. You'll need to define a specific domain, outline the RAG components, and explain how it integrates into the broader digital human pipeline, considering LLM authoring, guardrails, and animation.\n",
    "\n",
    "### Brief\n",
    "1.  **Select a Domain:** Choose a domain (a specialized medical assistant, a technical support agent for a complex product, a historical tour guide for a specific landmark, a legal information bot).\n",
    "2.  **Define the Knowledge Base:** What kind of information would your digital human need access to, and how would it be structured for RAG?\n",
    "3.  **Propose the RAG-Enabled Solution:** Describe your digital human and its interaction flow, emphasizing the role of RAG.\n",
    "\n",
    "### Deliverable\n",
    "Write a **400-500 word proposal** covering:\n",
    "\n",
    "1.  **Application and Core Problem (approx. 75 words):**\n",
    "    *   Briefly describe your chosen digital human application and its primary goal within the selected domain.\n",
    "    *   Why is RAG essential for this application, particularly in overcoming LLM limitations like hallucinations or outdated knowledge?\n",
    "\n",
    "2.  **RAG-Enhanced Digital Human Architecture and Capabilities (approx. 300 words):**\n",
    "    *   **Knowledge Base Design:** What specific types of documents or data would constitute your knowledge base (product manuals, research papers, legal precedents)? How would they be processed for the vector database ( chunking, embedding)?\n",
    "    *   **RAG Workflow:** Describe how a typical user query would flow through your RAG pipeline. Mention the roles of retrieval ( `vdb_top_k`), ranking ( `reranker_top_k`), and context injection into the LLM prompt. How would `NvidiaRAGService` (conceptually) facilitate this?\n",
    "    *   **Complementary AI:**\n",
    "        *   How would this RAG system work alongside **LLM Authoring** (a specific system prompt for summarization of retrieved content)?\n",
    "        *   How would **Guardrails** (both `GuardrailProcessor` and NeMo Guardrails NIMs) protect against inappropriate queries that might bypass RAG, or filter unsafe content in RAG's retrieved results?\n",
    "        *   How would **Animation** enhance the experience, perhaps by visualizing retrieved data (pointing to a diagram, using gestures for emphasis when citing sources)?\n",
    "    *   **Citation Strategy:** How would your digital human provide citations to users to build trust and allow for verification?\n",
    "    *   **High-level Data Flow:** Briefly sketch (in text) the order of operations in your Pipecat pipeline, integrating RAG, LLM, Guardrails, and Animation.\n",
    "\n",
    "3.  **Anticipated Benefits and Future Considerations (approx. 75 words):**\n",
    "    *   What are the expected improvements in accuracy, reliability, and user experience for your specific application due to RAG?\n",
    "    *   Briefly discuss one future enhancement for your RAG system (supporting multimodal document retrieval, incorporating real-time data feeds, or enabling conversational RAG with follow-up questions).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-conclusion",
   "metadata": {},
   "source": [
    "## Next Steps & Conclusion\n",
    "\n",
    "Congratulations! You've now gained a deep understanding of Retrieval-Augmented Generation (RAG) and its crucial role in building knowledgeable and trustworthy digital humans. You've explored its core stages, how it integrates into the `nvidia-pipecat` architecture, and its powerful synergy with LLM authoring, guardrails, and animation.\n",
    "\n",
    "RAG empowers your digital humans to move beyond static knowledge, providing dynamic, factual, and citable responses that significantly enhance their utility and credibility in specialized domains.\n",
    "\n",
    "This module concludes our exploration of the LLM-RAG fundamentals. You now have the conceptual tools to design sophisticated conversational AI systems. Keep experimenting, and apply these powerful techniques to your own innovative digital human projects!\n",
    "\n",
    "**To Prepare:**\n",
    "- Complete the assignment, focusing on a clear, well-reasoned design for your RAG-enabled digital human.\n",
    "- Review the NVIDIA RAG Blueprint documentation (linked in the introduction) to understand the practical aspects of deploying a RAG server.\n",
    "- Reflect on how all the modules so far (Pipecat basics, LLM authoring, guardrails, and RAG) come together to form a complete digital human system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
