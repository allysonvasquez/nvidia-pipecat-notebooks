{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e683941",
   "metadata": {},
   "source": [
    "# Module 4.2 - Multimodal LLM Integration\n",
    "\n",
    "## Module Introduction\n",
    "\n",
    "Welcome to Module 4.2 of the NVIDIA Digital Humans Teaching Kit! In the previous modules, you've mastered the fundamentals of RAG for text-based knowledge grounding. However, real-world information is rarely limited to text; it often includes images, diagrams, tables, and charts. To create truly intelligent and perceptive digital humans, they must be able to understand and reason with information presented in diverse modalities.\n",
    "\n",
    "This notebook will delve into how the NVIDIA RAG blueprint extends its capabilities to include multimodal understanding, specifically focusing on document intelligence (processing text, tables, charts from documents). We will explore how these multimodal inputs are processed for RAG and how they ultimately enrich the digital human's conversational abilities. While direct interaction with raw multimodal streams (like live camera feeds) within `nvidia-pipecat` may involve other components, this module emphasizes the power of the RAG blueprint in making complex documents \"readable\" for your digital human."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d8d23",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "Upon completing this notebook, you will be able to:\n",
    "\n",
    "*   Understand the concept of multimodal understanding within the context of the NVIDIA RAG blueprint's document intelligence.\n",
    "*   Explain how different modalities (text, tables, charts within documents) are processed and ingested for RAG.\n",
    "*   Discuss the benefits of leveraging multimodal document understanding for enhancing digital human dialogues.\n",
    "*   Outline how the multimodal features of the NVIDIA RAG blueprint are conceptually leveraged to provide richer context to LLMs.\n",
    "*   Recognize the difference between multimodal understanding for RAG (document intelligence) and broader real-time multimodal stream processing in digital humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa783cf",
   "metadata": {},
   "source": [
    "## Required Prerequisites and Setup\n",
    "\n",
    "To get the most out of this notebook, ensure you have:\n",
    "\n",
    "*   **Python Proficiency:** Familiarity with Python programming.\n",
    "*   **Jupyter Notebooks / VS Code Experience:** Comfort with navigating and executing code within a Jupyter environment.\n",
    "*   **Foundational AI Knowledge:** Basic understanding of LLMs, embeddings, and intelligent systems.\n",
    "*   **Module 0-3 Completion:** A stable environment set up from Module 0, and a grasp of the digital human pipeline, speech services, dialogue management, and RAG concepts covered in Modules 1, 2, and 3.\n",
    "\n",
    "### Module-Specific Setup\n",
    "\n",
    "This module primarily focuses on conceptual understanding of multimodal aspects within the RAG blueprint. Direct code implementation of complex multimodal processing is beyond this notebook's scope, as it's handled by the RAG blueprint's internal services.\n",
    "\n",
    "1.  **Sample Multimodal Documents:** We will refer to examples of documents containing mixed modalities for conceptual discussion.\n",
    "    *   **RA Note:** Suggest referring to examples of documents that combine text with tables, charts, or images (scientific papers, annual reports, product specification sheets). If possible, suggest providing conceptual (dummy) paths or references to such files within a `data/multimodal_docs` directory for students to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca2c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block Placeholder: Install necessary libraries for multimodal processing\n",
    "# RA Note: Ensure all dependencies for basic image processing are covered in the setup instructions above.\n",
    "# Example:\n",
    "# !pip install -q Pillow numpy\n",
    "# import PIL.Image\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bca949",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Multimodal Understanding in the NVIDIA RAG Blueprint\n",
    "\n",
    "Modern knowledge bases aren't just collections of plain text. They frequently include complex documents with rich layouts, embedded images, data-rich tables, and informative charts. The NVIDIA RAG blueprint is designed to process and understand these multimodal aspects, transforming them into a unified representation that can enhance LLM grounding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cfeae5",
   "metadata": {},
   "source": [
    "### 1.1 Beyond Text: Document Intelligence for RAG\n",
    "\n",
    "Document intelligence refers to the ability of an AI system to not only extract text from documents but also to understand their structure, layout, and the relationships between different data types (text, tables, images, charts). For RAG, this means:\n",
    "\n",
    "*   **Table Understanding:** Extracting data from tables and understanding row/column relationships, converting them into a text format that an LLM can process meaningfully.\n",
    "*   **Chart Interpretation:** Identifying data points, trends, and labels within charts, and summarizing them in a textual description.\n",
    "*   **Image Captioning/OCR:** For images embedded within documents, applying Optical Character Recognition (OCR) to extract text, and/or generating descriptive captions that can be added to the document chunk.\n",
    "*   **Layout Awareness:** Understanding how text, tables, and images are positioned relative to each other to preserve contextual meaning.\n",
    "\n",
    "This deep document intelligence within the RAG blueprint ensures that even highly complex or visually dense documents can contribute effectively to your digital human's knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7014b3",
   "metadata": {},
   "source": [
    "### 1.2 How Multimodal Data is Processed for RAG\n",
    "\n",
    "When a multimodal document is ingested into the NVIDIA RAG blueprint, a sophisticated pipeline comes into play:\n",
    "\n",
    "1.  **Document Parsing:** The raw document (PDF) is parsed to identify different content types (text, images, tables).\n",
    "2.  **Multimodal Feature Extraction:** Specialized models extract features from each modality:\n",
    "    *   Text is extracted and cleaned.\n",
    "    *   Tables are converted into structured text (Markdown tables or prose descriptions).\n",
    "    *   Images may undergo OCR for text content, or be passed through a vision model to generate descriptive captions or embeddings.\n",
    "3.  **Cross-Modal Alignment:** The extracted information from different modalities is then aligned and combined, often by inserting multimodal descriptions directly into the text chunks or creating a joint representation.\n",
    "4.  **Chunking and Embedding:** The enriched document content is then chunked (similar to Module 4.1) and converted into embeddings. These embeddings now implicitly capture meaning derived from both text and visual elements.\n",
    "\n",
    "**[RA Note: Insert a clear diagram illustrating the multimodal document ingestion pipeline for RAG. Show raw document -> parsing -> parallel processing for text, tables, images (with respective tools like OCR, table extractors) -> cross-modal fusion/alignment -> unified chunks -> embeddings -> vector database.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78324d24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Impact of Multimodal RAG on Digital Human Interaction\n",
    "\n",
    "The integration of multimodal document understanding into your RAG system significantly elevates the capabilities of your digital human, enabling richer, more accurate, and contextually aware interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fe622",
   "metadata": {},
   "source": [
    "### 2.1 Richer Context for LLMs\n",
    "\n",
    "When a digital human's RAG system can understand multimodal documents, the context provided to the LLM becomes far richer. Instead of just receiving text snippets, the LLM receives context that might include a textual description of a graph, key data points from a table, or extracted labels from a diagram. This allows the LLM to:\n",
    "\n",
    "*   **Answer complex questions:** Handle queries that require reasoning across text and visual information (\"What was the sales growth in Q3 based on the chart on page 7?\").\n",
    "*   **Improve factual accuracy:** Ground responses in data from tables or charts, not just prose.\n",
    "*   **Provide nuanced explanations:** Offer more comprehensive answers by drawing insights from all available document elements.\n",
    "\n",
    "### 2.2 Enhancing Conversational Grounding and Specificity\n",
    "\n",
    "Multimodal RAG empowers the digital human to provide highly specific and grounded answers, even when the query itself is purely textual but refers to visual information. For example, if a user asks about a specific feature in a product manual that is primarily described in a diagram, multimodal RAG can retrieve the relevant visual information and explain it verbally.\n",
    "\n",
    "This capability closes a significant gap, allowing digital humans to truly act as intelligent agents in scenarios where information is presented in diverse formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8d7e5",
   "metadata": {},
   "source": [
    "### 2.3 Example Scenarios for Multimodal RAG in Digital Humans\n",
    "\n",
    "Consider these real-world applications where multimodal RAG profoundly impacts digital human effectiveness:\n",
    "\n",
    "*   **Technical Support Agent:** A user uploads a screenshot of an error message or a diagram of a system. The digital human can understand the visual context, combine it with the user's spoken description, and provide precise troubleshooting steps.\n",
    "*   **Financial Assistant:** A user asks questions about a company's financial report, which contains extensive tables and charts. The digital human can analyze these visuals to answer questions about revenue trends, profit margins, or market share.\n",
    "*   **Educational Tutor:** A student provides an image of a complex mathematical problem or a scientific diagram. The digital human can interpret the visual information and guide the student through the solution or explanation.\n",
    "\n",
    "**[RA Note: Provide a markdown context story/scenario here where a digital human demonstrates advanced multimodal interaction involving a document containing text and visuals. Include a short dialogue example illustrating how the digital human leverages its multimodal understanding to answer a question about the document.]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4619e3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Conceptual Interaction with Multimodal RAG Features\n",
    "\n",
    "While the NVIDIA RAG blueprint handles the intricate details of multimodal processing internally during ingestion and querying, understanding how a digital human would conceptually leverage these capabilities is important. The interaction remains largely similar to text-based RAG, as the multimodal information is processed and embedded within the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54723d7",
   "metadata": {},
   "source": [
    "### 3.1 Leveraging the Blueprint's Multimodal Capabilities\n",
    "\n",
    "From the perspective of querying the RAG blueprint, the process is consistent whether the underlying knowledge base is text-only or multimodal. You send a text query, and the RAG system retrieves the most relevant chunks. The key difference is that these chunks now contain semantic information derived from all modalities of the original document.\n",
    "\n",
    "Therefore, the `NvidiaRAGService` in `nvidia-pipecat` (as introduced in Module 4.1) seamlessly handles this. When you query the RAG blueprint via this service, the response will be generated by an LLM that has been provided with context that includes multimodal understanding, even if your query itself is just text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block Placeholder: Conceptual Querying of a Multimodal RAG Blueprint\n",
    "# RA Note: Provide conceptual Python code snippets (as comments) demonstrating how a text query\n",
    "# would be sent to a RAG blueprint that supports multimodal document understanding.\n",
    "# Emphasize that the query interface doesn't change, but the underlying data processing does.\n",
    "# This code should *not* be runnable without a deployed service.\n",
    "\n",
    "# Example:\n",
    "# import requests\n",
    "#\n",
    "# def query_multimodal_rag_blueprint(user_query: str, rag_query_url: str = \"http://localhost:8081/query\"):\n",
    "#     payload = {\"query\": user_query}\n",
    "#     try:\n",
    "#         response = requests.post(rag_query_url, json=payload)\n",
    "#         response.raise_for_status()\n",
    "#         result = response.json()\n",
    "#         print(f\"Multimodal RAG Answer: {result.get('answer', 'No answer received.')}\")\n",
    "#         print(f\"Retrieved Sources: {result.get('sources', 'No sources found.')}\")\n",
    "#         return result\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error querying multimodal RAG blueprint: {e}\")\n",
    "#         return None\n",
    "#\n",
    "# # Dummy usage (won't run without a deployed service with multimodal knowledge):\n",
    "# # query_multimodal_rag_blueprint(\"Based on the sales report, what was the revenue in Q2 and how does it compare to Q1?\")\n",
    "# # This query assumes the RAG blueprint has processed a sales report with tables/charts.\n",
    "\n",
    "# RA Note: Add a markdown explanation that the LLM's capability to respond to such queries depends on the RAG blueprint's ability to extract and provide multimodal context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c78b577",
   "metadata": {},
   "source": [
    "### 3.2 Hands-on Lab: Conceptual Multimodal RAG Interaction\n",
    "\n",
    "**[RA Note: Develop a detailed hands-on lab section in markdown here. This lab will be conceptual, building on the assumption that a RAG blueprint with multimodal document understanding is deployed. The lab should guide students through:]**\n",
    "\n",
    "1.  **Scenario Setup:** Describe a realistic scenario where multimodal RAG is critical. For example, a digital human acting as a technical assistant responding to questions about a software manual that contains diagrams and code snippets.\n",
    "\n",
    "2.  **Conceptual Document Ingestion:** Walk through the conceptual process of ingesting such a multimodal document into the RAG blueprint. Emphasize that the blueprint handles the parsing, image analysis, and table extraction automatically.\n",
    "\n",
    "3.  **Simulated Querying:** Guide the student to formulate text-based queries that specifically target the multimodal aspects of the ingested documents (\"Describe the network architecture shown on page 3,\" \"What are the values in the 'throughput' column of the performance table?\").\n",
    "\n",
    "4.  **Expected Responses and RAG Benefits:** Discuss the types of responses the digital human would generate, highlighting how they demonstrate an understanding beyond mere text. For example, the digital human might describe visual elements or synthesize data from tables.\n",
    "\n",
    "5.  **Try This!** Include a \"Try This!\" section with suggestions for further conceptual exploration:\n",
    "    *   Consider how to handle video input, perhaps by extracting key frames and processing them for RAG. What are the challenges?\n",
    "    *   Discuss how the digital human might proactively describe its visual environment (if it were integrated with a live camera feed and a general visual understanding model, separate from the RAG blueprint's document understanding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94840b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Broader Multimodal Integration in Digital Human Pipelines (Beyond RAG)\n",
    "\n",
    "While the NVIDIA RAG blueprint excels at multimodal *document understanding* for factual grounding, digital humans can integrate other forms of real-time multimodal data for broader interactive experiences. This often involves processing live streams (like video or audio) and feeding them to models or for display in the digital human's UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d129c5",
   "metadata": {},
   "source": [
    "### 4.1 Real-time Image and Video Handling in `nvidia-pipecat`\n",
    "\n",
    "The `nvidia-pipecat` framework provides components for handling real-time media streams. For instance, classes related to image handling in `custom_view.py` (`ImageFrame`, `ImageProcessor`) are typically used for displaying visual information in a digital human's user interface, or for performing basic visual processing before feeding relevant data to other models or services. This is distinct from the RAG blueprint's deep document understanding but complements it.\n",
    "\n",
    "*   **Image Handling Components:** `nvidia-pipecat` can ingest image data (from a webcam feed) and manage it within its pipeline for display or for feature extraction by a separate model if needed. These components facilitate the visual aspects of the digital human's interaction, such as displaying what the digital human is \"seeing\" or showing supplemental visuals.\n",
    "\n",
    "**[RA Note: Provide a conceptual diagram of how a live camera feed might be ingested into `nvidia-pipecat`, processed (resized), and then potentially displayed in a UI or routed to a *separate* vision model (not the RAG blueprint) for real-time scene understanding, if relevant to `pipecat`'s broader capabilities.]**\n",
    "\n",
    "### 4.2 Integrating with Dedicated Multimodal LLMs (Conceptual)\n",
    "\n",
    "For scenarios requiring direct, real-time visual perception and reasoning that is *not* tied to retrieving information from a RAG knowledge base (\"Describe what you see in the room right now\"), a digital human might integrate with dedicated multimodal LLMs like LLaVA or VILA. These models often provide APIs that accept image inputs alongside text and generate coherent textual responses based on live visual analysis.\n",
    "\n",
    "This conceptual pipeline involves:\n",
    "\n",
    "1.  Capturing raw image frames (from a camera) via a `pipecat` processor.\n",
    "2.  Preprocessing these frames for the multimodal LLM (resizing, normalization).\n",
    "3.  Sending the processed image data and a text query to the multimodal LLM's API endpoint.\n",
    "4.  Receiving the LLM's textual response and feeding it back into the `pipecat` pipeline for TTS and animation.\n",
    "\n",
    "This broadens the digital human's perceptual capabilities beyond just text and structured documents handled by RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90206143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block Placeholder: Conceptual API Call to NVIDIA Multimodal LLM Service (LLaVA/VILA)\n",
    "# RA Note: Provide conceptual Python code snippets (as comments) demonstrating how one would\n",
    "# make an API call to a hypothetical NVIDIA multimodal LLM service that accepts image data and text.\n",
    "# This illustrates the general pattern for interacting with such models.\n",
    "# This code should *not* be runnable without the service.\n",
    "# Example:\n",
    "# import requests\n",
    "# import base64\n",
    "# from PIL import Image\n",
    "# from io import BytesIO\n",
    "#\n",
    "# def image_to_base64(image_path):\n",
    "#     with Image.open(image_path) as img_file:\n",
    "#         buffered = BytesIO()\n",
    "#         img_file.save(buffered, format=\"JPEG\")\n",
    "#         return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "#\n",
    "# def query_multimodal_llm_service(text_query: str, image_path: str, service_url: str = \"http://localhost:8000/multimodal_infer\"):\n",
    "#     base64_image = image_to_base64(image_path)\n",
    "#     payload = {\n",
    "#         \"text_input\": text_query,\n",
    "#         \"image_input\": base64_image # This would be an array of pixel data or similar in a real scenario\n",
    "#     }\n",
    "#     try:\n",
    "#         response = requests.post(service_url, json=payload)\n",
    "#         response.raise_for_status()\n",
    "#         result = response.json()\n",
    "#         print(f\"Multimodal LLM Response: {result.get('response', 'No response received.')}\")\n",
    "#         return result\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error querying multimodal LLM: {e}\")\n",
    "#         return None\n",
    "#\n",
    "# # Dummy usage (won't run without the service and a valid image):\n",
    "# # query_multimodal_llm_service(\"What is in this image?\", \"./data/images/sample_scene.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef344dc8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've expanded your understanding of digital humans by exploring multimodal LLM integration. You've learned how the NVIDIA RAG blueprint incorporates multimodal document understanding to provide richer, more accurate factual grounding. You've also conceptually touched upon how digital humans can process other forms of real-time multimodal data for broader interactive experiences.\n",
    "\n",
    "The ability to process and reason with information from various modalities significantly enhances the richness, intelligence, and interactivity of your digital human agents.\n",
    "\n",
    "### Reflection Exercises\n",
    "\n",
    "*   Consider a digital human application (an architectural guide, a museum curator). How would multimodal RAG (document understanding) and broader real-time multimodal capabilities (scene understanding) fundamentally change its usefulness and user experience?\n",
    "*   What are the primary technical challenges you foresee in building and deploying a real-time multimodal digital human that combines both RAG's document intelligence and live visual perception?\n",
    "*   How would you design the conversational flow if a digital human could see an object in real-time *and* retrieve detailed information about it from a RAG knowledge base? Create a short dialogue example.\n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "This module concludes our deep dive into RAG and Multimodal expansion within the Digital Humans Teaching Kit. The next modules, starting with **Module 5 – Advanced Pipeline Features & Quality Control**, will focus on ensuring the robustness, responsiveness, and reliability of your real-time digital human pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
