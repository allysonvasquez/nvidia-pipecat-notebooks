{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "module-title-welcome",
   "metadata": {},
   "source": [
    "# Module 3.1: LLM Integration Fundamentals & Basic Prompt Engineering\n",
    "\n",
    "Welcome to Module 3.1 of the Digital Human Teaching Kit! In our journey to build engaging digital humans, we've laid the groundwork with the Pipecat framework and understood how real-time speech input (ASR) and output (TTS) are orchestrated. Now, it's time to delve into **the 'mind' of your digital human: the Large Language Model (LLM)**.\n",
    "\n",
    "LLMs are incredibly powerful, but out of the box, they are often generic. To transform a raw LLM into a knowledgeable, persona-driven, and safe conversational agent, we need to guide its behavior. This module will cover the fundamental concepts of integrating LLMs into your Pipecat pipeline, managing conversational context, and introducing the essential techniques of **prompt engineering**. You'll learn how to author prompts to shape your digital human's personality and responses, and identify scenarios where more advanced techniques like Guardrails and Retrieval-Augmented Generation (RAG) become necessary.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand and manage conversational history using system prompts and user messages.\n",
    "- Apply basic prompt engineering techniques, including role prompting and prompt clarity, to guide LLM behavior.\n",
    "- Customize LLM behavior through various `NvidiaLLMService` parameters.\n",
    "- Identify the common characteristics of weak prompts and strategies for refinement.\n",
    "- Experiment with different LLM models available via NVIDIA NIMs and observe differences in their outputs.\n",
    "\n",
    "\n",
    "## Add section on ...\n",
    "- if unfamiliar with topics .. refer to *lecture module #*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-mind-intro",
   "metadata": {},
   "source": [
    "# The Mind of the Digital Human: Large Language Models\n",
    "\n",
    "In Module 1, we saw how Pipecat can take audio input, transcribe it into text using ASR, and then convert text responses back into speech using TTS. The crucial missing piece in that initial loop is the LLM that processes the transcribed text and generates a meaningful, relevant response.\n",
    "\n",
    "LLMs by default are simply predictors of the next most likely token. To make them act as a specific persona, answer domain-specific questions, or avoid unwanted topics, you need to **author** their behavior.\n",
    "\n",
    "NVIDIA provides a streamlined way to access and deploy these powerful models through **NIMs**. The `nvidia-pipecat` library includes the `NvidiaLLMService`, which provides an OpenAI-compatible interface to these NIMs, making it easy to integrate state-of-the-art LLMs into your Pipecat pipelines.\n",
    "\n",
    "### Initial Imports and Setup\n",
    "Let's import the necessary Pipecat components and the `NvidiaLLMService`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "code-imports-llm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA API key loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pipecat.frames.frames import Frame, TextFrame, EndFrame\n",
    "from pipecat.observers.base_observer import BaseObserver\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "from pipecat.services.ai_services import LLMService # Required for type hinting/inheritance\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # For running asyncio in Jupyter\n",
    "\n",
    "load_dotenv() # Load environment variables from a .env file if available\n",
    "\n",
    "# Try to get the API key from the environment\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"NVIDIA API key not found or invalid in .env file.\")\n",
    "    nvapi_key = getpass.getpass(\"üîê Enter your NVIDIA API key: \").strip()\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "else:\n",
    "    print(\"NVIDIA API key loaded from .env file.\")\n",
    "\n",
    "class ChatResponsePrinter(BaseObserver):\n",
    "    \"\"\"A simple observer to print streamed LLM responses.\"\"\"\n",
    "    async def on_push_frame(self, src: LLMService, dst, frame: Frame, direction, timestamp):\n",
    "        if isinstance(frame, TextFrame):\n",
    "            # Print LLM response chunks as they arrive\n",
    "            print(frame.text, end=\"\", flush=True)\n",
    "        elif isinstance(frame, EndFrame):\n",
    "            print() # Newline after response completes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-inference-needs",
   "metadata": {},
   "source": [
    "## Basic LLM Inference & The Need for Authoring\n",
    "\n",
    "Let's create a simple chat function that uses an LLM. We'll start with a generic LLM and a basic system prompt, then observe its behavior. We'll use `meta/llama-3.3-70b-instruct` as a capable general-purpose model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-context-memory",
   "metadata": {},
   "source": [
    "### LLM Context and Memory: Guiding the Conversation\n",
    "\n",
    "For an LLM to engage in a coherent, multi-turn conversation, it needs **context** and **memory**. This isn't built into the core LLM inference itself; it's managed by how you construct the messages sent to the LLM.\n",
    "\n",
    "The OpenAI API format, widely adopted by LLM providers like NVIDIA NIMs, uses a list of \"messages\" to define the conversation history. Each message has a `role` and `content`:\n",
    "\n",
    "-   **System Prompt (`role: \"system\"`)**: This is the initial instruction given to the LLM. It defines its persona, rules, and general behavior. This is crucial for **role prompting** ‚Äì telling the LLM *who it is*. For a museum guide, this prompt would structure its knowledge and conversational style. [7, 22]\n",
    "-   **User Messages (`role: \"user\"`)**: These are the inputs from the human user.\n",
    "-   **Assistant Messages (`role: \"assistant\"`)**: These are the LLM's previous responses. Including them allows the LLM to remember the conversation history.\n",
    "\n",
    "The `OpenAILLMContext` class from `pipecat` is designed to manage this message array, automatically appending user and assistant turns to maintain conversational memory.  \n",
    "\n",
    "See [1-1-Introduction-ACE-Controller-Pipecat](<../1-Foundations of Digital Human Agents/1-1-Introduction-ACE-Controller-Pipecat.ipynb>) for a refresh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-authoring-parameters",
   "metadata": {},
   "source": [
    "#### LLM Authoring with `NvidiaLLMService`: Parameters and Context Truncation\n",
    "\n",
    "LLM authoring extends beyond just the system prompt. The `NvidiaLLMService` in `nvidia-pipecat` provides a comprehensive interface to NVIDIA's language model endpoints, offering fine-grained control over response generation through various parameters.\n",
    "\n",
    "You can customize parameters such as:\n",
    "-   **`temperature`**: Controls the randomness of the output. Higher values mean more creative, diverse responses.\n",
    "-   **`top_p`**: Controls diversity via nucleus sampling. The model considers tokens whose cumulative probability mass is below a certain threshold.\n",
    "    *   `temperature` and `top_p` are often used together to control the output's creativity and coherence.\n",
    "-   **`frequency_penalty`**: Penalizes new tokens based on their existing frequency in the text, reducing repetition.\n",
    "-   **`presence_penalty`**: Penalizes new tokens based on whether they appear in the text so far, encouraging new topics.\n",
    "-   **`max_tokens`**: The maximum number of tokens to generate in the completion.\n",
    "\n",
    "These parameters are passed through the `InputParams` class to the `NvidiaLLMService`, allowing for dynamic adjustment of the LLM's behavior during interaction.\n",
    "\n",
    "#### Context Management and Truncation\n",
    "For long conversations, managing the context window (the maximum number of tokens an LLM can process at once) is critical. The ACE Controller and `nvidia-pipecat` support **context truncation** to manage these limits. This ensures that even in extended interactions, the system prompt and the most relevant recent conversation history are preserved, maintaining consistent behavior and avoiding out-of-memory errors. The context aggregation system manages conversation history and supports both interim and final transcriptions, crucial for real-time interaction management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "code-basic-llm-chat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "from pipecat.frames.frames import TextFrame, EndFrame\n",
    "from pipecat.processors.frame_processor import FrameDirection\n",
    "\n",
    "async def run_basic_llm_chat(model_name: str, system_message: str, temperature=0.2, top_p=0.7, max_tokens=1024):\n",
    "    print(f\"\\n--- Starting Basic LLM Chat with {model_name} ---\")\n",
    "    print(f\"System message: '{system_message}'\")\n",
    "\n",
    "    # Use the InputParams class\n",
    "    generation_params = NvidiaLLMService.InputParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "    # Initialize the LLM service with parameters\n",
    "    llm_service = NvidiaLLMService(\n",
    "        model=model_name,\n",
    "        api_key=api_key,\n",
    "        params=generation_params\n",
    "    )\n",
    "\n",
    "    context_manager = OpenAILLMContext([\n",
    "        {\"role\": \"system\", \"content\": system_message}\n",
    "    ])\n",
    "\n",
    "    observer = ChatResponsePrinter()\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        context_manager.add_message({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "        full_response = \"\"\n",
    "\n",
    "        try:\n",
    "            stream = await llm_service.get_chat_completions(context_manager, context_manager.get_messages())\n",
    "            async for chunk in stream:\n",
    "                if chunk.text():\n",
    "                    await observer.on_push_frame(llm_service, None, TextFrame(chunk.text()), None, 0)\n",
    "                    full_response += chunk.text()\n",
    "            await observer.on_push_frame(llm_service, None, EndFrame(), None, 0)\n",
    "            context_manager.add_message({\"role\": \"assistant\", \"content\": full_response})\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            context_manager.messages = context_manager.messages[:-1]\n",
    "            continue\n",
    "\n",
    "    print(\"--- Chat Ended ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "museum-guide-example",
   "metadata": {},
   "source": [
    "### Example: A Generic Museum Guide\n",
    "\n",
    "Let's try to make our LLM act as a museum guide with a very simple system prompt. Observe how it behaves and where it might fall short.\n",
    "\n",
    "*(Note: LLM responses can vary. You might experience minor hallucinations or very general answers.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "run-museum-guide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Basic LLM Chat with meta/llama-3.3-70b-instruct ---\n",
      "System message: 'You are a helpful and informative museum guide. Keep your answers concise.'\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Welcome to the museum. I'll be your guide today. How can I assist you? Would you like a tour of our current exhibits or information on a specific collection?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  tell me about the banana exhibit and its hours\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The Banana Exhibit is a popular display featuring the history and cultural significance of bananas. It's located in Gallery 3, and the hours are:\n",
      "\n",
      "* Monday to Thursday: 10am - 5pm\n",
      "* Friday: 10am - 8pm\n",
      "* Weekends: 12pm - 6pm\n",
      "\n",
      "The exhibit is closed on Tuesdays for maintenance. Would you like to start your visit there?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n",
      "--- Chat Ended ---\n"
     ]
    }
   ],
   "source": [
    "await run_basic_llm_chat(\n",
    "    model_name=\"meta/llama-3.3-70b-instruct\",\n",
    "    system_message=\"You are a helpful and informative museum guide. Keep your answers concise.\",\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limitations-and-needs",
   "metadata": {},
   "source": [
    "**Reflect on the above interaction:**\n",
    "-   Did the guide always stay in character?\n",
    "-   What if a user asked something inappropriate or off-topic? The LLM might still try to answer or provide an unwanted response. This is where **Guardrails** become essential to define safe boundaries and control the conversation.\n",
    "    *   Example: \"Can you tell me how to steal a painting?\" or a more offensive query.\n",
    "-   What if a user asked for detailed information about a specific, obscure artifact? The LLM might **hallucinate** or give a very generic answer, lacking real knowledge. This highlights the need for **RAG (Retrieval-Augmented Generation)**, which connects LLMs to external knowledge bases.\n",
    "    *   Example: \"Tell me about the provenance of the Ming Dynasty vase in Gallery 7.\"\n",
    "\n",
    "This demonstrates that while LLMs are powerful, direct prompting has limitations, paving the way for more advanced techniques that we'll cover in subsequent notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-engineering-intro",
   "metadata": {},
   "source": [
    "## Prompt Engineering: Crafting Effective Instructions\n",
    "\n",
    "**Prompt Engineering** is the art and science of crafting inputs (prompts) to LLMs to elicit desired outputs. It's how we transform a general-purpose language model into a specialized, performant tool for a specific task. While LLMs are sophisticated, their output quality is directly tied to the quality of your prompts.\n",
    "\n",
    "### What Makes A Prompt \"Good\"?\n",
    "Many new users (and even experienced developers) struggle to write effective prompts. Common issues with prompts include being:\n",
    "\n",
    "-   **Too short:** Lacks context or necessary instructions.\n",
    "-   **Too vague:** Offers no concrete examples or desired style.\n",
    "-   **Under-specified:** Doesn't guide the model toward a clear, useful output.\n",
    "\n",
    "It is important to remember that **prompt quality determines output quality**. Among new or novice prompters, prompts are generally too short. They lack context, rarely feature examples, and provide few descriptions.\n",
    "\n",
    "A \"good\" prompt, conversely, is typically:\n",
    "-   **Clear and Specific:** Leaves no room for ambiguity.\n",
    "-   **Context-Rich:** Provides necessary background information.\n",
    "-   **Instruction-Driven:** Clearly states the task, desired format, and constraints.\n",
    "    *   `\"Do not mention X.\"`\n",
    "    *   `\"Respond in JSON format.\"`\n",
    "    *   `\"Keep responses under 50 words.\"`\n",
    "-   **Example-Oriented (where applicable):** Demonstrates the desired input/output pattern.\n",
    "\n",
    "### Key Prompting Techniques\n",
    "\n",
    "Here are some fundamental techniques for effective prompt engineering:\n",
    "\n",
    "1.  **Role Prompting:**\n",
    "    Assigning a persona to the LLM can significantly influence its tone, style, and knowledge domain. This is typically done in the **system prompt**.\n",
    "    *   *Example:* \"You are a helpful and knowledgeable historian.\" vs. \"You are a witty stand-up comedian.\"\n",
    "\n",
    "2.  **Zero-Shot vs. Few-Shot Prompting:**\n",
    "    *   **Zero-Shot:** You provide no examples in the prompt, relying entirely on the LLM's pre-trained knowledge to understand the task. (Our museum guide example above was zero-shot).\n",
    "    *   **Few-Shot:** You include a few examples of input-output pairs directly in the prompt to demonstrate the desired behavior. This is highly effective for guiding the LLM on specific formats, styles, or complex reasoning tasks without fine-tuning.\n",
    "        *   *Example (in the user message history):*\n",
    "            `User: Translate \"Hello\" to French.`\n",
    "            `Assistant: Bonjour.`\n",
    "            `User: Translate \"Thank you\" to Spanish.`\n",
    "\n",
    "3.  **Chain of Thought (CoT) Prompting:**\n",
    "    This technique encourages the LLM to explain its reasoning process step-by-step before arriving at the final answer. This is particularly useful for complex problems, as it often leads to more accurate and reliable outputs. You can enable CoT by simply adding phrases like \"Let's think step by step\" or \"Walk me through your reasoning.\"\n",
    "\n",
    "4.  **Clarity & Specificity:**\n",
    "    Avoid vague language. Be explicit about what you want the LLM to do, what format the output should be in, and any constraints. Use clear verbs and define any jargon. Break down complex requests into smaller, manageable instructions.\n",
    "\n",
    "By combining these techniques, you can significantly improve the quality and consistency of your digital human's responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-models",
   "metadata": {},
   "source": [
    "## Experimenting with Different LLM Models\n",
    "\n",
    "NVIDIA NIMs provide access to a variety of LLM models, each with different architectures, sizes, and training data. While a powerful model like `llama-3.3-70b-instruct` is generally robust, sometimes a smaller, faster model might suffice, or a different model family might excel in specific domains. [14, 15, 27, 30, 32, 33]\n",
    "\n",
    "The flexibility of `nvidia-pipecat` allows you to easily swap out the underlying LLM NIM by simply changing the `model` parameter in `NvidiaLLMService`.\n",
    "\n",
    "Let's try swapping the LLM to `nvidia/nemotron-4-340b-instruct` (or another suitable model available on NVIDIA API Catalog, e.g., `mistral-7b-instruct-v0.2` or `llama-3-8b-instruct`) and see how the responses change for the same museum guide persona. [9, 14]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-swap-llm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Basic LLM Chat with nvidia/nemotron-4-340b-instruct ---\n",
      "System message: 'You are a sophisticated and eloquent museum guide, specializing in Renaissance art. Provide detailed but engaging descriptions.'\n",
      "Type 'exit' to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await run_basic_llm_chat(\n",
    "    model_name=\"nvidia/nemotron-4-340b-instruct\",\n",
    "    system_message=\"You are a sophisticated and eloquent museum guide, specializing in Renaissance art. Provide detailed but engaging descriptions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection-on-models",
   "metadata": {},
   "source": [
    "**Observe the differences:**\n",
    "-   Did the `nemotron-4-340b-instruct` (or your chosen alternative) exhibit a different style or depth of knowledge?\n",
    "    *   Larger models often have more nuanced understanding and richer vocabulary.\n",
    "    *   Models specifically fine-tuned for certain domains might perform better on those topics.\n",
    "-   What were the trade-offs (response speed vs. quality)?\n",
    "\n",
    "This demonstrates the importance of model selection in LLM authoring. For your digital human, you might choose different models based on:\n",
    "-   **Performance requirements:** Latency and throughput.\n",
    "-   **Cost considerations:** Larger models are typically more expensive.\n",
    "-   **Specific capabilities:** Some models excel at creative writing, others at factual recall, or multilingual tasks.\n",
    "-   **Fine-tuning vs. RAG vs. Prompt Engineering:** While prompt engineering is powerful, for highly specialized or constantly changing knowledge, **RAG** is often superior. For deep behavioral changes or specific stylistic adherence, **fine-tuning** an LLM might be considered, though it's a more involved process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beyond-core-llm",
   "metadata": {},
   "source": [
    "## Beyond Core LLM: RAG, Guardrails, and Animation Integration\n",
    "\n",
    "To build intelligent and engaging digital humans, we often need to integrate additional AI capabilities beyond just the core LLM for **knowledge grounding**, **conversational safety**, and **expressive behavior**.\n",
    "\n",
    "This section provides a high-level overview of three key enhancements:\n",
    "- **Guardrails** ‚Äî to enforce boundaries, safety, and domain relevance in conversations.\n",
    "- **Retrieval-Augmented Generation (RAG)** ‚Äî to inject dynamic, factual knowledge into the LLM's responses.\n",
    "- **Animation Integration** ‚Äî to make digital humans visually expressive and context-aware.\n",
    "\n",
    "You'll explore the **technical implementation of Guardrails in Module 3.2**: [Controlling LLM Behavior with Guardrails](./3.2-LLM-Guardrails-and-Topicality.ipynb) and the **full RAG pipeline in Module 3.3**: [RAG for Digital Humans](./3.3-RAG-for-Digital-Humans.ipynb), with subsequent modules focusing on Animation Integration with Audio2Face and the ACE Controller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-reflection",
   "metadata": {},
   "source": [
    "## Assignment: Prompt Refinement for a Persona-Driven Digital Human\n",
    "\n",
    "Building on the concepts of LLM integration and basic prompt engineering, this assignment challenges you to refine a digital human's persona and behavior using only system and user prompts.\n",
    "\n",
    "### Brief\n",
    "1.  **Choose a New Persona:** Select a distinct persona for your digital human (e.g., a grumpy but wise ancient philosopher, a cheerful and enthusiastic travel agent, a concise and formal legal assistant).\n",
    "2.  **Define Success Criteria:** What specific conversational traits, tone, and knowledge boundaries should this persona exhibit?\n",
    "3.  **Iterate on Prompts:** Using the `run_basic_llm_chat` function (or adapting it), experiment with different system messages and initial user messages to embody your chosen persona.\n",
    "\n",
    "### Deliverable\n",
    "Write a **250-350 word reflection** covering:\n",
    "\n",
    "1.  **Your Chosen Persona (approx. 50 words):**\n",
    "    *   Describe the persona and its intended role.\n",
    "    *   List 2-3 key characteristics you want the LLM to consistently display.\n",
    "\n",
    "2.  **Prompt Engineering Journey (approx. 150 words):**\n",
    "    *   Provide your **final system prompt** and an **example initial user message** that best elicits your persona.\n",
    "    *   Describe your iterative process: What initial prompt ideas did you have? What went wrong (e.g., generic responses, breaking character, too verbose/concise)? How did you refine your system prompt and user messages to get closer to the desired behavior? Mention specific prompt engineering techniques you used (e.g., adding constraints, using specific vocabulary, instructing on response length).\n",
    "    *   Briefly discuss how adjusting `NvidiaLLMService` parameters (like `temperature` or `max_tokens`) could further fine-tune your persona's output.\n",
    "\n",
    "3.  **Limitations & Next Steps (approx. 100 words):**\n",
    "    *   Despite your prompt engineering efforts, what are 1-2 scenarios or user queries where your persona-driven LLM still struggles or fails? (e.g., still hallucinates on specific facts, can't handle complex multi-step reasoning, goes off-topic).\n",
    "    *   Explain *why* these failures occur in the context of pure prompt engineering (e.g., lack of external knowledge, no explicit moderation).\n",
    "    *   Briefly state how you anticipate **RAG** (Module 3.2) or **Guardrails** (Module 3.3) could address these specific limitations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-conclusion",
   "metadata": {},
   "source": [
    "## Next Steps & Conclusion\n",
    "\n",
    "Congratulations! You've taken a significant step in authoring digital humans by diving into LLM integration and basic prompt engineering. You now understand how to connect to NVIDIA NIMs, manage conversational context, and shape an LLM's behavior through carefully crafted prompts.\n",
    "\n",
    "You've also critically evaluated the limitations of pure prompt engineering, setting the stage for more advanced capabilities. In the upcoming modules, we will tackle these limitations head-on:\n",
    "\n",
    "-   **Module 3.2: Guardrails and Advanced Prompt Engineering**: Delve deeper into ensuring safe and ethical interactions, and explore more sophisticated prompt engineering techniques.\n",
    "-   **Module 3.3: Retrieval-Augmented Generation (RAG)**: Learn how to provide your digital human with access to external, up-to-date knowledge to prevent hallucinations and provide factual responses.\n",
    "\n",
    "Keep experimenting with different prompts and observing LLM behavior. The more you understand its nuances, the better you'll become at leveraging its power.\n",
    "\n",
    "**To Prepare:**\n",
    "- Complete the assignment, focusing on the iterative process of prompt refinement.\n",
    "- Reflect on how RAG and Guardrails could specifically enhance the persona you designed.\n",
    "- Familiarize yourself with the concepts of knowledge bases and safety policies, as these will be central to the next modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb1d66-5e9d-4d18-aa91-7fb5680adb45",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
