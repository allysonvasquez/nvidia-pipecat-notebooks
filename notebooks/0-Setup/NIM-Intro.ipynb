{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4eea81-d35f-4487-af60-22c02873a3ba",
   "metadata": {},
   "source": [
    "# Introduction to NVIDIA Inference Microservice (NIM) APIs\n",
    "NIMs are like OpenAI‚Äôs API, but it‚Äôs designed to run a pletheroa of powerful models (like LLaMA 3, Mistral, or custom vision/speech models) either cloud hosted or on NVIDIA infrastructure. A NIM is a single model + software stack, packaged into a container and specificially designed to be run on NVIDIA RTX GPUs. NIMs can be used to run LLM models for chat, agents, and all other tasks that require inference.\n",
    "\n",
    "*TODO* purpose of this environment running LLMs is difficult locally without HW, always need api key, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d197a8-3ba9-43d4-89ef-064904acc5b2",
   "metadata": {},
   "source": [
    "## Content Overview \n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Basic NIM Call](#Basic-NIM-Call)\n",
    "- [Try it Yourself](#Try-It-Yourself:-Explore-Another-Chat-Model-from-the-NIM-Catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb9841-9a55-4ddd-8cdc-c58a790bbf1f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Prior to getting started, you will need an NVIDIA API Key from the NVIDIA API Catalog to access the models used in this notebook.  \n",
    "\n",
    "Need an API Key? It's Free!\n",
    "  1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**.\n",
    "  2. Select any model, such as `llama-3.3-70b-instruct`.\n",
    "  3. On the right panel above the sample code snippet, click on \"Get API Key\". This will prompt you to log in if you have not already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8552af-314c-4151-a911-eda8b7a94ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file if available\n",
    "load_dotenv()\n",
    "\n",
    "# Try to get the API key from the environment\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "# Prompt if not set or invalid\n",
    "if not api_key or not api_key.startswith(\"nvapi-\"):\n",
    "    print(\"NVIDIA API key not found or invalid.\")\n",
    "    api_key = getpass.getpass(\"üîê Enter your NVIDIA API key: \").strip()\n",
    "    if not api_key.startswith(\"nvapi-\"):\n",
    "        raise ValueError(f\"{api_key[:5]}... is not a valid NVIDIA API key\")\n",
    "    # Set in environment for the current session\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31249e5e-80d5-4843-90fd-77e607a0cd3b",
   "metadata": {},
   "source": [
    "## Basic NIM Call\n",
    "Lets perform a basic API call to a NIM to learn about its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc951799-14d3-4fd8-94c4-0e5a3a10d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA NIM supports the OpenAI-compatible API interface, so this client works with NIM too.\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client\n",
    "client = OpenAI(\n",
    "  base_url = \"https://integrate.api.nvidia.com/v1\",\n",
    "  api_key = os.environ[\"NVIDIA_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89081ab9-2fd1-490b-b02a-f4a842cc58d5",
   "metadata": {},
   "source": [
    "- `base_url`: Tells the OpenAI client to send requests to NVIDIA‚Äôs NIM endpoint instead of OpenAI‚Äôs.\n",
    "- `api_key`: Used for authentication. Required only if you‚Äôre calling from outside NVIDIA‚Äôs internal systems, and for this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe15ec-289f-418a-a4df-0e009873da43",
   "metadata": {},
   "source": [
    "### Create a Chat Completion Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab396f-95d1-43a7-824e-a25c18df0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"meta/llama-3.3-70b-instruct\",\n",
    "    \n",
    "  messages=[{\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"Tell me about Dumbledore.\"\n",
    "  }],\n",
    "    \n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bacb35-c773-47f2-856f-b42a80a452b6",
   "metadata": {},
   "source": [
    "- `model`: The model you want to use. In this case, it‚Äôs LLaMA 3.3 70B Instruct, hosted via NIM.\n",
    "- `messages`: Follows the Chat Markup Language (ChatML) format - like ChatGPT- where you provide a list of role-based messages. You can have:\n",
    "    - role: \"system\" ‚Äì sets overall behavior\n",
    "    - role: \"user\" ‚Äì the actual input or question\n",
    "    - role: \"assistant\" ‚Äì prior responses (for context)\n",
    "- `temperature`: Controls randomness. Lower = more deterministic.\n",
    "- `top_p`: Controls nucleus sampling (alternative to temperature).\n",
    "- `max_tokens`: The maximum number of tokens in the output.\n",
    "- `stream=True`: Enables streaming responses (partial chunks instead of waiting for the full response)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c703e-e19b-4458-aff1-7909a9eece6b",
   "metadata": {},
   "source": [
    "### Handle & Print Streaming Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ec807-0724-4b92-8122-aa089f94724f",
   "metadata": {},
   "source": [
    "The below loops through each streamed chunk of text as it arrives.\n",
    "- `chunk.choices[0].delta.content`: Contains just the new token/word from the model.\n",
    "- `print(..., end=\"\")`: Prints the text incrementally without adding a newline, creating a fluid chat experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e1fb9-af72-427d-a039-9c39f1d8a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac8c4b-702d-4096-a97e-9a15244efbe5",
   "metadata": {},
   "source": [
    "---\n",
    "This script:\n",
    "1. Uses the OpenAI SDK\n",
    "2. Points it to NVIDIA NIM‚Äôs OpenAI API compatible endpoint\n",
    "3. Requests a streamed chat response from `LLaMA 3.3 70B`\n",
    "4. Outputs the text live as it‚Äôs generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce52876-063e-4aef-992b-1a8401df4068",
   "metadata": {},
   "source": [
    "## Try It Yourself: Explore Another Chat Model from the NIM Catalog\n",
    "You‚Äôve just used [meta/llama-3.3-70b-instruct](https://build.nvidia.com/meta/llama-3_3-70b-instruct) for a streaming chat interaction. Now it‚Äôs your turn to explore the [NIM catalog](https://build.nvidia.com/search?q=Reasoning) and run a different model!  \n",
    "\n",
    "Find a chat-capable model, such as:\n",
    "- [Deepseek-R1](https://build.nvidia.com/deepseek-ai/deepseek-r1)\n",
    "- [Qwen2.5 7B Instruct](https://build.nvidia.com/qwen/qwen2_5-7b-instruct)\n",
    "- [Llama-3.1 Nemotron Ultra 253B](https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1)\n",
    "\n",
    "Update the below field with your chosen model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bb8dc-87b1-4556-898c-89767b477d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    \n",
    "  model=\"\",\n",
    "    \n",
    "  messages=[{\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"Tell me about Dumbledore.\"\n",
    "  }],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
