{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841b4dfd",
   "metadata": {},
   "source": [
    "# NVIDIA-Pipecat Automatic Speech Recognition Basics\n",
    "\n",
    "The RivaASRService provides streaming speech recognition using NVIDIA’s Riva ASR models. It supports real-time transcription with interim results and interruption handling.\n",
    "\n",
    "## Setup and Prerequisites\n",
    "Before running this notebook, make sure you have:\n",
    "- An NVIDIA API key for accessing cloud-hosted models via NVCF: [build.nvidia.com](build.nvidia.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c3fa1-bbbb-4502-a113-6acd3554877e",
   "metadata": {},
   "source": [
    "## Setup Environment and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0823bb-ab98-4abf-9849-7779418111d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your NVIDIA API key:  ········\n"
     ]
    }
   ],
   "source": [
    "#Enter Your NVIDIA API KEY\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faeacea7-96dd-40fb-97cd-1beda2c12b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pipecat.frames.frames import EndFrame, TTSSpeakFrame\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineTask\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "from nvidia_pipecat.services.riva_speech import RivaASRService, RivaTTSService\n",
    "from pipecat.transports.local.audio import LocalAudioTransport, LocalAudioTransportParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa08fbe-a2cc-407b-93e2-551aadbb1318",
   "metadata": {},
   "source": [
    "## Transcription with Riva ASR\n",
    "**ASR** takes an audio stream or audio buffer as input and returns one or more text transcripts, along with additional optional metadata. Speech recognition in Riva is a GPU-accelerated compute pipeline, with optimized performance and accuracy.  \n",
    "\n",
    "Riva provides state-of-the-art OOTB (out-of-the-box) models and pipelines for multiple languages, like English, Spanish, German, Russian and Mandarin, that can be easily deployed with nvidia-pipecat.  \n",
    "\n",
    "Now, let's generate a transcript using Riva ASR Service for a sample audio clip, starting with English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3dd1f7d-b4c9-43f7-9675-1c9c59010ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the RivaASRService\n",
    "stt = RivaASRService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), # set API Key\n",
    "    voice_id= \"English-US.Female-1\",  # define the voice\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc34d0c-2791-4cc8-b2e1-db75cbecf59e",
   "metadata": {},
   "source": [
    "### Offline recognition for English\n",
    "You can use Riva ASR in either **streaming** mode or **offline** mode. In streaming mode, a continuous stream of audio is captured and recognized, producing a stream of transcribed text.  \n",
    "In offline mode, an audio clip of a set length is transcribed to text. Riva ASR supports .wav files in pulse-code modulation (PCM) format; including .alaw, .mulaw, and .flac formats.\n",
    "\n",
    "Now, let's make a gRPC request to the Riva Speech server for ASR with a sample .wav file in offline mode. Start by loading the audio.\n",
    "Let's look at an example showing offline ASR for an English audio clip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fdbc8c-aca2-41a3-b7fd-a63a485322a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/avasquez/Developer/nvidia-pipecat-notebooks/notebooks/1-Foundations of Digital Human Agents\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be304f3-1905-4c0e-b9bc-1d50af03c9ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio_samples/en-Mark_Neutral.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This example uses a .wav file with LINEAR_PCM encoding.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# read in an audio file from local disk\u001b[39;00m\n\u001b[32m      3\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33maudio_samples/en-Mark_Neutral.wav\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[32m      5\u001b[39m     content = fh.read()\n\u001b[32m      6\u001b[39m ipd.Audio(path)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'audio_samples/en-Mark_Neutral.wav'"
     ]
    }
   ],
   "source": [
    "# This example uses a .wav file with LINEAR_PCM encoding.\n",
    "# read in an audio file from local disk\n",
    "path = \"./audio_samples/en-Mark_Neutral.wav\"\n",
    "with io.open(path, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745d9d44-11e6-44b5-a525-cdd02a1fd62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-01 11:00:04.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#0 -> RivaTTSService#0\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:04.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking RivaTTSService#0 -> LocalAudioOutputTransport#0\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:04.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioOutputTransport#0 -> PipelineSink#0\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:04.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#0 -> Pipeline#0\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:04.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#0 -> PipelineTaskSink#0\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:04.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 started running PipelineTask#0\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:05.754\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnvidia_pipecat.services.riva_speech\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m172\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [Hello there, how is it going!]\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:06.002\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_started_speaking\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mBot started speaking\u001b[0m\n",
      "\u001b[32m2025-05-01 11:00:08.061\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 finished running PipelineTask#0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    transport = LocalAudioTransport(LocalAudioTransportParams(audio_out_enabled=True))\n",
    "\n",
    "    pipeline = Pipeline([stt, transport.output()]) # We define our RivaTTS Service in the Pipeline\n",
    "\n",
    "    task = PipelineTask(pipeline)\n",
    "\n",
    "    async def say_something():\n",
    "        await asyncio.sleep(1)\n",
    "        await task.queue_frames([TTSSpeakFrame(message), EndFrame()])\n",
    "\n",
    "    runner = PipelineRunner(handle_sigint=False if sys.platform == \"win32\" else True)\n",
    "\n",
    "    await asyncio.gather(runner.run(task), say_something())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
