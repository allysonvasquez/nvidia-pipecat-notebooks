{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11050252-e275-4676-ba64-d7f46d58a9b6",
   "metadata": {},
   "source": [
    "# Digital Human Pipelines with Pipecat and ACE Controller\n",
    "\n",
    "This course is built around NVIDIA’s open-source Pipecat framework, called nvidia-pipecat, and the ACE Controller microservice. This course provides an end-to-end guide to building intelligent, interactive digital humans pipelines.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the core concepts behind the nvidia-pipecat framework and ACE Controller microservice.\n",
    "- Explore basics of applications it supports (voice assistants, avatars, and agents).\n",
    "\n",
    "---\n",
    "\n",
    "## What Does it Take to Bring a Digital Human to Life? \n",
    "\n",
    "> TODO FLESH OUT\n",
    "The end-to-end Digital Human Pipeline is a tightly integrated **pipeline of services and systems** that work together in real time to perceive input, generate intelligent responses, and express those responses through voice and animation.\n",
    "\n",
    "Below is a high-level architecture diagram showing a **typical end-to-end avatar workflow**. This includes speech input and output, AI reasoning, context management, animation rendering, and transport layers for real-time interaction.\n",
    "![diagram](../../docs/images/dht-agent-pipeline.png)\n",
    "\n",
    "Let’s break down the key components, then look at how this works in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Pipecat Framework\n",
    "### Overview\n",
    "[Pipecat](https://github.com/pipecat-ai/pipecat) is an open source Python framework for building real-time, multimodal AI applications. It streamlines the development of pipelines that orchestrate complex interactions across AI services, network transport, audio processing, and multimodal user interfaces.\n",
    "\n",
    "The [nvidia-pipecat](https://github.com/NVIDIA/ace-controller/tree/main) library builds on the Pipecat framework by adding a suite of NVIDIA-developed frame processors, multimodal data types, and services tailored for creating intelligent avatar-based interactions. This includes integration with powerful NVIDIA technologies such as Riva (for ASR and TTS), Audio2Face (for real-time facial animation), and Foundational RAG (for retrieval-augmented generation).\n",
    "\n",
    "In addition to connecting these services, nvidia-pipecat enhances the end-user experience through new processors that support speculative speech—enabling conversational agents to respond more quickly by processing stable interim speech results.\n",
    "\n",
    "While pipecat and nvidia-pipecat give you the building blocks for creating multimodal AI agents, ACE Controller is the orchestration layer that makes these pipelines production-ready.\n",
    "\n",
    "[ACE Controller](https://docs.nvidia.com/ace/ace-controller-microservice/1.0/index.html#ace-controller-microservice) wraps the Pipecat ecosystem in a scalable FastAPI microservice. It lets developers deploy voice-enabled digital humans (and other agents) that can handle multiple users, support RTSP audio/video input, and connect to NVIDIA ACE microservices such as:\n",
    "\t•\tRiva (Speech Recognition and Synthesis)\n",
    "\t•\tAudio2Face (Real-time facial animation)\n",
    "\t•\tAnimation Graph, Video Storage Toolkit (VST), and more\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197dff59-dbb0-4916-9d42-2a42751e8c08",
   "metadata": {},
   "source": [
    "## Understanding how Pipecat works: frames, processors, and pipelines.\n",
    "Pipecat’s core innovation is **real-time streaming**: rather than waiting for a complete sentence or an entire audio file, Pipecat immediately ingests and processes tiny chunks of data (called **frames**) the moment they arrive.  \n",
    "\n",
    "NVIDIA-Pipecat builds on this way of streaming to allow for low-latency digital human behaviors like speculative speech (the avatar begins formulating a reply before you finish talking) and synchronized lip-sync on an animated digital human.\n",
    "\n",
    "Below are the three fundamental building blocks you’ll assemble in every application within this course.\n",
    "\n",
    "---      \n",
    "Think of a **Frame** as a way to move data through your application. Just like packages on a conveyor belt - each Frame contains a specific type of cargo.  \n",
    "For example a Frame can be:\n",
    "\n",
    "| Example Frame Type | Typical Payload                    |\n",
    "|--------------------|------------------------------------|\n",
    "| `AudioFrame`       | User audio data from a microphone    |\n",
    "| `TextFrame`        | Partial or final transcript        |\n",
    "| `LLMResponseFrame` | LLM tokens as they stream in |\n",
    "| `TTSRawAudioFrame` | Synthesised speech samples         |\n",
    "| `EndFrame`         | A Control signal: “we’re done”       |\n",
    "\n",
    "Frames flow **downstream** (the normal left-to-right processing direction) or **upstream** (for control signals, cancellations, or error handling).\n",
    "\n",
    "---\n",
    "A **FrameProcessor** is a small, self-contained component that:\n",
    "\n",
    "1. **Consumes** only the frame types it knows how to handle.  \n",
    "2. **Yields** zero, one, or many new frames (think of splitting, transforming, or aggregating frame data).  \n",
    "3. **Passes through** any other frames unchanged, so they continue on to the next processor.\n",
    "\n",
    "Typical processors you’ll implement will be for the following use cases:\n",
    "| Processor kind         | Input frame(s)           | Output frame(s)            | Example use case                        |\n",
    "|------------------------|--------------------------|----------------------------|-----------------------------------------|\n",
    "| **Speech-to-Text (ASR)**   | `AudioFrame`             | `TextFrame`                | Convert mic audio into transcript text  |\n",
    "| **Language Model (LLM)**   | `TextFrame`              | `LLMResponseFrame`         | Generate conversational reply tokens    |\n",
    "| **Text-to-Speech (TTS)**   | `TextFrame`              | `TTSRawAudioFrame`         | Synthesize speech from reply text       |\n",
    "| **Logger / Observer**      | any frame                | same frame (side-effect)   | Write debug logs or metrics             |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "A **Pipeline** is simply an ordered list of processors. Frames enter at the head of the pipeline and exit at the tail. Under the hood, Pipecat schedules each processor asynchronously and manages everything so that slow components don’t overflow memory.\n",
    "\n",
    "Below is an example of an end-to-end pipeline:\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "    transport.input(),   # mic frames in\n",
    "    stt,                 # ASR\n",
    "    llm,                 # logic & reasoning\n",
    "    tts,                 # TTS\n",
    "    transport.output()   # play audio\n",
    "])\n",
    "```\n",
    "- `transport.input()` and `transport.output()` are special **Processors** that interface with the outside world (keyboard, microphone, speaker, WebSocket, etc.). \n",
    "- Between them, each AI-service processor transforms one frame type into the next.\n",
    "- When an `EndFrame` appears, the pipeline knows to cleanly shut down.\n",
    "\n",
    "\n",
    "**Why This Matters**\n",
    "\t•\tLow latency: Your Digital Human can begin responding before you’ve even finished speaking.\n",
    "\t•\tModularity: You can swap in any ASR, LLM, or TTS processor without changing the pipeline wiring.\n",
    "\t•\tScalability: The same pipeline code can run on your laptop, in a Docker container, or inside ACE Controller on Kubernetes.\n",
    "\n",
    "With this mental model in place - you’re ready to build and debug your first Pipecat application. In Module 2, we’ll replace our dummy processors with real NVIDIA Riva ASR/TTS services and explore applications of end-to-end speech latency.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b23d3-9243-4319-bad0-3db2659f5898",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 🗣️ Automatic Speech Recognition (ASR)\n",
    "\n",
    "- Captures the user's spoken input and transcribes it into text.\n",
    "- Typically runs continuously with **Voice Activity Detection (VAD)**.\n",
    "- Powers the understanding layer for voice-driven interaction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a53b94c-08fb-4695-bacc-6cd5bd739bb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RivaTTSService' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This cell builds the *smallest* possible Pipecat pipeline:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#   TTSSpeakFrame  ─▶  RivaTTSService  ─▶  LocalAudioTransport\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 1)  Configure the Riva TTS processor\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tts = \u001b[43mRivaTTSService\u001b[49m(\n\u001b[32m     10\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mNVIDIA_API_KEY\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;66;03m# set API Key\u001b[39;00m\n\u001b[32m     11\u001b[39m     voice_id= \u001b[33m\"\u001b[39m\u001b[33mEnglish-US.Female-1\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# define the voice\u001b[39;00m\n\u001b[32m     12\u001b[39m     )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 2)  Editable message (rerun the cell after you change it)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[32m     16\u001b[39m message=\u001b[33m\"\u001b[39m\u001b[33mHello there, how is it going!\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'RivaTTSService' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fb52fdd-3e5d-48cc-83a7-1a47a42d78da",
   "metadata": {},
   "source": [
    "#### 🔊 Text-to-Speech (TTS)\n",
    "\n",
    "- Converts the generated text response into natural-sounding audio.\n",
    "- Many pipelines use expressive TTS models to convey tone, mood, or personality.\n",
    "- This audio is also used to drive facial animation (via visemes or blendshape data).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb65823-a683-45b5-999a-16784c82e2b6",
   "metadata": {},
   "source": [
    "#### 🧠 Chat and Behavior Logic (LLM / RAG / Agents)\n",
    "\n",
    "- The **core “brain”** of the agent.\n",
    "- Most pipelines today use **LLMs** like GPT, LLaMA, or Claude to generate intelligent responses.\n",
    "- Many enhance LLMs with **RAG** (Retrieval-Augmented Generation) to ground replies in custom knowledge.\n",
    "- Increasingly, **Agent frameworks** are layered on top to enable tool use, memory, multi-step workflows, and reasoning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2092450-fd2a-4d38-b732-164dd4cbabf3",
   "metadata": {},
   "source": [
    "#### 🎭 Animation & Expression\n",
    "\n",
    "- Controls the **facial expressions, lip sync, eye gaze, and body posture** of the avatar.\n",
    "- Powered by services like **Audio2Face (A2F)** and **AnimGraph**.\n",
    "- The fidelity of animation depends on the quality of the avatar rig and how tightly it is integrated with the audio and behavior pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db51ce3-f6be-45dc-91e1-f3831241f0ad",
   "metadata": {},
   "source": [
    "#### nvidia-pipecat Enhancements\n",
    "\n",
    "| Feature                  | Description |\n",
    "|--------------------------|-------------|\n",
    "| **Context Aggregators**  | Retain history, memory, user preferences, or scene awareness. |\n",
    "| **Proactivity**          | Lets agents take initiative (e.g., interrupt, remind, assist). |\n",
    "| **Multimodal Input**     | Supports vision, gesture, or touch in addition to speech. |\n",
    "| **Tool Calling**         | Allows the agent to interact with APIs, databases, or simulations. |\n",
    "\n",
    "---\n",
    "\n",
    "Each component is a building block that we'll be incrementally deploying in each module. Together, they form a **modular architecture** that powers engaging, intelligent, and believable digital humans.\n",
    "\n",
    "> Each section will break down these components and incrementally piece them together so that by the end you can develop and deploy your own Digital Human Pipeline for any use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3af314-c1ff-4fdd-b9fc-4719f6678395",
   "metadata": {},
   "source": [
    "**Next up (Module 2)**: We’ll dive deep into **voice agents**—configuring Riva ASR/TTS and a simple voice agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38728436-d09b-41a6-984d-96e58cbdee8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
