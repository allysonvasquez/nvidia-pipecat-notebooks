{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7c2f39",
   "metadata": {},
   "source": [
    "# Module 1.1: Pipecat Core Concepts & Your First Pipeline\n",
    "\n",
    "In this module, we introduce how to stream responses from a large language model (LLM) using NVIDIA Pipecat. The goal is to build foundational familiarity with Pipecat and its extension, `nvidia-pipecat`, which simplifies integration with NVIDIA's multimodal and avatar pipelines.\n",
    "\n",
    "This notebook focuses on the text-only interaction path using the `NvidiaLLMService`, which wraps access to NIM chat-capable models such as LLaMA 3.3. In later modules, this will be extended into speech-to-speech pipelines, real-time animation, and digital human interfaces.\n",
    "\n",
    "## What is Pipecat?\n",
    "\n",
    "Pipecat is an asynchronous dataflow framework for building real-time pipelines. It lets developers define and connect streaming components like ASR, LLMs, TTS, and more using a modular and testable architecture.\n",
    "\n",
    "Key Pipecat concepts include:\n",
    "\n",
    "- **Processors**: building blocks that transform, generate, or route data.\n",
    "- **Frames**: units of data (e.g., text, audio) passed between processors.\n",
    "- **Pipelines**: ordered sequences of processors.\n",
    "- **PipelineTasks**: runnable instances of pipelines with per-session metadata.\n",
    "- **PipelineRunner**: orchestrates execution of multiple concurrent tasks.\n",
    "\n",
    "`nvidia-pipecat` extends Pipecat with services and helpers for working with NVIDIA-hosted endpoints (e.g., NIMs), transcript synchronization, avatar controllers, and multi-modal transport layers (e.g., WebRTC or ACE Transport).\n",
    "\n",
    "## Objective of This Notebook\n",
    "\n",
    "You will:\n",
    "- Understand Pipecat's core architectural principles: real-time streaming, frames, processors, and pipelines, based on the official Pipecat Core Concepts.\n",
    "- Implement a basic `FrameProcessor`.\n",
    "- Construct, run, and push data through a simple text-based Pipecat `Pipeline`.\n",
    "- Define an LLM context and stream outputs from the `NvidiaLLMService` using `nvidia-pipecat`.\n",
    "- Build a minimal streaming chatbot loop that maintains conversation history.\n",
    "\n",
    "This sets the stage for integrating text responses with downstream services such as TTS and animation in future modules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-title-objective-v3",
   "metadata": {},
   "source": [
    "# Module 1.1: Pipecat Core Concepts & Your First Pipeline\n",
    "\n",
    "Welcome to your hands-on introduction to **Pipecat**! In Module 1.0, we discussed the high-level architecture of Digital Humans. Now, we'll explore the fundamental building blocks of Pipecat, the open-source Python framework designed for real-time, streaming AI applications. This notebook will guide you through Pipecat's core concepts and help you build your very first pipeline.\n",
    "\n",
    "Understanding these base Pipecat mechanics is essential before we move on to using `nvidia-pipecat` to integrate powerful NVIDIA AI technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d9665-7af0-4e7f-985e-af54ecc2ef19",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Python environment set up as per `0-0-Environment-Setup-Guide.md`.\n",
    "- Jupyter kernel `nv-pipecat-env` selected.\n",
    "\n",
    "Prior to getting started, you will need an NVIDIA API Key from the NVIDIA API Catalog to access the models used in this notebook.  \n",
    "\n",
    "Need an API Key? It's Free!\n",
    "  1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**.\n",
    "  2. Select any model, such as `llama-3.3-70b-instruct`.\n",
    "  3. On the right panel above the sample code snippet, click on \"Get API Key\". This will prompt you to log in if you have not already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430720f9-9b83-4c8b-bed6-cfe99a60b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply() # For running asyncio in Jupyter\n",
    "\n",
    "load_dotenv() # Load environment variables from a .env file if available\n",
    "\n",
    "# Try to get the API key from the environment\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "# Prompt if not set or invalid\n",
    "if not api_key or not api_key.startswith(\"nvapi-\"):\n",
    "    print(\"NVIDIA API key not found or invalid.\")\n",
    "    api_key = getpass.getpass(\"üîê Enter your NVIDIA API key: \").strip()\n",
    "    if not api_key.startswith(\"nvapi-\"):\n",
    "        raise ValueError(f\"{api_key[:5]}... is not a valid NVIDIA API key\")\n",
    "    # Set in environment for the current session\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-pipecat-core-concepts-intro",
   "metadata": {},
   "source": [
    "## Pipecat Core Concepts: Frames, Processors, and Pipelines\n",
    "\n",
    "Pipecat uses a pipeline-based architecture to handle real-time AI processing. Instead of waiting for complete responses at each step, Pipecat processes data in small units called **frames** that flow through the pipeline. This streaming approach is key to creating natural, responsive interactions.\n",
    "\n",
    "*(The following explanations are adapted from the official Pipecat Core Concepts guide).*\n",
    "\n",
    "### Real-time Processing in Action\n",
    "Consider a voice assistant:\n",
    "- Speech is transcribed in real-time as the user speaks.\n",
    "- Transcription is sent to an LLM as it becomes available.\n",
    "- LLM responses are processed as they stream in.\n",
    "- Text-to-speech begins generating audio for early sentences while later ones are still being generated.\n",
    "This creates a fluid, low-latency experience.\n",
    "\n",
    "### Architecture Overview\n",
    "Pipecat organizes these processes using three key components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-pipecat-frames-official",
   "metadata": {},
   "source": [
    "### 1. Frames: The Data Containers\n",
    "Frames are chunks of data moving through your application. Examples include:\n",
    "\n",
    "- Audio data from a microphone\n",
    "- Text from transcription\n",
    "- LLM responses\n",
    "- Generated speech audio\n",
    "- Images or video\n",
    "- Control signals (e.g., `EndFrame`, `UserStartedSpeakingFrame`)\n",
    "\n",
    "Frames can flow **downstream** (normal processing) or **upstream** (for errors and control signals).\n",
    "\n",
    "**Key Frame Types (from `pipecat.frames.frames` and `nvidia_pipecat.frames.riva` we'll use initially):**\n",
    "\n",
    "| Frame Type         | Description                                                  |\n",
    "|--------------------|--------------------------------------------------------------|\n",
    "| `TextFrame`        | A string of text.                                            |\n",
    "| `TranscriptionFrame`| Contains speech-to-text results                             |\n",
    "| `TTSAudioRawFrame`  | Contains audio data from text-to-speech (bytes).            |\n",
    "| `ActionFrame`  | Represents actions to be performed by an agent.            |\n",
    "| `EndFrame`         | Control signal indicating the end of a stream or processing. |\n",
    "*(We'll introduce more specialized frames as needed.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-imports-initial-v3",
   "metadata": {},
   "source": [
    "### Initial Imports\n",
    "Let's import the necessary Pipecat components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "m1-1-code-imports-v3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pipecat.frames.frames import Frame, TextFrame, EndFrame, StartFrame\n",
    "from pipecat.observers.base_observer import BaseObserver\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineTask, PipelineParams, FrameDirection\n",
    "from pipecat.processors.aggregators.sentence import SentenceAggregator\n",
    "from pipecat.processors.frame_processor import FrameProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-pipecat-processors-official",
   "metadata": {},
   "source": [
    "### 2. Processors (Services)\n",
    "`Frame processors` are the building blocks of a pipeline. They are responsible for transforming or responding to the data (or Frames) flowing through the system. They can perform a variety of tasks, such as:\n",
    "\t‚Ä¢\tSimple operations like joining partial transcripts into full sentences\n",
    "\t‚Ä¢\tInvoking AI services, such as an LLM that generates a response from a message array\n",
    "\t‚Ä¢\tConverting text into other modalities, such as audio or images\n",
    "\n",
    "If a frame processor encounters a frame it doesn‚Äôt need to handle, it can pass the frame along the pipeline unchanged. This allows downstream processors to operate on it and helps maintain a smooth, uninterrupted flow through the pipeline.\n",
    "\n",
    "Implementing a single processor, like the `SentenceAggregator` looks like:\n",
    "\n",
    "```python\n",
    "from pipecat.processors.aggregators.sentence import SentenceAggregator  # Example of a processor \n",
    "  \n",
    "# This processor aggregates text frames into a complete sentence: \"Hello\" \"World\" -> \"Hello World\"\n",
    "aggregator = SentenceAggregator()\n",
    "```\n",
    "\n",
    "You can use predefined Processors, or create a custom one to fit your logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-pipecat-pipelines-official",
   "metadata": {},
   "source": [
    "### 3. Pipelines - The Assembly Line\n",
    "We can connect multiple processors together within a `Pipeline`. Pipelines connect multiple processors together, creating a path for frames to flow. Pipecat supports simple linear pipelines as well as more complex parallel ones (though we'll start with linear).\n",
    "\n",
    "```python\n",
    "# Simple linear pipeline structure\n",
    "pipeline = Pipeline([\n",
    "    Processor1(), # ex: aggregator\n",
    "    Processor2(),\n",
    "    # ... and so on\n",
    "])\n",
    "```\n",
    "The pipeline often includes a **transport** at the beginning and/or end, which is the connection to the real world (microphone, speakers, WebSocket)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-running-pipelines-v3",
   "metadata": {},
   "source": [
    "## Running a Pipecat Pipeline: Tasks and Runners\n",
    "\"tasks\" and \"runners\" are core components of the pipeline architecture that manage the execution of AI communication pipelines.\n",
    "\n",
    "1.  **`Pipeline`**: An instance of your defined pipeline (list of processors).\n",
    "2.  **`PipelineTask`**: Connects to a pipeline using source and sink processors, which allow it to push frames into the pipeline and receive frames from it.\n",
    "3.  **`PipelineRunner`**: Manages the execution of one or more `PipelineTask` instances within an `asyncio` event loop.\n",
    "\n",
    "Let's create and run our first pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-lab-title-hands-on-v3",
   "metadata": {},
   "source": [
    "## Building a Simple Pipeline\n",
    "\n",
    "We'll construct a pipeline: `String input -> Sentence Aggregator -> Output`.\n",
    "We'll manually push `TextFrame`s to demonstrate the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-lab-output-explanation-v3",
   "metadata": {},
   "source": [
    "Let's piece everything together and create a simple pipeline that aggregates text:\n",
    "\n",
    "### Setting Up the Pipeline - Create an Observer\n",
    "First, we need a way to *see* the aggregated sentence once the Pipeline task is completed. Here, we define a simple `Observer` that listens for frames as they are pushed between processors. This prints out the aggregated text after they've been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075bc3d1-0a15-4dcb-b2ad-8eed11b063bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FramePrinter(BaseObserver):\n",
    "    async def on_push_frame(self, src: FrameProcessor, dst: FrameProcessor, frame: Frame, direction: FrameDirection, timestamp: int):\n",
    "        \n",
    "        # Only print frames coming from the SentenceAggregator\n",
    "        if isinstance(frame, TextFrame) and isinstance(src, SentenceAggregator):\n",
    "            \n",
    "            print(f\"Aggregated sentence: {frame.text}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754c85e-21c9-4593-af26-03b36090b8be",
   "metadata": {},
   "source": [
    "### Step 3: Define the Pipeline Task\n",
    "\n",
    "Now we build a simple pipeline using a `SentenceAggregator`, which joins text fragments into full sentences. We attach our `FramePrinter` as an observer so we can see the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4ae66b-cff2-4b77-9e79-8e451a84d0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "async def test_simple_pipeline():\n",
    "    \n",
    "    aggregator = SentenceAggregator() # Create a sentence aggregator processor\n",
    "\n",
    "    \n",
    "    pipeline = Pipeline([aggregator]) # Create a pipeline with our aggregator processor\n",
    "\n",
    "    # Create a pipeline task with our Observer\n",
    "    task = PipelineTask(\n",
    "        pipeline,\n",
    "        params=PipelineParams(observers=[FramePrinter()])\n",
    "    )\n",
    "    \n",
    "    runner = PipelineRunner() # We use PipelineRunner to execute the pipeline.\n",
    "\n",
    "    run_task = asyncio.create_task(runner.run(task)) # Start the pipeline by running it in the background\n",
    "\n",
    "    await asyncio.sleep(0.1) # Give a little time for the pipeline to initialize\n",
    "\n",
    "    # Queue a few text frames to simulate user input, and end the stream with an EndFrame.\n",
    "    await task.queue_frame(TextFrame(\"Hello, \"))\n",
    "    await task.queue_frame(TextFrame(\"world!\"))\n",
    "\n",
    "    # End the pipeline\n",
    "    await task.queue_frame(EndFrame())\n",
    "\n",
    "    # Wait for the pipeline to complete. The result is printed by the Observer.\n",
    "    await run_task\n",
    "    print(\"Pipeline execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3fb82-1c40-40d2-a87a-548daf9e52e8",
   "metadata": {},
   "source": [
    "\n",
    "### Execute the Pipeline\n",
    "\n",
    "We now run the function that initializes and runs our sample pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ef6c16-0845-45f9-8caf-6f4c182a5fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 14:14:33.153\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#0 -> SentenceAggregator#0\u001b[0m\n",
      "\u001b[32m2025-05-14 14:14:33.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking SentenceAggregator#0 -> PipelineSink#0\u001b[0m\n",
      "\u001b[32m2025-05-14 14:14:33.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#0 -> Pipeline#0\u001b[0m\n",
      "\u001b[32m2025-05-14 14:14:33.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#0 -> PipelineTaskSink#0\u001b[0m\n",
      "\u001b[32m2025-05-14 14:14:33.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 started running PipelineTask#0\u001b[0m\n",
      "\u001b[32m2025-05-14 14:14:33.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 finished running PipelineTask#0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated sentence: Hello, world!\n",
      "Pipeline execution completed\n"
     ]
    }
   ],
   "source": [
    "# Run our test pipeline\n",
    "await test_simple_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd7ace-eedc-4aef-8afe-7768a235ecce",
   "metadata": {},
   "source": [
    "Lets visualize the Data Flow:  \n",
    "![viz](../../../docs/images/aggregator-nvidia-pipecat-flow.png)\n",
    "\n",
    "| Component                    | Purpose                                               |\n",
    "|-----------------------------|--------------------------------------------------------|\n",
    "| `TextFrame(\"Hello, \")`      | Raw text input to the pipeline (a frame)                         |\n",
    "| `TextFrame(\"world!\")`       | Another input frame                                 |\n",
    "| `SentenceAggregator`        | Processor that joins fragments into one sentence                      |\n",
    "| `TextFrame(\"Hello, world!\")`| The processed output frame                             |\n",
    "| `FramePrinter` (observer)   | Monitors the output and logs it to the console         |\n",
    "| Console Output              | Where you see the final result                         |\n",
    "\n",
    "This pattern is fundamental in Pipecat: data flows through processors, and observers can monitor the flow at any point without interfering with it. You‚Äôll see this same logic apply in more advanced pipelines that include ASR, TTS, LLMs, and avatar animation.\n",
    "\n",
    "We strongly recommend getting comfortable with pipecat first, leveraging their documentation and example pipelines to get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-nvidia-pipecat-context-v3",
   "metadata": {},
   "source": [
    "## Next: What is ACE Controller and `nvidia-pipecat`?\n",
    "\n",
    "NVIDIA-pipecat is an extension of the open-source Pipecat framework that adds NVIDIA-specific capabilities for creating real-time, multimodal conversational AI agents with a focus on avatar interactions. It's part of the ACE Controller SDK, which allows developers to build services that manage multimodal, real-time interactions with voice bots and avatars using NVIDIA ACE.\n",
    "\n",
    "`nvidia-pipecat` extends Pipecat in three main areas:\n",
    "1. Additional Frame Processors and Services: Services like NvidiaLLM, Riva, and Audio2Face are added.\n",
    "2. Addition of multimodal frames: Adds support for new frame types for avatar interactions \n",
    "3. HTTP and WebSocket Server Implementation: A FastAPI-based implementation for communication to ACE (Avatar Cloud Engine)\n",
    "\n",
    "nvidia-pipecat is part of the ACE Controller SDK, which allows developers to create controllers that leverage NVIDIA services for voice-enabled and multimodal conversational AI agents.\n",
    "\n",
    "Starting in **Module 2**, we will use these `nvidia-pipecat` services to build a functional voice agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1782ac-c102-40c7-857e-762ea7f6992c",
   "metadata": {},
   "source": [
    "### Building a Basic LLM NIM Pipeline with Llama\n",
    "Now, let's build a simple chat application using nvidia-pipecat and Llama. We'll use the `NvidiaLLMService` class, which provides an interface to NVIDIA's language model endpoints with support for streaming responses and parameter customization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66198dd-104c-47cc-bb59-aa05a8ea28da",
   "metadata": {},
   "source": [
    "The `NvidiaLLMService` class allows us to connect to NVIDIA NIM LLM endpoints. For our example, we'll use the Llama 3.3-70B model:  \n",
    ">*If you are unfamiliar with NIM endpoints, available NIMs, or advanced configurations, see our [NIM-Intro](../0-Setup/NIM-Intro.ipynb) in Module 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a76dc18-403c-43cc-9589-1435bda014d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext  \n",
    "from pipecat.services.ai_services import LLMService  \n",
    "\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed60567c-496c-43c8-b162-78627355c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NVIDIA LLM service with Llama 3.3 70B\n",
    "llm = NvidiaLLMService(  \n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    "    base_url=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff630c-869b-45fd-b4d8-bf9109486d0a",
   "metadata": {},
   "source": [
    "We use nvidia-pipecats `NvidiaLLMService` as our LLM provider to easily interface with NIM endpoints. If you are following along with a locally run NIM, you can set the `base_url` to your local endpoint.\n",
    "\n",
    "Next, lets define a simple system prompt to provide instruction on how the LLM should behave during the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7283f75f-8e01-4d19-b080-b71989e61294",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant. Keep responses short and polite.\",\n",
    "}]\n",
    "\n",
    "context = OpenAILLMContext(messages)\n",
    "context_aggregator = llm.create_context_aggregator(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586384c-2546-49cf-8843-89ed799c2709",
   "metadata": {},
   "source": [
    "The `OpenAILLMContext` manages the conversation history between the system, user, and assistant, acting like a chat memory manager. The `context_aggregator` uses this context and the LLM to convert user input into a full message history and will pass it to the LLM for a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2d768-666b-4f5d-8603-ecb01ba8fd3a",
   "metadata": {},
   "source": [
    "### Create a Simple Chat Function\n",
    "This function creates a simple streaming chat loop. It takes user input, sends it to the LLM with the full message history, streams the response token by token, prints it in real time, and saves the assistant‚Äôs reply to the conversation context. The loop continues until the user types ‚Äúexit‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c28039-66b5-4d10-8748-bc52e9569df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat():\n",
    "    print(\"Streaming LLM Chat (type 'exit' to quit)\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        context.add_message({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        print(\"Assistant: \", end=\"\", flush=True)\n",
    "        response = \"\"\n",
    "\n",
    "        # Pipecat handles streaming\n",
    "        stream = await llm.get_chat_completions(context, context.get_messages())\n",
    "\n",
    "        async for chunk in stream:\n",
    "            if chunk.text():\n",
    "                print(chunk.text(), end=\"\", flush=True)\n",
    "                response += chunk.text()\n",
    "\n",
    "        print()\n",
    "        context.add_message({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d59c1201-5ac0-44e5-913a-3571e0491bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming LLM Chat (type 'exit' to quit)\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hey there!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hello! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  la la la\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: That's a happy tune! Is everything okay?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What's the first word I said?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Your first word was \"Hey\".\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Thanks! Bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Bye! Have a great day!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Run the interactive chat. Type exit to quit\n",
    "await chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acade621-453a-44e3-a138-fd193ff4922e",
   "metadata": {},
   "source": [
    "In this example, we do **not** use a `Pipeline` because the chat function is manually controlling the flow of input and output using the `NvidiaLLMService` directly. This works well for simple, linear use cases like basic text chat, where you just need to collect input, call the LLM, and print the response.\n",
    "\n",
    "You would use a `Pipeline` when you want to connect multiple Services or Processors in a structured, real-time flow ‚Äî such as chaining speech-to-text (ASR), LLM, and text-to-speech (TTS), or adding processors like sentence aggregation, speculative generation, or streaming animation.  \n",
    "\n",
    "Pipelines are especially useful when building scalable or reusable systems like those used in digital human interfaces, where data needs to be transformed and routed through many processors in sequence. We will implement `nvidia-pipeca`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1-1-further-reading-exercises-v3",
   "metadata": {},
   "source": [
    "## Exercises & Further Exploration\n",
    "\n",
    "1. **Build a Custom Frame Processor**  \n",
    "   Create a new processor that uppercases all `TextFrame` contents.  \n",
    "   *Hint: override `process_frame()`.*\n",
    "\n",
    "2. **Chain Multiple Text Transforms**  \n",
    "   Create a pipeline with two processors: one that trims whitespace, and one that adds punctuation.  \n",
    "   Try it with a sentence like `\"hello there \"`.\n",
    "\n",
    "3. **Use Observers to Log Debug Info**  \n",
    "   Attach an observer to print every frame that passes between two processors.  \n",
    "   Can you filter logs to show only frames containing a keyword?\n",
    "\n",
    "4. **Implement a Simple Echo Bot Pipeline**  \n",
    "   Write a Pipecat pipeline that returns every `TextFrame` input as a response.  \n",
    "   Extend it to store previous messages in memory.\n",
    "\n",
    "5. **Swap in the NVIDIA LLM**  \n",
    "   Replace the echo logic with `NvidiaLLMService` using `OpenAILLMContext`.  \n",
    "   Prompt it with custom system instructions and experiment with tone changes.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Documentation to Review\n",
    "\n",
    "- **Pipecat Core Concepts:** [Pipecat GitHub Docs - Core Concepts](https://github.com/pipecat-ai/pipecat/blob/main/docs/concepts.md)  \n",
    "- **Pipecat Processors:** [Pipecat GitHub Docs - Processors](https://github.com/pipecat-ai/pipecat/blob/main/docs/processors.md)  \n",
    "- **NVIDIA ACE Controller User Guide (for `nvidia-pipecat` overview):** [ACE Controller User Guide](https://docs.nvidia.com/ace/ace-controller-microservice/1.0/user-guide.html)\n",
    "\n",
    "---\n",
    "\n",
    "This lesson introduced you to NVIDIA Pipecat and showed how to build a basic LLM NIM pipeline using the Llama 3.3 70B model for simple chat functionality. The `nvidia-pipecat` extension makes it easy to integrate NVIDIA's powerful AI services into your applications, enabling you to create sophisticated conversational agents.\n",
    "\n",
    "The next module will focus on building a complete speech-to-speech voice agents using the services provided by `nvidia-pipecat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c2cb6-07c4-4f27-b748-be918e2713fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
