{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60f00f0-3904-4243-b907-2ad6a6995eda",
   "metadata": {},
   "source": [
    "# Speech-to-Speech Voice Agent with NVIDIA Pipecat\n",
    "\n",
    "Welcome to Module 2! In this notebook, you‚Äôll learn how to build a basic **voice-enabled AI agent** using the NVIDIA Pipecat framework. \n",
    "\n",
    "By the end of this module, you will have a working conversational agent that:\n",
    "- Listens to user speech\n",
    "- Transcribes it into text\n",
    "- Generates an intelligent response\n",
    "- Speaks the response back to the user\n",
    "\n",
    "This notebook focuses on the **fundamental building blocks** for digital humans and intelligent avatars, and introduces the nvidia-pipecat framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d66222-d9d9-4d8d-a78e-b8677f9f8971",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**ACE Controller** is a framework for building advanced conversational agents, built on top of NVIDIA Pipecat. It provides a modular pipeline for connecting speech-to-text (STT), large language models (LLMs), and text-to-speech (TTS) modules, and is designed for real-time, interactive applications.\n",
    "\n",
    "**Goal:** Deploy a basic voice agent that acts as a friendly museum guide, using a FastAPI server and websocket-based communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d9bf4-8429-4fee-bb12-4847730e004b",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Prior to getting started, you will need to create an API Key for the NVIDIA API Catalog for the voice agent.\n",
    "\n",
    "### Obtain API Keys\n",
    "#### NGC API Key\n",
    "- NVIDIA API Catalog\n",
    "  1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/meta/llama-3_3-70b-instruct)**.\n",
    "  2. This will take you to the `llama-3.3-70b-instruct` model.\n",
    "  3. On the right above the sample code snippet, click on \"Get API Key\". This will prompt you to log in if you have not already.\n",
    "\n",
    "### Export API Keys\n",
    "Save these API keys as environment variables in the .env file of this directory.\n",
    "\n",
    "Below will check to see if the NVIDIA API Key is set as an environment variable. If not, it will prompt you to enter the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d63a6-c727-4570-afaa-6a3d52c93068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982fa52-2195-4ad7-8691-bf41791c4429",
   "metadata": {},
   "source": [
    "# Configure ACE Transport for WebSocket Communication\n",
    "\n",
    "In this section, we configure how our Pipecat agent **communicates with a client**.\n",
    "\n",
    "### What's Happening?\n",
    "\n",
    "- A **WebSocket** is a network protocol (like HTTP) that creates a two-way connection between the client and server.\n",
    "- Our Pipecat agent uses this WebSocket to **receive user audio** and **send back AI responses** (like text and synthesized speech) with low latency.\n",
    "- **ACETransport** is the Pipecat transport class responsible for **managing these audio/text events** over the WebSocket.\n",
    "\n",
    "üîπ ACETransport also supports **RTSP input** for streaming audio if needed ‚Äî a feature useful when scaling up to real-time video avatars or remote microphone inputs.\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Pipecat Transports\n",
    "\n",
    "In Pipecat, a **Transport** defines how frames (chunks of data like audio, text, or images) move between the external world and the internal AI pipeline.\n",
    "\n",
    "Different transport types support different connection methods:\n",
    "- WebSocket (for browser or app clients)\n",
    "- RTSP streams (for camera/mic feeds)\n",
    "- Custom transports (for specialized devices)\n",
    "\n",
    "In this notebook, we use **ACETransport** because it easily integrates with other ACE microservices needed for communicating with a digital human.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Try It Yourself:\n",
    "\n",
    "- Change the `vad_audio_passthrough` flag to `False` in `ACETransportParams` and observe how audio streaming changes.\n",
    "- Think about: How would you modify the transport if your client was sending **video frames** instead of just audio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c51db8-927f-4a15-82a1-c243be26d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.audio.vad.silero import SileroVADAnalyzer\n",
    "from nvidia_pipecat.transports.network.ace_fastapi_websocket import ACETransport, ACETransportParams\n",
    "\n",
    "# Transport setup\n",
    "def create_transport(pipeline_metadata):\n",
    "    \n",
    "    return ACETransport(\n",
    "        # Connect the websocket provided by the pipeline\n",
    "        websocket=pipeline_metadata.websocket,  # Active connection between the client and the server\n",
    "        \n",
    "        # Set transport parameters\n",
    "        params=ACETransportParams(\n",
    "            vad_enabled=True,  # Enable Voice Activity Detection (VAD)\n",
    "            vad_analyzer=SileroVADAnalyzer(),   # Use Silero model to detect when the user is speaking\n",
    "            vad_audio_passthrough=True,  # Pass through audio even when VAD is active (does not cut off)\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437c935-f6a4-4660-8320-38759287ffde",
   "metadata": {},
   "source": [
    "# Set Up AI Services: ASR, TTS, and LLM\n",
    "\n",
    "Now that our transport is ready to handle connections, let's set up the **AI services** that will drive our conversational agent.\n",
    "\n",
    "### What's Happening?\n",
    "\n",
    "- **ASR (Automatic Speech Recognition):** Converts user speech into text.  \n",
    "  ‚ûî We use **RivaASRService**, NVIDIA's high-accuracy, low-latency speech-to-text engine.\n",
    "  \n",
    "- **TTS (Text-to-Speech):** Converts the AI‚Äôs text replies into natural-sounding speech.  \n",
    "  ‚ûî We use **RivaTTSService** for flexible, high-quality voice synthesis.\n",
    "\n",
    "- **LLM (Large Language Model):** Generates intelligent text responses based on the conversation history.  \n",
    "  ‚ûî We use **NvidiaLLMService** to access cloud-hosted NIM LLMs like Meta Llama 3.\n",
    "\n",
    "These three services form the **core intelligence and voice** of our digital human agent.\n",
    "\n",
    "---\n",
    "### Understanding Pipecat Services\n",
    "\n",
    "In Pipecat, **Services** are special types of frame processors that:\n",
    "- Take incoming frames (audio or text)\n",
    "- Call an external AI model (like ASR, LLM, TTS)\n",
    "- Output transformed frames (transcripts, responses, synthesized audio)\n",
    "\n",
    "**NVIDIA Pipecat** extends the basic Pipecat framework with ready-made service processors that connect to NVIDIA Riva, Audio2Face, Foundational RAG, and more. For now, this notebook will focus on Riva services.\n",
    "\n",
    "---\n",
    "#### ‚úèÔ∏è Try It Yourself:\n",
    "\n",
    "- Change the **LLM model** name in `NvidiaLLMService` to use a different NIM-hosted model. These can be found at [build.nvidia.com](build.nvidia.com)\n",
    "- Modify the **TTS voice_id** to hear your agent respond with a different voice or accent.\n",
    "- Explore the `language` parameter ‚Äî can you make your agent speak or understand another language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfa884-28cd-4e86-ab7d-b52526fbf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "from nvidia_pipecat.services.riva_speech import RivaASRService, RivaTTSService\n",
    "\n",
    "def create_services():\n",
    "    # Setting up a LLM service\n",
    "    llm = NvidiaLLMService(\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    "        model=\"meta/llama-3.3-70b-instruct\",\n",
    "    )\n",
    "\n",
    "    # Setting up an ASR service\n",
    "    stt = RivaASRService(api_key=os.getenv(\"NVIDIA_API_KEY\"))\n",
    "\n",
    "    # Setting up a TTS service\n",
    "    tts = RivaTTSService(api_key=os.getenv(\"NVIDIA_API_KEY\"))\n",
    "\n",
    "    return llm, stt, tts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227305a-b778-4f6e-b259-fbd89190ab4f",
   "metadata": {},
   "source": [
    "# Define the Services\n",
    "We use ACE for transport, Llama-3.3-70B-Instruct NIM for LLM, Riva for STT & TTS, and Silero for VAD (Voice Activity Detection).. We'll also se a system prompt to make the agent act as a friendly museum guide.\n",
    "\n",
    "we will showcase how to build a simple speech-to-speech voice assistant pipeline using nvidia-pipecat along with the pipecat-ai library and deploy it for testing. This pipeline will use WebSocket-based ACETransport, Riva ASR and TTS models, and NVIDIA LLM Service. It is recommended to first follow the Pipecat documentation or the Pipecat Overview section to understand core concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9569c9f-68eb-4ad9-b665-71c98b5e57ad",
   "metadata": {},
   "source": [
    "### Define LLM Prompt\n",
    "Let's set a basic prompt for the LLM. You can edit the prompt as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a183c507-709f-47d6-a272-59ad8d68560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "You are Lydia; a conversational voice agent who acts as a friendly museum curator. \n",
    "You listen carefully to visitors and answer their questions about the exhibits, collections, and the museum itself. \n",
    "The purpose is to demonstrate natural, open-ended voice conversation.\n",
    "\n",
    "Here is background content to reference in the conversation. Only use the background content provided.\n",
    "\n",
    "BACKGROUND:\n",
    "\n",
    "You work at a prestigious art and history museum. \n",
    "The museum's key exhibits include:\n",
    "  - Ancient civilizations (Egypt, Greece, Rome)\n",
    "  - Renaissance art (Da Vinci, Michelangelo, Botticelli)\n",
    "  - Modern art (Picasso, Matisse, O'Keeffe)\n",
    "  - Natural history (Dinosaurs, fossils, early mammals)\n",
    "  - Technological innovation (early computers, space exploration artifacts)\n",
    "\n",
    "The museum is also known for its interactive experiences, educational programs, and traveling exhibits that rotate every six months.\n",
    "\n",
    "CRITICAL VOICE REQUIREMENTS:\n",
    "\n",
    "Your responses will be converted to audio. \n",
    "Please avoid special characters except for '!' or '?'. \n",
    "Speak clearly and naturally as a professional curator would.\n",
    "\n",
    "RESPONSE REQUIREMENTS:\n",
    "\n",
    "Speaking style:\n",
    "- Keep responses natural, brief, and welcoming\n",
    "- Start with one clear fact or comment related to the visitor's question\n",
    "- Add one or two short supporting details if relevant\n",
    "- Then ask a question to continue the conversation\n",
    "- Never repeat or rephrase information already said\n",
    "- Never restate the visitor's exact words\n",
    "- Avoid filler phrases like also, additionally, furthermore, moreover\n",
    "\n",
    "Example of BAD response (too long):\n",
    "\"Our Ancient Egypt collection includes artifacts from the Old Kingdom, Middle Kingdom, and New Kingdom. You will find funerary masks, canopic jars, and intricate jewelry, many of which were used in religious ceremonies or burial practices. It's fascinating to explore the craftsmanship of the time. Would you like me to recommend a guided tour?\"\n",
    "\n",
    "Example of BAD response (too short):\n",
    "\"We have Egyptian artifacts. Want a tour?\"\n",
    "\n",
    "Example of GOOD response:\n",
    "\"Our Egyptian gallery features burial artifacts from the New Kingdom. Are you more interested in jewelry or tomb relics?\"\n",
    "\n",
    "Natural Acknowledgments:\n",
    "- Use short, professional acknowledgments like \"That's a great question\" or \"Fascinating topic\"\n",
    "- Stay focused on museum content\n",
    "- Avoid emotional support or overly casual phrases like \"No worries\" or \"You're doing great\"\n",
    "\n",
    "Example of BAD acknowledgment:\n",
    "\"That's wonderful! You're asking such great questions.\"\n",
    "\n",
    "Example of GOOD acknowledgment:\n",
    "\"Fascinating topic. Our modern art gallery is one of the most visited. Are you interested in early 20th century works?\"\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "You can:\n",
    "  - Answer questions about the museum exhibits, collections, and programs\n",
    "  - Share interesting facts about art, history, and science based on the background\n",
    "  - Recommend galleries or activities based on visitor interest\n",
    "\n",
    "You cannot:\n",
    "  - Provide information outside of the background content\n",
    "  - Make up exhibits or historical facts\n",
    "\n",
    "INITIAL GREETING:\n",
    "\n",
    "Introduce yourself by saying:\n",
    "\"Hello, I'm Lydia, the curator here. I'm excited to share stories and discoveries from our exhibits. What brings you to the museum today?\"\n",
    "\n",
    "If the visitor introduces themselves, reply with:\n",
    "\"Nice to meet you! Is there a particular exhibit you're most excited to explore?\"\n",
    "\n",
    "If the visitor does not introduce themselves, simply continue the conversation naturally.\n",
    "\"\"\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e53534b-02b8-4bb0-b5f4-3fd9adc7a959",
   "metadata": {},
   "source": [
    "# Initialize the Context Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec77b3-2043-43e0-bdb0-d4fb6f9335de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "def create_context_aggregator(llm_service):\n",
    "    \"\"\"\n",
    "    Set up the LLM conversational context and aggregator.\n",
    "    \"\"\"\n",
    "    \n",
    "    context = OpenAILLMContext(messages)\n",
    "    context_aggregator = llm_service.create_context_aggregator(context)\n",
    "    \n",
    "    return context, context_aggregator, messages  # Note: return messages too for later use!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04490c96-1d86-4f48-9704-eb498981ed8f",
   "metadata": {},
   "source": [
    "# Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca32afb-7485-4d8c-8847-4ce216c72408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.frames.frames import LLMMessagesFrame\n",
    "from pipecat.pipeline.task import PipelineParams, PipelineTask\n",
    "from nvidia_pipecat.pipeline.ace_pipeline_runner import ACEPipelineRunner, PipelineMetadata\n",
    "\n",
    "# Full pipeline task setup\n",
    "async def create_pipeline_task(pipeline_metadata: PipelineMetadata):\n",
    "    \"\"\"\n",
    "    Creates the main speech-to-speech conversational agent pipeline.\n",
    "    \"\"\"\n",
    "    # Create transport\n",
    "    transport = create_transport(pipeline_metadata)\n",
    "\n",
    "    # Create services\n",
    "    llm, stt, tts = create_services()\n",
    "\n",
    "    # Create context and aggregator\n",
    "    context, context_aggregator, messages = create_context_aggregator(llm)\n",
    "\n",
    "    # Define the processing pipeline\n",
    "    pipeline = Pipeline([\n",
    "        transport.input(),\n",
    "        stt,\n",
    "        context_aggregator.user(),\n",
    "        llm,\n",
    "        tts,\n",
    "        transport.output(),\n",
    "        context_aggregator.assistant(),\n",
    "    ])\n",
    "\n",
    "    task = PipelineTask(pipeline)\n",
    "\n",
    "    # Event handler for when client connects\n",
    "    @transport.event_handler(\"on_client_connected\")\n",
    "    async def on_client_connected(transport, client):\n",
    "        messages.append({\"role\": \"system\", \"content\": \"Introduce yourself to the user.\"})\n",
    "        await task.queue_frames([LLMMessagesFrame(messages)])\n",
    "\n",
    "    return task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f0958-2031-4737-b8d9-dfbf7d0ca564",
   "metadata": {},
   "source": [
    "# Launch the FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e829af0-e6a2-4995-9659-8892122a894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from fastapi import FastAPI\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from nvidia_pipecat.transports.services.ace_controller.routers.websocket_router import router as websocket_router\n",
    "\n",
    "\n",
    "# FastAPI app setup\n",
    "app = FastAPI()\n",
    "\n",
    "# Websocket route\n",
    "app.include_router(websocket_router)\n",
    "\n",
    "# Set pipeline runner - Only run ONCE!\n",
    "runner = ACEPipelineRunner(pipeline_callback=create_pipeline_task)\n",
    "\n",
    "# Mount static web client (for connecting users)\n",
    "app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344fa79-3839-4b45-9cac-10533badfee5",
   "metadata": {},
   "source": [
    "The ACEPipelineRunner above should only be run once per session. If changes need to be made, we recommend restarting the kernel using the refresh icon in the toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c184c4-757f-4c66-965c-61a68f669e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-30 16:46:08.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnvidia_pipecat.services.riva_speech\u001b[0m:\u001b[36m__next__\u001b[0m:\u001b[36m561\u001b[0m - \u001b[1mASR service is idle for 30 seconds, terminating active RIVA ASR request...\u001b[0m\n",
      "\u001b[32m2025-04-30 16:46:08.319\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnvidia_pipecat.services.riva_speech\u001b[0m:\u001b[36m_response_handler\u001b[0m:\u001b[36m434\u001b[0m - \u001b[34m\u001b[1mRiva ASR streaming request terminated.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run server (notebook users might skip this and run externally)\n",
    "# Run server within Jupyter Notebook\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Start the Uvicorn server\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894faf6-ed6b-4552-8ba1-ae2206f4d022",
   "metadata": {},
   "source": [
    "Now go to http://localhost:8100/static/index.html\n",
    "\n",
    "Leave this cell running to interface with the web ui."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
