{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841b4dfd",
   "metadata": {},
   "source": [
    "# NVIDIA-Pipecat Text to Speech Basics\n",
    "\n",
    "Welcome to this section of Module 2, where we dive into Text-to-Speech (TTS) integration using NVIDIA Pipecat. You'll learn how to convert text, whether static or dynamically generated by a Large Language Model (LLM), into audible speech in a streaming fashion.\n",
    "\n",
    "The `RivaTTSService` is a key component of the `nvidia-pipecat` library. It leverages NVIDIA's Riva TTS models to provide high-quality speech synthesis. This service is designed for real-time applications, making it ideal for digital humans and voice agents.\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand how `RivaTTSService` processes text frames and generates audio frames within a Pipecat pipeline.\n",
    "- Implement a basic pipeline to synthesize speech from predefined text.\n",
    "- Extend the pipeline to synthesize speech from dynamically generated text using `NvidiaLLMService`.\n",
    "- Explore customization options for voice and language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bb277-420b-463f-90ad-e79a763d569e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before you begin, ensure you have:\n",
    "- Set up your Python environment according to `0-0-Environment-Setup-Guide.md`.\n",
    "- Selected the `nv-pipecat-env` Jupyter kernel.\n",
    "- An NVIDIA API Key from the NVIDIA API Catalog to access the models used in this notebook. This key should be in your `.env` file or you'll be prompted for it.\n",
    "\n",
    "**Need an API Key? It's Free!**\n",
    "1. Navigate to the **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**.\n",
    "2. Select any model (e.g., `meta/llama-3.3-70b-instruct`).\n",
    "3. Click \"Get API Key\" on the model's page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c0823bb-ab98-4abf-9849-7779418111d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faeacea7-96dd-40fb-97cd-1beda2c12b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Audio\n",
    "\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.task import PipelineTask\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.frames.frames import LLMMessagesFrame, TTSSpeakFrame, EndFrame\n",
    "from pipecat.transports.local.audio import LocalAudioTransport, LocalAudioTransportParams\n",
    "\n",
    "from nvidia_pipecat.services.riva_speech import RivaTTSService\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa08fbe-a2cc-407b-93e2-551aadbb1318",
   "metadata": {},
   "source": [
    "## Part 1: Generating Speech from Predefined Text\n",
    "\n",
    "First, we'll create a simple pipeline that takes a fixed string of text, converts it to speech using `RivaTTSService`, and plays it back.\n",
    "\n",
    "### The `RivaTTSService`\n",
    "The `RivaTTSService` is a Pipecat `FrameProcessor` specifically designed for NVIDIA's Riva TTS. Key features include:\n",
    "- **Input:** It primarily processes `TextFrame` or `TTSSpeakFrame`. A `TTSSpeakFrame` is a specialized frame that signals the TTS service to synthesize the contained text.\n",
    "- **Output:** It generates `TTSAudioRawFrame`s, which contain chunks of the synthesized audio, and control frames like `TTSStartedFrame` and `TTSStoppedFrame`.\n",
    "- **Configuration:** You can specify the `api_key` (for cloud models), `voice_id` (to choose different voices and languages), `sample_rate`, and other TTS parameters.\n",
    "\n",
    "Let's define our TTS service instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3dd1f7d-b4c9-43f7-9675-1c9c59010ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our text-to-speech service instance\n",
    "tts_service = RivaTTSService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), \n",
    "    voice_id=\"English-US.Female-1\"  # Example: A standard US English female voice. Explore other voice_ids!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f987c7-4c38-48e3-a642-3664bcb4c0c4",
   "metadata": {},
   "source": [
    "### Building and Running the Static TTS Pipeline\n",
    "We'll define a message and then construct a pipeline to speak it. The `LocalAudioTransport` is used here to play the audio output on your local machine.\n",
    "\n",
    "The pipeline will be: `TTSSpeakFrame (queued manually)` → `RivaTTSService` → `TTSAudioRawFrame (streamed)` → `LocalAudioTransport (output)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9be304f3-1905-4c0e-b9bc-1d50af03c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The message you want the agent to speak. Try changing this!\n",
    "static_message = \"Hello from NVIDIA Pipecat! I can speak this pre defined text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "745d9d44-11e6-44b5-a525-cdd02a1fd62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 15:25:19.311\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#4 -> RivaTTSService#2\u001b[0m\n",
      "\u001b[32m2025-05-14 15:25:19.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking RivaTTSService#2 -> LocalAudioOutputTransport#4\u001b[0m\n",
      "\u001b[32m2025-05-14 15:25:19.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioOutputTransport#4 -> PipelineSink#4\u001b[0m\n",
      "\u001b[32m2025-05-14 15:25:19.313\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#4 -> Pipeline#4\u001b[0m\n",
      "\u001b[32m2025-05-14 15:25:19.313\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#4 -> PipelineTaskSink#4\u001b[0m\n",
      "\u001b[32m2025-05-14 15:25:19.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#4 started running PipelineTask#4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to speak: 'Hello from NVIDIA Pipecat! I can speak this pre defined text.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 15:25:20.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnvidia_pipecat.services.riva_speech\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m172\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [Hello from NVIDIA Pipecat! I can speak this pre defined text.]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message queued for TTS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 15:25:20.659\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_started_speaking\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mBot started speaking\u001b[0m\n",
      "\u001b[32m2025-05-14 15:25:24.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#4 finished running PipelineTask#4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static TTS pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "async def run_static_tts_pipeline():\n",
    "    print(f\"Attempting to speak: '{static_message}'\")\n",
    "    # LocalAudioTransport handles playback of audio frames from TTS.\n",
    "    audio_transport = LocalAudioTransport(LocalAudioTransportParams(audio_out_enabled=True))\n",
    "\n",
    "    # Define the pipeline: TTS service -> Audio output transport\n",
    "    pipeline = Pipeline([tts_service, audio_transport.output()])\n",
    "\n",
    "    # Create a task for this pipeline execution\n",
    "    task = PipelineTask(pipeline)\n",
    "\n",
    "    # This inner function will queue the text to be spoken after the pipeline starts.\n",
    "    async def speak_message():\n",
    "        await asyncio.sleep(1)  # Allow pipeline to initialize\n",
    "        # TTSSpeakFrame signals the TTS service to synthesize the text.\n",
    "        # EndFrame signals the end of input for this task.\n",
    "        await task.queue_frames([TTSSpeakFrame(static_message), EndFrame()])\n",
    "        print(\"Message queued for TTS.\")\n",
    "\n",
    "    runner = PipelineRunner()\n",
    "\n",
    "    # Run the pipeline task and the message queuing concurrently\n",
    "    await asyncio.gather(runner.run(task), speak_message())\n",
    "    print(\"Static TTS pipeline finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await run_static_tts_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7311f-6e32-4b21-b046-c1e0b100903d",
   "metadata": {},
   "source": [
    "Turn up your volume to hear the output! \n",
    "\n",
    "In this example:\n",
    "1. We create a `PipelineTask` for our simple TTS pipeline.\n",
    "2. We manually queue a `TTSSpeakFrame` containing our `static_message` into the task. This frame acts as the input to the `RivaTTSService`.\n",
    "3. The `RivaTTSService` processes this frame, synthesizes speech, and outputs a stream of `TTSAudioRawFrame`s.\n",
    "4. The `LocalAudioTransport` consumes these audio frames and plays them through your speakers.\n",
    "\n",
    "### Exercise:\n",
    "- Modify the `static_message` variable and re-run the cell to hear different outputs.\n",
    "- Change the `voice_id` in the `RivaTTSService` definition. You can find available voices in the Riva or NIM documentation for the TTS service. For example, try `\"English-US.Male-1\"` or explore other languages/accents if available and your API key has access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c02ff-5044-45a9-ad25-04eca9cc827e",
   "metadata": {},
   "source": [
    "## Part 2: Generating Speech from LLM-Generated Text\n",
    "\n",
    "While speaking static text is useful, voice agents typically need to speak dynamically generated content, often from an LLM. Let's enhance our pipeline to include `NvidiaLLMService` for an LLM -> TTS pipeline.\n",
    "\n",
    "### The `NvidiaLLMService`\n",
    "This service (introduced in Module 1.1) connects to NVIDIA NIM LLM endpoints. \n",
    "- **Input:** It expects an `LLMMessagesFrame`, which contains a list of messages (system prompt, user queries, assistant history).\n",
    "- **Output:** It streams `TextFrame` (or `LLMTokenFrame`) objects containing the LLM's response.\n",
    "\n",
    "These output `TextFrame`s will then be consumed by our `RivaTTSService`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "574fc150-02b9-4e0f-8eed-496452a1d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our LLM service\n",
    "llm_service = NvidiaLLMService(  \n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    "    model=\"meta/llama-3.3-70b-instruct\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d357be-141b-4896-834d-f907c3d46116",
   "metadata": {},
   "source": [
    "### Defining User Input and System Prompt for the LLM\n",
    "We'll provide a simple user query and a system prompt to guide the LLM's response style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c235f9ed-cb98-4c58-b5fb-935a0ccb1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input to be processed by the LLM\n",
    "dynamic_user_input = \"Tell me a short, interesting fact about virtual humans.\"\n",
    "\n",
    "# System prompt for the LLM\n",
    "llm_system_prompt = \"You are a helpful and enthusiastic assistant. Keep your responses concise and engaging.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d1769-f36d-41a8-a67d-ed97e0808e6e",
   "metadata": {},
   "source": [
    "### Building and Running the LLM-TTS Pipeline\n",
    "The pipeline will now be: `LLMMessagesFrame (queued manually)` → `NvidiaLLMService` → `TextFrame (streamed)` → `RivaTTSService` → `TTSAudioRawFrame (streamed)` → `LocalAudioTransport (output)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fac68583-6a62-4f43-b7d5-0e6bb099afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 15:27:46.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#5 -> NvidiaLLMService#2\u001b[0m\n",
      "\u001b[32m2025-05-14 15:27:46.726\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking NvidiaLLMService#2 -> RivaTTSService#2\u001b[0m\n",
      "\u001b[32m2025-05-14 15:27:46.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking RivaTTSService#2 -> LocalAudioOutputTransport#5\u001b[0m\n",
      "\u001b[32m2025-05-14 15:27:46.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking LocalAudioOutputTransport#5 -> PipelineSink#5\u001b[0m\n",
      "\u001b[32m2025-05-14 15:27:46.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#5 -> Pipeline#5\u001b[0m\n",
      "\u001b[32m2025-05-14 15:27:46.728\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#5 -> PipelineTaskSink#5\u001b[0m\n",
      "\u001b[32m2025-05-14 15:27:46.729\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#5 started running PipelineTask#5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User asks: 'Tell me a short, interesting fact about virtual humans.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 15:27:47.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnvidia_pipecat.services.nvidia_llm\u001b[0m:\u001b[36m_stream_chat_completions\u001b[0m:\u001b[36m176\u001b[0m - \u001b[34m\u001b[1mGenerating chat: [{\"role\": \"system\", \"content\": \"You are a helpful and enthusiastic assistant. Keep your responses concise and engaging.\", \"name\": \"system\"}, {\"role\": \"user\", \"content\": \"Tell me a short, interesting fact about virtual humans.\", \"name\": \"user\"}]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message queued for LLM and then TTS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-14 15:27:48.604\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnvidia_pipecat.services.riva_speech\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m172\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [Did you know that virtual humans, also known as digital humans, can now be created with such precision that they can even mimic the subtleties of human emotions and behaviors, making them almost indistinguishable from real people?]\u001b[0m\n",
      "\u001b[32m2025-05-14 15:27:49.111\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_started_speaking\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mBot started speaking\u001b[0m\n",
      "\u001b[32m2025-05-14 15:28:02.143\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#5 finished running PipelineTask#5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic LLM-TTS pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "async def run_dynamic_tts_pipeline():  \n",
    "    print(f\"User asks: '{dynamic_user_input}'\")\n",
    "    # Set up audio output transport, same as before\n",
    "    audio_transport = LocalAudioTransport(LocalAudioTransportParams(audio_out_enabled=True))  \n",
    "      \n",
    "    # Create a pipeline: LLM service -> TTS service -> Audio output transport\n",
    "    pipeline = Pipeline([llm_service, tts_service, audio_transport.output()])  \n",
    "      \n",
    "    task = PipelineTask(pipeline)  \n",
    "      \n",
    "    async def generate_and_speak():  \n",
    "        await asyncio.sleep(1) # Allow pipeline to initialize\n",
    "          \n",
    "        # Prepare messages for the LLM\n",
    "        messages_for_llm = [  \n",
    "            {\"role\": \"system\", \"content\": llm_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": dynamic_user_input}\n",
    "        ]  \n",
    "          \n",
    "        # Queue the LLMMessagesFrame to the LLM service.\n",
    "        # The LLM's output (TextFrames) will automatically flow to the TTS service.\n",
    "        await task.queue_frames([LLMMessagesFrame(messages_for_llm), EndFrame()])\n",
    "        print(\"Message queued for LLM and then TTS.\")\n",
    "      \n",
    "    runner = PipelineRunner()  \n",
    "      \n",
    "    await asyncio.gather(runner.run(task), generate_and_speak())  \n",
    "    print(\"Dynamic LLM-TTS pipeline finished.\")\n",
    "  \n",
    "if __name__ == \"__main__\":  \n",
    "    await run_dynamic_tts_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abfa992-6be3-46ca-a5e8-f5cddeb98f04",
   "metadata": {},
   "source": [
    "### How This Works\n",
    "1.  The `LLMMessagesFrame` is sent to the `NvidiaLLMService`.\n",
    "2.  The LLM processes the input and system prompt, generating a response as a stream of `TextFrame`s (or `LLMTokenFrame`s that get aggregated into `TextFrame`s implicitly by the `LLMService` before outputting if not handled by a downstream token aggregator).\n",
    "3.  These `TextFrame`s are then passed sequentially to the `RivaTTSService`.\n",
    "4.  `RivaTTSService` converts the incoming text chunks into `TTSAudioRawFrame`s.\n",
    "5.  `LocalAudioTransport` plays the audio as it's received, demonstrating the streaming capability from LLM text generation through to speech output.\n",
    "\n",
    "This creates a complete pipeline from a user query to a spoken response:\n",
    "`User Input (text)` → `NvidiaLLMService` → `TextFrame (stream)` → `RivaTTSService` → `TTSAudioRawFrame (stream)` → `Spoken Output`\n",
    "\n",
    "### ✏️ Exercises & Further Exploration:\n",
    "1.  **Change LLM Model:** In `llm_service`, try a different `model` from the NVIDIA NIM catalog (a smaller, faster model, or one specialized for chat if available).\n",
    "2.  **Modify System Prompt:** Experiment with different `llm_system_prompt` values to see how it influences the LLM's tone and content, and subsequently the spoken output.\n",
    "3.  **Temperature Control:** Add a `temperature` parameter to the `NvidiaLLMService` initialization (`temperature=0.7`). Observe how different temperature values affect the creativity/predictability of the LLM's responses and the resulting speech.\n",
    "4.  **Observe Frames (Advanced):** Adapt the `FramePrinter` observer from Module 1.1 to log the `TextFrame`s coming from the LLM and the `TTSAudioRawFrame`s from the TTS. This will help visualize the streaming flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b45681-5181-4d1c-bb03-a6db71db5907",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, you've learned how to use `RivaTTSService` within NVIDIA Pipecat to synthesize speech, both from static text and from dynamic text generated by an LLM. You've seen how Pipecat's pipeline architecture allows for seamless, streaming integration of these powerful AI services.\n",
    "\n",
    "Key takeaways:\n",
    "- `RivaTTSService` converts `TextFrame` or `TTSSpeakFrame` inputs into `TTSAudioRawFrame` outputs.\n",
    "- Pipelines can chain services like LLMs and TTS to create responsive voice interactions.\n",
    "- `LocalAudioTransport` provides a simple way to hear TTS output during development.\n",
    "\n",
    "In the next sections and modules, we will build upon this foundation by integrating Automatic Speech Recognition (ASR) to create a full speech-to-speech conversational agent, and explore more advanced features of `nvidia-pipecat` for building sophisticated digital humans."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
