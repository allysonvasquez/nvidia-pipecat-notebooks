{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31f2dc9-8ccc-4c1f-9f5e-f9630bdec938",
   "metadata": {},
   "source": [
    "# Intro to Speech Pipelines for Digital Humans\n",
    "This module introduces the core technical components of a real-time voice agent system, with a focus on speech-to-speech (S2S) processing. These pipelines form the backbone of interactive digital human applications across support, education, entertainment, and more.\n",
    "\n",
    "We won’t cover chatbot prompting or dialogue logic here—that comes in the next module. The goal of this section is to understand how speech data flows through a pipeline: from raw audio input to synthesized voice output.\n",
    "The focus here is on building towards a real-time, interactive digital human application, such as a customer support avatar, gaming use case, etc. but we will focus on voice and speech pipeline technical components.\n",
    "\n",
    "⚠️ This notebook is intentionally light on theory. The priority is to help you implement, compare, and modify working components in a modular AI agent stack.\n",
    "\n",
    "## By the end of this module, students should be able to:\n",
    "- Describe the full stack of a speech to speech pipeline and its role in a digital human.  z\n",
    "- Compare and choose appropriate technologies for ASR and TTS.\n",
    "- Design for real-time constraints like streaming vs. batch, and low latency interaction.\n",
    "- Understand how data flows between components and what format conversions are required.\n",
    "- Maybe we lightly mention deployment considerations, including hardware, containerization, and service orchestration.\n",
    "\n",
    "# Core Components of a Voice Agent Pipeline\n",
    "**Automatic Speech Recognition**: Converts spoken language into text.  \n",
    "**LLM/NLU**: Understand input and generate a meaningful response.  \n",
    "**Text to Speech**: Converts the LLM response into speech.  \n",
    "**Voice Activity Detection**: Detects when a user is speaking. (Optional) \n",
    "\n",
    "\n",
    "\n",
    "Implementing effective end-to-end (e2e) conversational systems is a major challenge.  \n",
    "Applications like voice assistants are nondeterministic in nature and the above multi-component architecture introduces problems like latency, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2b519-03f1-4e58-b711-42682e1653f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell builds the *smallest* possible Pipecat pipeline:\n",
    "#   TTSSpeakFrame  ─▶  RivaTTSService  ─▶  LocalAudioTransport\n",
    "#\n",
    "# When you run it, you should hear the configured voice speak\n",
    "# the message defined in `message`.\n",
    "\n",
    "# 1)  Configure the Riva TTS processor\n",
    "# ------------------------------------\n",
    "tts = RivaTTSService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), # set API Key\n",
    "    voice_id= \"English-US.Female-1\",  # define the voice\n",
    "    )\n",
    "\n",
    "# 2)  Editable message (rerun the cell after you change it)\n",
    "# ---------------------------------------------------------\n",
    "message=\"Hello there, how is it going!\"\n",
    "\n",
    "# 3)  Async driver\n",
    "# ----------------\n",
    "async def main():\n",
    "    # LocalAudioTransport plays the raw audio directly on your machine.\n",
    "    transport = LocalAudioTransport(LocalAudioTransportParams(audio_out_enabled=True))\n",
    "\n",
    "    # Build pipeline: [Riva TTS] → [Audio output]\n",
    "    pipeline = Pipeline([tts, transport.output()]) # We define our RivaTTS Service in the Pipeline\n",
    "\n",
    "    # Wrap in a PipelineTask so the runner can start/stop it\n",
    "    task = PipelineTask(pipeline)\n",
    "\n",
    "    # This allows for a single speech request, then closes.\n",
    "    async def say_something():\n",
    "        # Small delay so the pipeline is fully up before we push frames\n",
    "        await asyncio.sleep(1)\n",
    "        \n",
    "        await task.queue_frames([\n",
    "            TTSSpeakFrame(message), # trigger TTS\n",
    "            EndFrame() # signal the pipeline is done\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # PipelineRunner handles the event loop & graceful shutdown\n",
    "    runner = PipelineRunner(handle_sigint=False if sys.platform == \"win32\" else True)\n",
    "\n",
    "    # Run the pipeline and our helper in parallel\n",
    "    await asyncio.gather(runner.run(task), say_something())\n",
    "\n",
    "# 4)  Jupyter notebooks need 'nest_asyncio' to nest loops safely\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac8dcd-d3d6-4fdf-b125-1fd01449a16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d8694a-e84a-4fd9-9889-21eb9a2abfc2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "A real-time voice agent for digital humans requires careful coordination of VAD, ASR, LLM, and TTS. Each step must be optimized for latency, accuracy, and compatibility. In this module, we introduced the essential technical building blocks. In the next, we’ll apply them to interactive prompting and memory-enabled agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9796f6-37da-4a8b-92eb-2f56f0e6d678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
