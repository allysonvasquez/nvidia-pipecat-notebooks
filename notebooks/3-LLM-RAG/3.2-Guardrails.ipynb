{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "module-title-welcome",
   "metadata": {},
   "source": [
    "# Module 3.2: Implementing Guardrails for Safe Digital Human Interactions\n",
    "\n",
    "Welcome to Module 3.2 of the Digital Human Teaching Kit! As we build increasingly sophisticated AI-driven conversational agents, ensuring their safe, ethical, and appropriate behavior becomes paramount. In previous modules, you've learned to construct real-time pipelines and integrate multimodal inputs with powerful LLMs. However, without proper controls, even the most advanced AI can produce undesirable or harmful content.\n",
    "\n",
    "This module introduces the critical concept of **guardrails** in conversational AI. We will delve into how these safety mechanisms are implemented within the `nvidia-pipecat` framework, specifically focusing on the `GuardrailProcessor` found in the NVIDIA ACE Controller codebase. We'll also explore how NVIDIA's powerful **NeMo Guardrails NIMs** provide semantic understanding for content and topical safety. Beyond guardrails, we will reinforce concepts of **LLM authoring and prompt engineering** and introduce **Retrieval-Augmented Generation (RAG)**, essential for grounded responses. Finally, we'll touch upon how **animation integration** enhances the digital human experience. Understanding and applying these concepts is essential for deploying trustworthy and engaging digital humans in real-world applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Define guardrails in the context of conversational AI and explain their importance.\n",
    "- Understand and implement the `GuardrailProcessor` class from `nvidia-pipecat` for keyword-based filtering.\n",
    "- Apply advanced topical and safety guardrails using NVIDIA NIM models for semantic content control.\n",
    "- Deepen understanding of LLM authoring and parameter customization using `NvidiaLLMService`.\n",
    "- Identify the role of Retrieval-Augmented Generation (RAG) and its integration with digital humans.\n",
    "- Recognize how animation integration enriches the digital human's expressiveness.\n",
    "- Discuss the strengths and limitations of different guardrail implementations and how they complement each other.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "code-imports-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA API key loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import os\n",
    "import getpass\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from pipecat.frames.frames import Frame, TextFrame, EndFrame, StartFrame, TranscriptionFrame, TTSSpeakFrame\n",
    "from pipecat.observers.base_observer import BaseObserver\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineTask, PipelineParams, FrameDirection\n",
    "from pipecat.processors.frame_processor import FrameProcessor\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "# Specific import for the Pipecat-based guardrail\n",
    "from nvidia_pipecat.processors.guardrail import GuardrailProcessor\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService # Will be used conceptually\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # For running asyncio in Jupyter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure NVIDIA_API_KEY is set\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"NVIDIA API key not found or invalid in .env file.\")\n",
    "    nvapi_key = getpass.getpass(\"üîê Enter your NVIDIA API key: \").strip()\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "else:\n",
    "    print(\"NVIDIA API key loaded from .env file.\")\n",
    "\n",
    "# Initialize the OpenAI client for NVIDIA NIMs (for direct API calls to guardrail NIMs)\n",
    "nim_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ.get(\"NVIDIA_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guardrails-intro",
   "metadata": {},
   "source": [
    "# Guardrails in Conversational AI Systems\n",
    "\n",
    "**Guardrails** are safety mechanisms designed to filter and control user inputs and AI outputs in conversational AI systems. Their primary purpose is to prevent the system from processing, generating, or relaying inappropriate, harmful, offensive, or off-topic content. Think of them as a protective layer that ensures your digital human adheres to predefined safety policies and desired behaviors.\n",
    "\n",
    "Guardrails help enforce rules for LLM output and input, typically categorized into three major types:\n",
    "-   **Topical Guardrails** ‚Äì Keep conversations focused by blocking or redirecting off-topic inputs.\n",
    "-   **Safety Guardrails** ‚Äì Prevent unsafe, harmful, or inappropriate responses like hate speech, violence, sexual content, and misinformation.\n",
    "-   **Security Guardrails** ‚Äì Protect sensitive information and prevent risky outputs like jailbreak attempts or account information.\n",
    "\n",
    "In the context of a real-time digital human pipeline, guardrails act as checkpoints, intercepting user transcriptions or LLM responses and applying rules to them. If a rule is violated, the guardrail can block the content, provide a predefined safe response, or escalate the interaction.\n",
    "\n",
    "NVIDIA offers **NeMo Guardrails**, an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. This framework supports full customization of rules and actions to take when interfacing with LLMs, moving beyond simple keyword blocking to more semantic understanding.\n",
    "\n",
    "In this notebook, we will explore two distinct approaches to implementing guardrails: a direct, keyword-based processor within `nvidia-pipecat`, and the more advanced, semantic NIM-based guardrails powered by NeMo Guardrails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-guardrailprocessor",
   "metadata": {},
   "source": [
    "## Core Implementation: `GuardrailProcessor` (Keyword-Based)\n",
    "\n",
    "The NVIDIA ACE Controller codebase provides a foundational guardrail implementation through the `GuardrailProcessor` class. This processor extends Pipecat's `FrameProcessor` and is designed to perform word-based content filtering on incoming frames.\n",
    "\n",
    "### Initialization and Configuration\n",
    "\n",
    "The `GuardrailProcessor` is initialized with key configurable parameters, allowing you to tailor its behavior.\n",
    "\n",
    "-   **`blocked_words`**: A list of words or phrases that the guardrail should detect and block. This list is normalized to lowercase for case-insensitive matching.\n",
    "-   **`block_message`**: The default response that the digital human will issue when it detects blocked content. This provides immediate feedback to the user about the restriction.\n",
    "\n",
    "A key design decision here is to normalize inputs and blocked words to lowercase, ensuring that the filtering is case-insensitive. You can also initialize the processor without any blocked words, effectively disabling its filtering capability until words are added.\n",
    "\n",
    "### Core Filtering Logic\n",
    "\n",
    "The heart of the `GuardrailProcessor` lies in its content checking method. It implements a sophisticated blocking algorithm to ensure accuracy:\n",
    "\n",
    "-   **Word Boundary Matching**: It uses regular expressions (specifically `\\b{word}\\b`) to ensure exact word matches, preventing substrings from being inadvertently blocked. For example, if \"ass\" is blocked, \"assistant\" will not be blocked.\n",
    "-   **Case-insensitive Comparison**: All queries are converted to lowercase before matching against the `blocked_words` list.\n",
    "-   **Early Termination**: The method efficiently returns `True` as soon as the first blocked word is found, optimizing performance for longer inputs.\n",
    "\n",
    "This deterministic, word-based filtering provides a reliable first line of defense against unwanted content.\n",
    "\n",
    "### Frame Processing Flow\n",
    "\n",
    "The `process_frame` method within the `GuardrailProcessor` handles the actual filtering within the Pipecat pipeline.\n",
    "\n",
    "-   **Frame Type Checking**: It specifically processes `TranscriptionFrame` objects, which typically carry the user's speech-to-text output. Other frame types are passed through unchanged.\n",
    "-   **Content Evaluation**: It calls the internal blocking algorithm on the `TranscriptionFrame`'s text content.\n",
    "-   **Response Generation**: If blocked content is detected, the processor generates a `TTSSpeakFrame` containing the predefined `block_message`. This frame can then be sent downstream to a Text-to-Speech (TTS) service to inform the user.\n",
    "-   **Pipeline Control**: Crucially, when content is blocked, the original `TranscriptionFrame` (or any problematic frame) is **not** propagated downstream to subsequent processors (like an LLM). This ensures that the sensitive or inappropriate content never engages the core AI logic, upholding safety policies.\n",
    "\n",
    "This mechanism ensures that the guardrail acts as an effective gatekeeper, maintaining control over the conversation flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-observer",
   "metadata": {},
   "source": [
    "### Custom Observer for Pipeline Output\n",
    "We'll use a simple `ResponsePrinter` observer to visualize the frames that pass through the pipeline, especially the `TTSSpeakFrame` generated by the guardrail. Observers are non-intrusive and excellent for debugging and monitoring pipeline activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "code-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponsePrinter(BaseObserver):\n",
    "    async def on_push_frame(self, src: FrameProcessor, dst: FrameProcessor, frame: Frame, direction: FrameDirection, timestamp: int):\n",
    "        if direction == FrameDirection.DOWNSTREAM:\n",
    "            if isinstance(frame, TextFrame):\n",
    "                print(f\"[Pipeline Output: TextFrame] -> '{frame.text}'\")\n",
    "            elif isinstance(frame, TTSSpeakFrame):\n",
    "                print(f\"[Pipeline Output: TTSSpeakFrame] -> '{frame.text}'\")\n",
    "            elif isinstance(frame, EndFrame):\n",
    "                print(\"[Pipeline Output: EndFrame] Pipeline turn complete.\")\n",
    "            # Optionally print other frame types for debugging\n",
    "            # else:\n",
    "            #     print(f\"[Pipeline Output: Other Frame] -> {type(frame).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-driven-understanding",
   "metadata": {},
   "source": [
    "## Test-Driven Understanding: `GuardrailProcessor` Behaviors\n",
    "\n",
    "Let's create a utility function to run small pipeline tests and demonstrate the `GuardrailProcessor`'s behaviors. We'll simulate user input by pushing `TranscriptionFrame`s, as this is what the `GuardrailProcessor` is designed to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "code-test-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_guardrail_test_pipeline(input_text: str, blocked_words: List[str], block_message: str = \"I'm sorry, I cannot discuss that topic.\"):\n",
    "    print(f\"\\n--- Testing GuardrailProcessor: Input='{input_text}', Blocked={blocked_words} ---\")\n",
    "    guardrail = GuardrailProcessor(\n",
    "        blocked_words=blocked_words,\n",
    "        block_message=block_message\n",
    "    )\n",
    "    pipeline = Pipeline([guardrail])\n",
    "    task = PipelineTask(\n",
    "        pipeline,\n",
    "        params=PipelineParams(observers=[ResponsePrinter()])\n",
    "    )\n",
    "    runner = PipelineRunner()\n",
    "    run_task = asyncio.create_task(runner.run(task))\n",
    "\n",
    "    await asyncio.sleep(0.01) # Give pipeline a moment to start\n",
    "\n",
    "    # Push the simulated user input as a TranscriptionFrame\n",
    "    # For TranscriptionFrame, a dummy user_id and timestamp are needed\n",
    "    await task.queue_frame(TranscriptionFrame(text=input_text, user_id=\"test_user\", timestamp=0))\n",
    "    await task.queue_frame(EndFrame()) # Signal end of input for this turn\n",
    "\n",
    "    await run_task # Wait for the pipeline to finish processing\n",
    "    print(\"--- Test Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-verification",
   "metadata": {},
   "source": [
    "### Behavioral Verification\n",
    "\n",
    "Let's run through the four key behaviors of the `GuardrailProcessor` as described in its test suite, demonstrating its functionality in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "test-blocked-word",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-28 10:20:57.224\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#0 -> GuardrailProcessor#0\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.225\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking GuardrailProcessor#0 -> PipelineSink#0\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.226\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#0 -> Pipeline#0\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.227\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#0 -> PipelineTaskSink#0\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.228\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 started running PipelineTask#0\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.239\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnvidia_pipecat.processors.guardrail\u001b[0m:\u001b[36mis_query_blocked\u001b[0m:\u001b[36m57\u001b[0m - \u001b[34m\u001b[1mQuery: I love football, it's a great sport! contains blocked word: football\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.240\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mnvidia_pipecat.processors.guardrail\u001b[0m:\u001b[36mprocess_frame\u001b[0m:\u001b[36m72\u001b[0m - \u001b[34m\u001b[1mBlocked query detected: I love football, it's a great sport!\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 finished running PipelineTask#0\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#1 -> GuardrailProcessor#1\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking GuardrailProcessor#1 -> PipelineSink#1\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.244\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#1 -> Pipeline#1\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#1 -> PipelineTaskSink#1\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.245\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#1 started running PipelineTask#1\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#1 finished running PipelineTask#1\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#2 -> GuardrailProcessor#2\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking GuardrailProcessor#2 -> PipelineSink#2\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#2 -> Pipeline#2\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#2 -> PipelineTaskSink#2\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#2 started running PipelineTask#2\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.304\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#2 finished running PipelineTask#2\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#3 -> GuardrailProcessor#3\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking GuardrailProcessor#3 -> PipelineSink#3\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.305\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking PipelineTaskSource#3 -> Pipeline#3\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#3 -> PipelineTaskSink#3\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.306\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#3 started running PipelineTask#3\u001b[0m\n",
      "\u001b[32m2025-05-28 10:20:57.333\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m50\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#3 finished running PipelineTask#3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing GuardrailProcessor: Input='I love football, it's a great sport!', Blocked=['football'] ---\n",
      "[Pipeline Output: TextFrame] -> 'I love football, it's a great sport!'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'I love football, it's a great sport!'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TTSSpeakFrame] -> 'I'm sorry, I cannot discuss that topic.'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TTSSpeakFrame] -> 'I'm sorry, I cannot discuss that topic.'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "--- Test Completed ---\n",
      "\n",
      "--- Testing GuardrailProcessor: Input='What is the capital of France?', Blocked=['politics'] ---\n",
      "[Pipeline Output: TextFrame] -> 'What is the capital of France?'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'What is the capital of France?'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'What is the capital of France?'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'What is the capital of France?'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "--- Test Completed ---\n",
      "\n",
      "--- Testing GuardrailProcessor: Input='I enjoy playing football on the weekend.', Blocked=['foot'] ---\n",
      "[Pipeline Output: TextFrame] -> 'I enjoy playing football on the weekend.'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'I enjoy playing football on the weekend.'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'I enjoy playing football on the weekend.'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'I enjoy playing football on the weekend.'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "--- Test Completed ---\n",
      "\n",
      "--- Testing GuardrailProcessor: Input='Tell me a joke!', Blocked=[] ---\n",
      "[Pipeline Output: TextFrame] -> 'Tell me a joke!'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'Tell me a joke!'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'Tell me a joke!'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "[Pipeline Output: TextFrame] -> 'Tell me a joke!'\n",
      "[Pipeline Output: EndFrame] Pipeline turn complete.\n",
      "--- Test Completed ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Blocked word detection: Input with a blocked word should trigger the rejection message\n",
    "await run_guardrail_test_pipeline(\n",
    "    input_text=\"I love football, it's a great sport!\",\n",
    "    blocked_words=[\"football\"]\n",
    ")\n",
    "\n",
    "# 2. Passthrough behavior: Non-blocked content should pass unchanged\n",
    "await run_guardrail_test_pipeline(\n",
    "    input_text=\"What is the capital of France?\",\n",
    "    blocked_words=[\"politics\"]\n",
    ")\n",
    "\n",
    "# 3. Substring handling: \"football\" should be allowed when only \"foot\" is blocked (due to word boundary matching)\n",
    "await run_guardrail_test_pipeline(\n",
    "    input_text=\"I enjoy playing football on the weekend.\",\n",
    "    blocked_words=[\"foot\"]\n",
    ")\n",
    "\n",
    "# 4. Default behavior: No filtering when no words are configured\n",
    "await run_guardrail_test_pipeline(\n",
    "    input_text=\"Tell me a joke!\",\n",
    "    blocked_words=[] # Empty list means no words are blocked\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integration-in-pipeline",
   "metadata": {},
   "source": [
    "## `GuardrailProcessor` Integration in a Conversational Pipeline\n",
    "\n",
    "The `GuardrailProcessor` is designed to integrate seamlessly into a broader Pipecat conversational AI pipeline. It typically sits early in the Cognition Layer, right after the Perception Layer (after the Automatic Speech Recognition service produces `TranscriptionFrame`s)\n",
    "\n",
    "When a blocked query is detected, the `GuardrailProcessor` plays a crucial role in controlling the flow:\n",
    "\n",
    "-   It intercepts the `TranscriptionFrame` containing the user's input.\n",
    "-   It generates a `TTSSpeakFrame` with the rejection message, which will be routed to the TTS service for audible output.\n",
    "-   Crucially, it **prevents the original blocked content** from reaching downstream processors like the Large Language Model (LLM) or a RAG system. This ensures that the sensitive or inappropriate content never engages the core AI logic, upholding safety policies.\n",
    "\n",
    "This modular design allows keyword-based guardrails to act as a robust safety layer without interfering with the normal operation of the pipeline for valid inputs.\n",
    "\n",
    "![Guardrail Processor in Pipeline](../../../docs/images/guardrail-pipeline-flow.png)\n",
    "*<p align=\"center\">Conceptual diagram of GuardrailProcessor's position in a digital human pipeline.</p>*\n",
    "\n",
    "| Component                   | Responsibility                                                 | Example NVIDIA Technology/Concept       |\n",
    "|-----------------------------|----------------------------------------------------------------|-----------------------------------------|\n",
    "| **Perception Layer**        |                                                                |                                         |\n",
    "| ASR Service                 | Converts user speech to text                                   | NVIDIA Riva ASR                         |\n",
    "| **Cognition Layer**         |                                                                |                                         |\n",
    "| **GuardrailProcessor**      | **Filters and blocks inappropriate user input**                | `nvidia-pipecat.processors.guardrail.GuardrailProcessor` |\n",
    "| LLM Service                 | Generates responses (receives only safe inputs)                | NVIDIA NIM LLMs                         |\n",
    "| RAG System (Optional)       | Augments LLM with external knowledge (receives only safe inputs) | NVIDIA NeMo Retriever NIM               |\n",
    "| **Generation Layer**        |                                                                |                                         |\n",
    "| TTS Service                 | Converts text response to speech (for LLM response or block message) | NVIDIA Riva TTS                         |\n",
    "| Animation Engine (Optional) | Drives avatar's expressions/gestures                           | NVIDIA Audio2Face                       |\n",
    "\n",
    "As seen in the diagram and table, the `GuardrailProcessor` is strategically placed to act as an immediate gatekeeper, ensuring that only permissible content proceeds through the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "design-patterns-considerations",
   "metadata": {},
   "source": [
    "## Design Patterns and Considerations for `GuardrailProcessor`\n",
    "\n",
    "The `GuardrailProcessor` offers a foundational approach to content filtering. It embodies several strengths but also has inherent limitations due to its rule-based nature.\n",
    "\n",
    "**Strengths:**\n",
    "-   **Simple and Deterministic:** Easy to configure and understand, providing predictable blocking based on exact word matches.\n",
    "-   **Configurable and Extensible:** The `blocked_words` list and `block_message` are easily customizable, allowing for adaptation to different use cases and policies.\n",
    "-   **Clean Integration:** Seamlessly integrates with Pipecat's frame-based architecture, acting as a clear choke point in the data flow.\n",
    "-   **Preserves Pipeline Flow:** For non-blocked content, it passes frames along efficiently, maintaining the low-latency real-time characteristics of the pipeline.\n",
    "\n",
    "**Limitations:**\n",
    "-   **Basic Keyword Matching:** Lacks semantic understanding. It cannot detect intent, sarcasm, nuanced inappropriate language, or circumventions (using symbols or misspellings to bypass filters).\n",
    "-   **No Context Awareness:** Operates solely on the current input frame, without considering previous turns in the conversation. This can lead to false positives or negatives in complex dialogues.\n",
    "-   **Potential for False Positives/Negatives:** A blocked word might appear in a benign context, or a harmful phrase might not contain any explicitly blocked words.\n",
    "-   **Limited to Exact Word Boundary Matching:** While a strength for precision, it means that variants or more complex phrases might not be caught unless explicitly added to the `blocked_words` list.\n",
    "\n",
    "This implementation serves as an excellent starting point for content filtering in conversational AI systems, particularly for scenarios requiring deterministic, rule-based content control. For more advanced safety, it is often combined with other techniques, such as LLM-based moderation or custom behavior logic that can be implemented using NVIDIA NeMo Guardrails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-guardrails-nims",
   "metadata": {},
   "source": [
    "## Advanced Guardrails: NVIDIA NIMs for Topical and Content Safety\n",
    "\n",
    "While keyword-based guardrails are useful for explicit blocking, modern AI applications require more nuanced control. **NVIDIA NeMo Guardrails** is an open-source toolkit that allows for programmable guardrails, enabling more sophisticated control over LLM behavior. This framework can leverage specialized NVIDIA NIMs to perform semantic classification of content and topics.\n",
    "\n",
    "Most content moderation tools (like generic LLama Guard models) rely on predefined taxonomies of harms, covering general sensitive areas like violence or hate speech. While useful, these models often lack the flexibility to adapt to domain-specific needs for Digital Human use-cases. In contrast, NeMo Guardrails, with its specialized NIMs, allows developers to:\n",
    "-   Define custom allowed/disallowed topics.\n",
    "-   Enforce domain-restricted interactions, like an AI teacher that only answers math questions.\n",
    "-   Respond with context-aware refusals or redirections.\n",
    "-   Identify specific categories of unsafe content.\n",
    "\n",
    "Let's explore two key NeMo Guardrails NIMs available via the NVIDIA API Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "code-install-nemoguardrails",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nemoguardrails in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: aiohttp>=3.10.11 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (3.11.18)\n",
      "Requirement already satisfied: annoy>=1.17.3 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (1.17.3)\n",
      "Requirement already satisfied: fastapi>=0.103.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.115.12)\n",
      "Requirement already satisfied: fastembed<0.4.1,>=0.2.2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.1.6 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (3.1.6)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.2.14 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.3.25)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.0.16 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.3.23)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.2.14 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.3.59)\n",
      "Requirement already satisfied: lark>=1.1.7 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (1.2.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.6 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (1.6.0)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (1.19.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (2.2.3)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (3.0.51)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (2.10.6)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (6.0.2)\n",
      "Requirement already satisfied: rich>=13.5.2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (14.0.0)\n",
      "Requirement already satisfied: simpleeval>=0.9.13 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (1.0.3)\n",
      "Requirement already satisfied: starlette>=0.27.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.46.2)\n",
      "Requirement already satisfied: typer>=0.8 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.13.1)\n",
      "Requirement already satisfied: uvicorn>=0.23 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (0.32.1)\n",
      "Requirement already satisfied: watchdog>=3.0.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nemoguardrails) (6.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from aiohttp>=3.10.11->nemoguardrails) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from aiohttp>=3.10.11->nemoguardrails) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from aiohttp>=3.10.11->nemoguardrails) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from aiohttp>=3.10.11->nemoguardrails) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from aiohttp>=3.10.11->nemoguardrails) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from aiohttp>=3.10.11->nemoguardrails) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from aiohttp>=3.10.11->nemoguardrails) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastapi>=0.103.0->nemoguardrails) (4.13.2)\n",
      "Requirement already satisfied: PyStemmer<3.0.0,>=2.2.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (2.2.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (0.31.1)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (0.7.3)\n",
      "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (4.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (1.26.4)\n",
      "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (1.17.0)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (10.4.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (2.32.3)\n",
      "Requirement already satisfied: snowballstemmer<3.0.0,>=2.2.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (2.2.0)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (0.21.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fastembed<0.4.1,>=0.2.2->nemoguardrails) (4.67.1)\n",
      "Requirement already satisfied: anyio in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.24.1->nemoguardrails) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.24.1->nemoguardrails) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.24.1->nemoguardrails) (1.0.8)\n",
      "Requirement already satisfied: idna in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpx>=0.24.1->nemoguardrails) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->nemoguardrails) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jinja2>=3.1.6->nemoguardrails) (3.0.2)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain<0.4.0,>=0.2.14->nemoguardrails) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain<0.4.0,>=0.2.14->nemoguardrails) (0.3.42)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain<0.4.0,>=0.2.14->nemoguardrails) (2.0.40)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.0.16->nemoguardrails) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.0.16->nemoguardrails) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.0.16->nemoguardrails) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain-community<0.4.0,>=0.0.16->nemoguardrails) (0.4.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.2.14->nemoguardrails) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.2.14->nemoguardrails) (24.2)\n",
      "Requirement already satisfied: coloredlogs in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.17.0->nemoguardrails) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.17.0->nemoguardrails) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.17.0->nemoguardrails) (5.29.4)\n",
      "Requirement already satisfied: sympy in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.17.0->nemoguardrails) (1.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas<3,>=1.4.0->nemoguardrails) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas<3,>=1.4.0->nemoguardrails) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas<3,>=1.4.0->nemoguardrails) (2025.2)\n",
      "Requirement already satisfied: wcwidth in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from prompt-toolkit>=3.0->nemoguardrails) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic>=1.10->nemoguardrails) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic>=1.10->nemoguardrails) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from rich>=13.5.2->nemoguardrails) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from rich>=13.5.2->nemoguardrails) (2.19.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from typer>=0.8->nemoguardrails) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from typer>=0.8->nemoguardrails) (1.5.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from anyio->httpx>=0.24.1->nemoguardrails) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.0.16->nemoguardrails) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.0.16->nemoguardrails) (0.9.0)\n",
      "Requirement already satisfied: filelock in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.4.1,>=0.2.2->nemoguardrails) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.4.1,>=0.2.2->nemoguardrails) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed<0.4.1,>=0.2.2->nemoguardrails) (1.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.2.14->nemoguardrails) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.2.14->nemoguardrails) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.2.14->nemoguardrails) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.2.14->nemoguardrails) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nemoguardrails) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.0.16->nemoguardrails) (1.0.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.0.16->nemoguardrails) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->nemoguardrails) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests<3.0,>=2.31->fastembed<0.4.1,>=0.2.2->nemoguardrails) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests<3.0,>=2.31->fastembed<0.4.1,>=0.2.2->nemoguardrails) (2.4.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->nemoguardrails) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->nemoguardrails) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/avasquez/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.0.16->nemoguardrails) (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies for NeMo Guardrails framework (if you plan to build custom rails)\n",
    "# Note: This is separate from the nvidia-pipecat GuardrailProcessor and NIM API calls.\n",
    "!pip install nemoguardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topical-guardrail-nim",
   "metadata": {},
   "source": [
    "### Topical Guardrail NIM: `llama-3.1-nemoguard-8b-topic-control`\n",
    "\n",
    "The `llama-3.1-nemoguard-8b-topic-control` model is a dialog moderation model trained by NVIDIA, based on the Llama 3.1 8B Instruct foundation model. It serves as a topical guardrail, helping language applications stay aligned with developer-defined content boundaries.\n",
    "\n",
    "This model is fine-tuned using the CantTalkAboutThis dataset ‚Äî a carefully constructed collection of over 10,000 dialog samples. These samples are designed to teach the model how to identify when user input deviates from permitted topics and to respond appropriately, such as by refusing the request, redirecting the conversation, or providing a neutral fallback.\n",
    "\n",
    "Let's interact with this model to test its ability to classify questions as on-topic or off-topic within a museum guide scenario. We'll use the `nim_client` initialized from earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system-prompt-explanation",
   "metadata": {},
   "source": [
    "**System Instruction / System Prompt for NIMs**\n",
    "For these specialized guardrail NIMs, the system prompt is crucial. It defines the rules and context that the NIM will use to *classify* the user's input. Unlike a general LLM where the system prompt sets a persona, here it's about defining the **policy**.\n",
    "\n",
    "Notice how the prompt below clearly states the role and provides numbered rules. This structured approach helps the guardrail NIM interpret its task effectively. The NIM acts as a classifier, assessing whether the user's input adheres to these rules and then outputting a classification ('on-topic' or 'off-topic')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "define-topical-system-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the System Prompt for the Topical Guardrail NIM\n",
    "topical_system_prompt = (\n",
    "    \"You are an AI museum guide for the Modern Art & Technology Museum. Your role is to provide factual, accessible information about exhibits, artists, and museum logistics. \"\n",
    "    \"You must follow these guardrails:\\n\\n\"\n",
    "    \"1. Do not speculate about the value or future of artwork.\\n\"\n",
    "    \"2. Do not make personal or political commentary about the artists or their work.\\n\"\n",
    "    \"3. Do not provide medical, legal, or travel advice unrelated to museum logistics.\\n\"\n",
    "    \"4. If asked about topics outside the museum's scope (like global politics, conspiracy theories, or offensive content), politely redirect to museum-relevant topics or suggest asking a staff member.\\n\"\n",
    "    \"5. Maintain a polite, professional, and educational tone at all times.\\n\\n\"\n",
    "    \"Based on the user's question and the conversation history, classify the user's question as either 'on-topic' or 'off-topic'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "user-question-topical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a user input - Try changing this to an off-topic question!\n",
    "user_question_topical = \"What is an exhibit here?\" # Try: \"What's your opinion on the presidential election?\" or \"Tell me about current world conflicts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "run-topical-guardrail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topical Guardrail Result: on-topic \n"
     ]
    }
   ],
   "source": [
    "# Call the Topical Guardrail NIM\n",
    "topical_completion = nim_client.chat.completions.create(\n",
    "    model=\"nvidia/llama-3.1-nemoguard-8b-topic-control\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": topical_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_question_topical}\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    top_p=1,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "topic_result = topical_completion.choices[0].message.content\n",
    "print(f\"Topical Guardrail Result: {topic_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-response",
   "metadata": {},
   "source": [
    "Observe how the model enforces boundaries, and begin to understand how you might define similar guardrails for your own AI agents. The NIM will output either `on-topic` or `off-topic`. You can then use this classification to determine the digital human's response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "code-conditional-response",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digital Human Response: Our AI art exhibit explores how emerging technologies like machine learning are reshaping modern artistic expression. Feel free to explore it in the West Gallery!\n"
     ]
    }
   ],
   "source": [
    "if topic_result.strip() == \"off-topic\":\n",
    "    assistant_response_based_on_topic = (\n",
    "        \"I'm here to assist with questions about our exhibits, artists, and museum logistics. \"\n",
    "        \"For topics beyond the museum's scope, I recommend speaking with one of our staff members.\"\n",
    "    )\n",
    "else:\n",
    "    # In a real pipeline, you'd call your main LLM here to generate a detailed response.\n",
    "    # For this example, we simulate a real response.\n",
    "    assistant_response_based_on_topic = (\n",
    "        \"Our AI art exhibit explores how emerging technologies like machine learning are reshaping modern artistic expression. \"\n",
    "        \"Feel free to explore it in the West Gallery!\"\n",
    "    )\n",
    "\n",
    "print(f\"Digital Human Response: {assistant_response_based_on_topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "safety-guardrail-nim",
   "metadata": {},
   "source": [
    "### Safety Guardrail NIM: `llama-3.1-nemoguard-8b-content-safety`\n",
    "\n",
    "The `llama-3.1-nemoguard-8b-content-safety` model is a content moderation guardrail. It is built on the Llama 3.1 8B Instruct base model, and fine-tuned using the Aegis 2.0 dataset‚Äî a collection of 30,000 dialogue samples encompassing a comprehensive taxonomy of unsafe content categories.\n",
    "\n",
    "In applications like virtual assistants, ensuring that AI interactions remain free from harmful or inappropriate content is critical. This content safety guardrail serves as a protective layer, preventing the AI from engaging in or propagating content that falls into categories like violence, hate speech, sexual content, profanity, misinformation, and privacy violations.\n",
    "\n",
    "This model can evaluate **both** user inputs and LLM-generated responses, classifying them as ‚Äúsafe‚Äù or ‚Äúunsafe‚Äù and identifying specific categories of violations when applicable.\n",
    "\n",
    "Let's run a safety guardrail on our previous museum exhibit response and a potentially unsafe user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "run-safety-guardrail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking Safe Content ---\n",
      "Content Safety Output (Safe Example): {\"User Safety\": \"safe\", \"Response Safety\": \"safe\"} \n",
      "\n",
      "--- Checking Potentially Unsafe Content ---\n",
      "Content Safety Output (Unsafe Example): {\"User Safety\": \"unsafe\", \"Response Safety\": \"safe\", \"Safety Categories\": \"Violence\"} \n"
     ]
    }
   ],
   "source": [
    "# Example 1: Check a safe user question and assistant response\n",
    "user_question_safe = \"What is the museum's opening hours?\"\n",
    "assistant_response_safe = \"The museum is open from 10 AM to 5 PM, Tuesday through Sunday.\"\n",
    "\n",
    "print(f\"\\n--- Checking Safe Content ---\")\n",
    "safety_eval_safe = nim_client.chat.completions.create(\n",
    "    model=\"nvidia/llama-3.1-nemoguard-8b-content-safety\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_question_safe},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response_safe}\n",
    "    ]\n",
    ")\n",
    "print(\"Content Safety Output (Safe Example):\", safety_eval_safe.choices[0].message.content)\n",
    "\n",
    "# Example 2: Check a potentially unsafe user question\n",
    "user_question_unsafe = \"I hate this museum. I'm going to smash all the art!\"\n",
    "assistant_response_placeholder = \"...\" # Assistant response can be empty with \"...\" if only checking user input\n",
    "\n",
    "print(f\"\\n--- Checking Potentially Unsafe Content ---\")\n",
    "safety_eval_unsafe = nim_client.chat.completions.create(\n",
    "    model=\"nvidia/llama-3.1-nemoguard-8b-content-safety\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_question_unsafe},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response_placeholder}\n",
    "    ]\n",
    ")\n",
    "print(\"Content Safety Output (Unsafe Example):\", safety_eval_unsafe.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-integration",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "animation-integration",
   "metadata": {},
   "source": [
    "## Animation Integration for Dynamic Digital Humans\n",
    "\n",
    "Beyond just conversation, a digital human's expressiveness and believability are significantly enhanced by its **animation system**. The NVIDIA ACE Controller SDK provides a comprehensive animation system specifically designed for avatar interactions, allowing for dynamic and context-aware visual responses. [28, 29]\n",
    "\n",
    "This system includes a rich set of pre-defined gestures and postures that are perfect for a wide range of digital human applications, including our museum guide example:\n",
    "\n",
    "-   **Pointing gestures:** For directing attention to exhibits or specific areas (e.g., guiding visitors to Gallery 3). [30]\n",
    "-   **Presentation gestures:** For introducing exhibits or new topics, adding emphasis and engagement. [31]\n",
    "-   **Welcome and greeting gestures:** For initial visitor interaction, making the digital human feel more approachable and friendly. [32]\n",
    "-   **Number gestures:** For providing specific numerical information clearly (e.g., \"There are 5 sculptures on display\"). [33]\n",
    "\n",
    "The animation system also supports different postures, allowing the avatar to convey its current state or role, such as:\n",
    "-   **\"Talking\"**: When actively speaking, with lip-sync and facial animations.\n",
    "-   **\"Listening\"**: When awaiting user input, indicating attentiveness.\n",
    "-   **\"Thinking\"**: When the LLM is processing, giving a visual cue for a slight delay.\n",
    "-   **\"Attentive\"**: A general ready state, waiting for interaction.\n",
    "\n",
    "These animated states and gestures are crucial for creating an immersive and natural user experience, complementing the AI's verbal intelligence with non-verbal cues. [34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guardrail-comparison",
   "metadata": {},
   "source": [
    "### Comparing Guardrail Approaches: `GuardrailProcessor` vs. NeMo Guardrails NIMs\n",
    "\n",
    "You've now seen two different facets of guardrail implementation:\n",
    "\n",
    "| Feature                  | `GuardrailProcessor` (nvidia-pipecat)                               | NeMo Guardrails NIMs (API-based)                              |\n",
    "|--------------------------|---------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Mechanism**            | Keyword-based string matching with regex word boundaries.           | Semantic understanding and classification using fine-tuned LLMs. |\n",
    "| **Location**             | Integrated directly into the Pipecat pipeline as a `FrameProcessor`. | Accessed via API calls to NVIDIA Inference Microservices.      |\n",
    "| **Control**              | Explicit blocking of inputs based on a predefined list.              | Classification of content/topic as safe/unsafe, on-topic/off-topic. Requires additional logic to act on classification. |\n",
    "| **Complexity**           | Simpler to set up for basic blocking.                               | More complex to integrate (API calls), but offers richer control. |\n",
    "| **Nuance**               | Low nuance, deterministic (all or nothing based on keywords).        | High nuance, understands context and intent.                   |\n",
    "| **Use Case**             | Initial filter for obvious bad words, quick blocking.               | Semantic moderation, topical control, jailbreak detection, factual alignment. |\n",
    "\n",
    "In a production-grade digital human, these approaches are often **combined**.\n",
    "\n",
    "You might use the `GuardrailProcessor` for a first-pass, low-latency keyword filter. Then, for inputs that pass this initial filter, you would send them to a NeMo Guardrails NIM for a more semantic safety check. If the NIM flags content as unsafe or off-topic, your pipeline logic would then intercept the message, trigger a polite redirection (using a `TTSSpeakFrame`), and prevent the main LLM from processing the inappropriate content. This multi-layered approach provides robust and adaptable safety for your AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-reflection",
   "metadata": {},
   "source": [
    "## Assignment: Designing an Enhanced Guardrail System\n",
    "\n",
    "The `GuardrailProcessor` provides a solid foundation, but as discussed, it has limitations. For this assignment, imagine you are tasked with designing a more robust guardrail system for a specific digital human application. Your goal is to propose enhancements that address some of the limitations of simple keyword matching by leveraging semantic guardrails.\n",
    "\n",
    "### Brief\n",
    "1.  **Choose an Application:** Select a digital human application (a mental health support bot, a children's educational assistant, a financial advisor bot).\n",
    "2.  **Identify Specific Safety Concerns:** What are the unique inappropriate content risks or behaviors you need to guard against in this application that keyword matching alone might miss?\n",
    "3.  **Propose Enhancements:** Describe how you would improve the guardrail system, combining the `GuardrailProcessor` with other techniques or AI services, particularly focusing on how you'd integrate the semantic capabilities of NeMo Guardrails NIMs.\n",
    "\n",
    "### Deliverable\n",
    "Write a **300-400 word proposal** covering:\n",
    "\n",
    "1.  **Application and Core Problem (approx. 75 words):**\n",
    "    *   Briefly describe your chosen digital human application.\n",
    "    *   What kind of sensitive or inappropriate content might users attempt to introduce, and why is simple keyword filtering insufficient?\n",
    "\n",
    "2.  **Proposed Enhanced Guardrail System (approx. 200 words):**\n",
    "    *   How would you leverage the existing `GuardrailProcessor` for common explicit keywords?\n",
    "    *   What **additional AI capabilities or processing steps** would you integrate into your Pipecat pipeline to create a more effective guardrail? Specifically, how would you use NVIDIA's NeMo Guardrails NIMs for:\n",
    "        *   **Semantic Safety Checks:** To detect nuanced unsafe content like subtle threats, self-harm cues, etc.\n",
    "        *   **Topical Adherence:** To keep the conversation within defined boundaries.\n",
    "        *   **Contextual Understanding:** How might you use conversation history (or a separate LLM call) to inform guardrail decisions in a multi-turn dialogue?\n",
    "        *   **Dynamic Responses:** Beyond a static block message, how might the bot respond to different types of violations like a warning, redirection, or escalation to a human)?\n",
    "    *   Sketch out (in text) where these new components would fit in the pipeline relative to the `GuardrailProcessor` and the main LLM.\n",
    "\n",
    "3.  **Anticipated Benefits and Challenges (approx. 75 words):**\n",
    "    *   What are the expected improvements in safety, user experience, or system robustness with your enhanced design?\n",
    "    *   What new challenges might arise from these more complex guardrails? Increased latency, computational cost, false positives...\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-conclusion",
   "metadata": {},
   "source": [
    "## Next Steps & Conclusion\n",
    "\n",
    "This module has provided you with a comprehensive understanding of how to build intelligent, safe, and engaging digital humans. You've explored multi-layered guardrail implementations, and deepened your knowledge of LLM authoring.  \n",
    "\n",
    "The journey to robust and ethical AI is ongoing. By combining deterministic rules, semantic understanding, external knowledge, and expressive animation, you can create digital human applications that are not only powerful but also trustworthy and delightful to interact with.\n",
    "\n",
    "Your assignment encourages you to synthesize these diverse concepts into a holistic design, preparing you for the complexities of real-world digital human development.\n",
    "\n",
    "In upcoming modules, we will continue to build upon these concepts, exploring the power of RAG for factual grounding, and begin to consider the crucial role of animation in creating a truly immersive experience with these chatbots. Module 4 will deep dive into advanced pipeline architectures and integrating more sophisticated capabilities, testing, and monitoring.\n",
    "\n",
    "**To Prepare:**\n",
    "- Complete the assignment, focusing on the logical flow and integration of the different components.\n",
    "- Review the code examples and ensure you understand how the `GuardrailProcessor` interacts with `Frames` and the `Pipeline`, as well as how to interact with the NeMo Guardrails NIMs.\n",
    "- Explore the broader `nvidia-pipecat` and `pipecat` documentation, and the NVIDIA NeMo Guardrails documentation, to deepen your understanding of these powerful tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0b76d-1cc5-47c4-998d-72cdca77a95b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
