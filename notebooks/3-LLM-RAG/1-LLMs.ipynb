{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e040f300-9536-48f9-96ff-5b9b29576639",
   "metadata": {},
   "source": [
    "things to add here\n",
    "\n",
    "\n",
    "- Intro to LLM authoring\n",
    "- \"the mind\" piece connecting to the speech we implemented in module 1\n",
    "- Introduce idea: any model you use is very generic. we need to rail. provide context\n",
    "\n",
    "\n",
    "Showing basic inference with NIM\n",
    "basic authoring prompt for museum guide\n",
    "noting hallucinations, user prompting for things that would require RAG/Guardrails\n",
    "\n",
    "\n",
    "- how llms interpret prompts\n",
    "- intro to context - system prompt vs user messages\n",
    "- intro to memeory conversational history.\n",
    "- touch on shaping behavior using prompts, how rails & rag & tool calling start to come in\n",
    "\n",
    "highlight more on problems & limitations\n",
    "\n",
    "\n",
    "touch on using different models to see which you prefer for your domain. touch on fine-tune vs rag vs prompt engineering\n",
    "\n",
    "show swapping the LLM NIM with nemotron-mini - understand differences in output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac8719-7fa5-4ec8-909a-c7c072efb042",
   "metadata": {},
   "source": [
    "# Content\n",
    "[Basic Prompting](#Basic Prompting) \n",
    "Role Prompting\n",
    "Zero-shot vs few-shot examples\n",
    "Chain of Thought\n",
    "Prompt clarity & specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2861e4-b38a-4008-a024-565ada0f5820",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "Learning Objectives:\n",
    "- Understand the common characteristics of weak or “bad” prompts.\n",
    "- Examine how leading LLM providers like OpenAI and Anthropic automatically enhance or guide prompt generation.\n",
    "- Experiment with prompt refinement to improve generative outputs.\n",
    "- Reflect on UX design patterns that quietly assist users in creating better prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad5960-8efe-4ce8-b063-a8013684bb39",
   "metadata": {},
   "source": [
    "### What Makes A Prompt \"Good\"?\n",
    "Many new users (and even experienced developers) struggle to write effective prompts. Common issues with prompts include being:\n",
    "- Too short — lacks context or necessary instructions.\n",
    "- Too vague — offers no concrete examples or desired style.\n",
    "- Under-specified — doesn’t guide the model toward a clear, useful output.\n",
    "\n",
    "The result? Uninspiring, inconsistent, or irrelevant AI responses. In AI systems it is important to remember that **prompt quality determines output quality**.\n",
    "\n",
    "Among new or novice prompters, prompts are generally too short. They lack context, rarely feature examples, and provide few descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e75ee5-3d68-4946-a3da-572475bf136e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
