{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "module-title-welcome",
   "metadata": {},
   "source": [
    "# Module 3.3: Retrieval-Augmented Generation (RAG) for Knowledgeable Digital Humans\n",
    "\n",
    "Welcome to Module 3.3 of the Digital Human Teaching Kit! In previous modules, you've mastered the fundamentals of LLM integration and refined digital human behavior through prompt engineering (Module 3.1), and established safety boundaries with various guardrails (Module 3.2). While powerful, even the most advanced LLMs have limitations: their knowledge is static (limited to their training data) and they can sometimes \"hallucinate\" or provide incorrect information. [16, 25]\n",
    "\n",
    "This module introduces **Retrieval-Augmented Generation (RAG)**, a transformative technique that empowers your digital human to access, interpret, and cite up-to-date, factual information from external knowledge bases. We will explore how RAG bridges the gap between general AI and domain-specific expertise, enabling your digital human to become a truly knowledgeable and trustworthy agent, such as a museum guide. We'll examine the core RAG pipeline, implement a **simple local RAG example**, and understand how NVIDIA's specialized RAG services and blueprints fit into this powerful paradigm. [1, 5]\n",
    "\n",
    "## Learning Objectives\n",
    "- Explain the core concepts of Retrieval-Augmented Generation (RAG) and its importance for digital humans.\n",
    "- Understand the three main stages of a RAG pipeline: retrieval, ranking, and generation.\n",
    "- Implement a simple, runnable RAG pipeline using open-source tools (LangChain, FAISS) combined with NVIDIA NIMs.\n",
    "- Identify how `NvidiaRAGService` (from the ACE Controller SDK) integrates with deployed RAG servers for production use cases.\n",
    "- Conceptualize the flow of data and context injection within a RAG-enabled digital human pipeline.\n",
    "- Explore practical use cases for RAG in domain-specific applications like a museum guide.\n",
    "- Discuss how RAG complements LLM authoring and guardrails for a multi-layered, robust AI system.\n",
    "\n",
    "## Prerequisites\n",
    "- Strong Python programming skills.\n",
    "- Familiarity with Pipecat core concepts: `Frames`, `Processors`, and `Pipelines` (Module 1.1).\n",
    "- Understanding of LLM integration, context management, and prompt engineering (Module 3.1).\n",
    "- Familiarity with guardrails for conversational AI (Module 3.2).\n",
    "- An active NVIDIA API Key for accessing NVIDIA NIMs.\n",
    "- **Additional Python packages:** You will need to install `faiss-cpu`, `langchain-community`, and `sentence-transformers` for the practical RAG example in this notebook.\n",
    "\n",
    "    ```bash\n",
    "    pip install faiss-cpu langchain-community sentence-transformers\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "code-imports-setup",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnvidia_pipecat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnvidia_rag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NvidiaRAGCitation, NvidiaRAGCitationsFrame, NvidiaRAGSettingsFrame\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService # Uncomment if you have a RAG server running\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# LangChain specific imports for the local RAG example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import getpass\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from pipecat.frames.frames import Frame, TextFrame, EndFrame, StartFrame, TranscriptionFrame, TTSSpeakFrame\n",
    "from pipecat.observers.base_observer import BaseObserver\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineTask, PipelineParams, FrameDirection\n",
    "from pipecat.processors.frame_processor import FrameProcessor\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "from pipecat.processors.frameworks.langchain import LangchainProcessor\n",
    "\n",
    "# NVIDIA specific services from nvidia-pipecat\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "from nvidia_pipecat.frames.nvidia_rag import NvidiaRAGCitation, NvidiaRAGCitationsFrame, NvidiaRAGSettingsFrame\n",
    "# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService # Uncomment if you have a RAG server running\n",
    "\n",
    "# LangChain specific imports for the local RAG example\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # For running asyncio in Jupyter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"NVIDIA API key not found or invalid in .env file.\")\n",
    "    api_key = getpass.getpass(\"üîê Enter your NVIDIA API key: \").strip()\n",
    "    assert api_key.startswith(\"nvapi-\"), f\"{api_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = api_key\n",
    "else:\n",
    "    print(\"NVIDIA API key loaded from .env file.\")\n",
    "\n",
    "# Initialize a dummy OpenAI client for demonstration (for direct NIM calls if needed)\n",
    "nim_client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ.get(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "class SimpleChatObserver(BaseObserver):\n",
    "    \"\"\"A simple observer to print streamed responses from LLM-like services.\"\"\"\n",
    "    async def on_push_frame(self, src: FrameProcessor, dst: FrameProcessor, frame: Frame, direction: FrameDirection, timestamp: int):\n",
    "        if isinstance(frame, TextFrame):\n",
    "            print(frame.text, end=\"\", flush=True)\n",
    "        elif isinstance(frame, NvidiaRAGCitationsFrame):\n",
    "            print(\"\\n--- Citations ---\")\n",
    "            for i, citation in enumerate(frame.citations):\n",
    "                print(f\"[{i+1}] Document: {citation.document_name}, Score: {citation.score:.2f}\")\n",
    "                print(f\"    Content Snippet: {citation.content.decode('utf-8')[:100]}...\") # Decode and truncate for display\n",
    "            print(\"-----------------\")\n",
    "        elif isinstance(frame, EndFrame):\n",
    "            print() # Newline after response completes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-rag",
   "metadata": {},
   "source": [
    "# Introduction: What is RAG and Why It Matters for Digital Humans\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge during the generation process. In the context of building digital humans using NVIDIA's ACE Controller framework, RAG serves as a critical component that bridges the gap between general language understanding and domain-specific expertise. [1]\n",
    "\n",
    "For your digital human, such as a museum guide agent, RAG transforms a general-purpose conversational AI into a knowledgeable curator. While your `NvidiaLLMService` provides natural language capabilities and your `GuardrailProcessor` ensures safe interactions, RAG adds the crucial ability to access and incorporate factual information from your museum's artifact database, historical records, and curatorial expertise. [1, 5, 10]\n",
    "\n",
    "RAG addresses one of the fundamental challenges in deploying AI systems in educational and factual settings: **hallucination**. By grounding responses in your museum's verified content, RAG significantly reduces the risk of providing incorrect historical facts or misattributing artwork. [16]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how-rag-works",
   "metadata": {},
   "source": [
    "## How RAG Works: The Three-Stage Pipeline\n",
    "\n",
    "The RAG process, as implemented in systems like the NVIDIA RAG Blueprint, can be broken down into three main stages, which work in concert to deliver accurate and relevant responses: [1]\n",
    "\n",
    "### 1. Retrieval Stage\n",
    "When a user asks a question (e.g., \"Tell me about the Ming Dynasty vase in room 3\"), the RAG system first searches through your configured document collection to find relevant information. The `NvidiaRAGService` (part of the ACE Controller SDK) interacts with a RAG server that typically uses a vector database (VDB) approach where documents are converted into numerical representations called *embeddings*. The user's query is also converted into an embedding, and then semantically similar (i.e., relevant) content is retrieved from the VDB. [2]\n",
    "\n",
    "The retrieval process is controlled by parameters like `vdb_top_k`, which determines how many top-ranked document chunks to initially retrieve from the knowledge base. A higher `vdb_top_k` value means retrieving more potential candidates. [2, 11]\n",
    "\n",
    "### 2. Ranking/Reranking Stage\n",
    "Not all retrieved documents are equally relevant or may contain redundant information. To optimize the quality of the context provided to the LLM, a reranker component is used. This component scores and prioritizes the most pertinent information from the initial retrieval set. This stage is controlled by the `reranker_top_k` parameter, which typically selects a smaller number of higher-quality chunks from the initial `vdb_top_k` results. This step is crucial for ensuring the LLM receives the most concise and relevant information. [3, 11]\n",
    "\n",
    "### 3. Generation with Context Injection\n",
    "The selected, high-quality documents from the reranking stage are then injected into the LLM prompt as context. The LLM receives this *augmented* prompt, which now contains the user's original question plus the retrieved factual information. The LLM then generates a response that incorporates this retrieved knowledge, creating answers that are both conversational and factually grounded. [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lab-local-rag",
   "metadata": {},
   "source": [
    "## Lab: Building a Simple Local RAG Pipeline for a Museum Guide\n",
    "\n",
    "To demonstrate RAG concepts hands-on, we'll build a simplified RAG pipeline directly within this notebook. This example utilizes popular open-source libraries like LangChain and FAISS (a vector database library) to handle document processing and retrieval, combined with NVIDIA NIM for LLM inference. While `nvidia-pipecat`'s `NvidiaRAGService` is designed for integration with a full RAG server, this local setup will allow you to see the RAG workflow in action. [3, 4]\n",
    "\n",
    "Our museum guide will answer questions based on a small, predefined set of museum-related documents. We'll use `LangchainProcessor` from `pipecat` to integrate our RAG chain into a Pipecat pipeline, allowing for streaming responses.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1.  **Document Preparation:** Define a small corpus of museum-related text. Split it into manageable chunks.\n",
    "2.  **Embedding & Vector Database:** Convert text chunks into numerical embeddings using a `HuggingFaceEmbeddings` model. Store these embeddings in a local FAISS vector database.\n",
    "3.  **LLM Setup:** Initialize an `ChatOpenAI` instance pointing to an NVIDIA NIM LLM endpoint.\n",
    "4.  **RAG Chain Creation:** Assemble a `RetrievalQA` chain (from LangChain) that connects the LLM with the vector database retriever.\n",
    "5.  **Pipecat Integration:** Wrap the LangChain RAG chain in a `LangchainProcessor` and run it within a Pipecat `Pipeline`.\n",
    "6.  **Query & Observe:** Send user queries through the pipeline and observe the RAG-augmented responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-local-rag-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_and_run_simple_rag_pipeline(user_query: str):\n",
    "    print(f\"\\n--- Running Simple Local RAG Pipeline for query: '{user_query}' ---\")\n",
    "\n",
    "    # 1. Create a small document collection for our museum guide\n",
    "    museum_documents = [\n",
    "        \"The Modern Art & Technology Museum features a groundbreaking exhibit on AI-generated art in the West Gallery. It explores how machine learning reshapes artistic expression.\",\n",
    "        \"Vincent van Gogh's 'The Starry Night' is a masterpiece from 1889, showcasing his unique post-impressionist style. It depicts a dramatic night sky over a tranquil village.\",\n",
    "        \"The museum is open from 10 AM to 5 PM, Tuesday through Sunday. Admission is $20 for adults, and free for members.\",\n",
    "        \"Our gift shop offers a wide array of art books, unique souvenirs, and reproductions of famous artworks. It closes 15 minutes before the museum.\",\n",
    "        \"The ancient Egyptian artifacts collection is located on the second floor, featuring sarcophagi, hieroglyphs, and sculptures dating back to 2000 BCE.\"\n",
    "    ]\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20) # Adjusted chunk size/overlap\n",
    "    texts = text_splitter.create_documents(museum_documents)\n",
    "\n",
    "    # Create embeddings and a local FAISS vector database\n",
    "    # Using a general-purpose embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    print(\"Local FAISS vector database created.\")\n",
    "\n",
    "    # 2. Create NVIDIA NIM LLM (using OpenAI-compatible interface for LangChain). We'll use a strong instruct model from NVIDIA NIMs\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    "        model=\"nvidia/llama-3.1-nemotron-4-340b-instruct\" # Using a powerful model\n",
    "    )\n",
    "    print(f\"LLM initialized: {llm.model_name}\")\n",
    "\n",
    "    # 3. Create RAG chain using LangChain's RetrievalQA\n",
    "    # 'stuff' chain_type puts all retrieved documents into the prompt.\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 most relevant chunks\n",
    "    )\n",
    "    print(\"LangChain RetrievalQA chain created.\")\n",
    "\n",
    "    # 4. Create Pipecat pipeline with LangchainProcessor\n",
    "    # The LangchainProcessor takes a LangChain Runnable (like our qa_chain) and processes TextFrames.\n",
    "    # 'transcript_key' tells the processor where to put the incoming text in the LangChain chain's input.\n",
    "    langchain_processor = LangchainProcessor(qa_chain, transcript_key=\"query\")\n",
    "\n",
    "    # Define a simple pipeline with our LangchainProcessor\n",
    "    pipeline = Pipeline([langchain_processor])\n",
    "    \n",
    "    # Create a PipelineTask and Runner\n",
    "    task = PipelineTask(\n",
    "        pipeline,\n",
    "        params=PipelineParams(observers=[SimpleChatObserver()]) # Use our observer to print results\n",
    "    )\n",
    "    runner = PipelineRunner()\n",
    "    \n",
    "    # Start the pipeline in the background\n",
    "    run_task = asyncio.create_task(runner.run(task))\n",
    "    await asyncio.sleep(0.1) # Give pipeline time to initialize\n",
    "\n",
    "    # 5. Send the user query (as a TextFrame) into the pipeline\n",
    "    print(\"\\nSending query to RAG pipeline...\")\n",
    "    await task.queue_frame(TextFrame(user_query))\n",
    "    await task.queue_frame(EndFrame()) # Signal end of input\n",
    "\n",
    "    # Wait for the pipeline to complete processing\n",
    "    await run_task\n",
    "    print(\"--- RAG Pipeline Execution Completed ---\")\n",
    "\n",
    "# Run some example queries\n",
    "await create_and_run_simple_rag_pipeline(\"Where is the AI art exhibit?\")\n",
    "await create_and_run_simple_rag_pipeline(\"Tell me about The Starry Night.\")\n",
    "await create_and_run_simple_rag_pipeline(\"What are the museum hours?\")\n",
    "await create_and_run_simple_rag_pipeline(\"Where is the ancient Egyptian collection?\")\n",
    "await create_and_run_simple_rag_pipeline(\"What souvenirs can I buy?\")\n",
    "\n",
    "# Query that should not be in the knowledge base, potentially leading to a more generic or evasive LLM response\n",
    "await create_and_run_simple_rag_pipeline(\"Who painted the Mona Lisa?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-integration-architecture",
   "metadata": {},
   "source": [
    "## RAG Integration in the `nvidia-pipecat` Architecture (Production-Ready)\n",
    "\n",
    "The `NvidiaRAGService` (part of the ACE Controller SDK's `nvidia-pipecat` library) is designed for seamless integration with NVIDIA's full RAG server deployments, which can be deployed as NVIDIA NIMs or on your own infrastructure. This service simplifies complex RAG workflows into a manageable Pipecat component. [4, 5, 17]\n",
    "\n",
    "The `NvidiaRAGService` itself doesn't host the knowledge base; it communicates with a separate RAG server that handles the heavy lifting of document indexing and retrieval. This RAG server typically follows architectures defined by NVIDIA's Generative AI Examples (like the [NVIDIA RAG Blueprint](https://github.com/NVIDIA-AI-Blueprints/rag)). [5, 17]\n",
    "\n",
    "### Frame Processing Flow (Conceptual for `NvidiaRAGService`)\n",
    "Here's a conceptual flow demonstrating where `NvidiaRAGService` fits into a production-grade digital human pipeline:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"User Query (Voice/Text)\"] --> B[\"ASR (TranscriptionFrame)\"]\n",
    "    B --> C[\"GuardRailProcessor (Keyword/Semantic)\"]\n",
    "    C --> D[\"OpenAILLMContext (Manage History)\"]\n",
    "    D -- User Question --> E[\"NvidiaRAGService (Retrieval/Ranking)\"]\n",
    "    E -- Query to RAG Server --> F[\"RAG Server (Vector DB, Documents)\"]\n",
    "    F --> E -- Retrieved Chunks + Citations --> G[\"LLM (NvidiaLLMService) with Augmented Context\"]\n",
    "    G --> H[\"TTS (TTSSpeakFrame)\"]\n",
    "    H --> I[\"Animation (Avatar Motion)\"]\n",
    "    I --> J[\"Digital Human Response\"]\n",
    "```\n",
    "\n",
    "The `NvidiaRAGService` processes `OpenAILLMContext` objects, extracts the conversation history, and enriches the context with retrieved documents before sending it to the LLM for final generation. This process can also yield `NvidiaRAGCitationsFrame`s for traceability. [6, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep-dive-nvidiaragservice",
   "metadata": {},
   "source": [
    "## Deep Dive: `NvidiaRAGService` (from ACE Controller SDK)\n",
    "\n",
    "The `NvidiaRAGService` class, derived from `OpenAILLMService`, provides the official interface to NVIDIA's RAG server. It abstracts away the complexities of interacting with the RAG backend, allowing you to focus on building your digital human's logic. [1]\n",
    "\n",
    "### Basic Configuration and Parameters\n",
    "\n",
    "When initializing `NvidiaRAGService`, you'll set key parameters that control its interaction with the RAG backend and the LLM: [7]\n",
    "\n",
    "-   **`collection_name`**: A string identifier for your specific document collection within the RAG server (e.g., \"modern_art_museum_collection\"). This tells the RAG server which knowledge base to query.\n",
    "-   **`rag_server_url`**: The HTTP/S endpoint URL of your deployed NVIDIA RAG server (e.g., `\"http://localhost:8081\"` for a local deployment, or a cloud NIM endpoint). [1]\n",
    "-   **`use_knowledge_base`**: A boolean flag (`True`/`False`) to enable or disable RAG retrieval for a given query. This allows you to toggle RAG functionality at runtime.\n",
    "-   **`temperature`, `top_p`, `max_tokens`**: These parameters control the LLM's generation behavior (randomness, diversity, length) *after* the RAG context has been injected. They function identically to the parameters in `NvidiaLLMService`. [21]\n",
    "-   **`vdb_top_k`**: An integer indicating the number of initial document chunks to retrieve from the vector database. A higher value means more documents are considered by the reranker. [2, 11]\n",
    "-   **`reranker_top_k`**: An integer specifying how many top-ranked documents to select after the reranking stage. These are the chunks that will actually be sent to the LLM as context. A lower value here prioritizes precision. [3, 11]\n",
    "-   **`enable_citations`**: A boolean (`True`/`False`) to control whether the RAG service should return detailed source references (`NvidiaRAGCitationsFrame`) along with the LLM's generated response. [9]\n",
    "-   **`suffix_prompt`**: An optional string appended to the last user message before sending it to the RAG server. This can be used for subtle prompt injection specific to RAG queries.\n",
    "-   **`stop_words`**: Words that stop LLM generation. [1]\n",
    "\n",
    "```python\n",
    "# Conceptual instantiation of NvidiaRAGService\n",
    "# (Requires a running NVIDIA RAG server, which is external to this notebook's execution environment)\n",
    "\n",
    "# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService\n",
    "\n",
    "# rag_service = NvidiaRAGService(\n",
    "#     collection_name=\"my_museum_artifacts\",\n",
    "#     rag_server_url=\"http://your-rag-server-ip:port\", # IMPORTANT: Replace with your actual RAG server URL\n",
    "#     use_knowledge_base=True,\n",
    "#     enable_citations=True,\n",
    "#     vdb_top_k=20,       # Initial retrieval of 20 document chunks\n",
    "#     reranker_top_k=4,   # Select top 4 after re-ranking for LLM context\n",
    "#     temperature=0.2,    # Control LLM creativity\n",
    "#     max_tokens=500,     # Limit LLM response length\n",
    "#     stop_words=[\"thank you\"]\n",
    "# )\n",
    "```\n",
    "\n",
    "### Dynamic Configuration with `NvidiaRAGSettingsFrame`\n",
    "\n",
    "The `NvidiaRAGService` supports runtime configuration changes through the `NvidiaRAGSettingsFrame`. When this frame is pushed into the pipeline, the service's internal settings are updated. This allows you to dynamically adjust parameters‚Äîfor instance, switching between different museum collections (e.g., \"ancient_history\" vs. \"renaissance_art\") or modifying retrieval settings based on the user's current context or explicit preferences. The `_update_settings` method handles these changes internally. [8]\n",
    "\n",
    "```python\n",
    "# Conceptual example of dynamically updating RAG settings in a pipeline\n",
    "# await pipeline_task.queue_frame(NvidiaRAGSettingsFrame({\n",
    "#     \"collection_name\": \"new_exhibit_documents\",\n",
    "#     \"vdb_top_k\": 10, # Adjust retrieval for new context\n",
    "#     \"enable_citations\": False\n",
    "# }))\n",
    "```\n",
    "\n",
    "### Citation Handling (`NvidiaRAGCitationsFrame`)\n",
    "\n",
    "One of RAG's most valuable features for museum applications is its robust citation support. When `enable_citations` is `True`, the `NvidiaRAGService` can return `NvidiaRAGCitationsFrame` objects along with the generated `TextFrame`s. These citation frames contain detailed source information derived from the retrieved documents. [9]\n",
    "\n",
    "As seen in the `SimpleChatObserver` at the beginning of this notebook, each citation typically includes: [9]\n",
    "-   `document_type`: The type of source (e.g., \"Exhibit Label\", \"Research Paper\", \"Artifact Record\").\n",
    "-   `document_id`: A unique identifier for the source document.\n",
    "-   `document_name`: A human-readable name for the source (e.g., \"The Starry Night Exhibit Description\").\n",
    "-   `content`: The exact snippet of text from the source that was used to generate the response.\n",
    "-   `metadata`: Any additional, structured information about the source.\n",
    "-   `score`: A relevance score from the retrieval/ranking process.\n",
    "\n",
    "This allows your museum guide to not only provide accurate information but also direct visitors to specific artifacts, display cases, or additional online resources, enhancing the visitor experience and trustworthiness. [9, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "museum-domain-use-cases",
   "metadata": {},
   "source": [
    "## Museum Domain Use Cases: RAG in Action\n",
    "\n",
    "Let's explore practical scenarios where RAG transforms a generic digital human into an expert museum guide:\n",
    "\n",
    "### 1. Artifact Information Queries\n",
    "When visitors ask about specific pieces (e.g., \"Tell me about the blue vase in Gallery 7\"), RAG retrieves detailed curatorial descriptions, historical context, acquisition details, and related artifacts from your collection management system. The LLM then synthesizes this information into a natural, conversational response. [13]\n",
    "\n",
    "### 2. Contextual Recommendations\n",
    "A visitor might ask, \"What else should I see if I'm interested in Roman sculpture?\" RAG can access thematic connections across your collection, identifying related artists, periods, or geographical origins, providing personalized tour suggestions that a general LLM would struggle with. [13]\n",
    "\n",
    "### 3. Historical Context\n",
    "RAG enables your guide to draw from extensive historical databases, academic papers, and curatorial research to provide rich contextual information about time periods, artistic movements, or cultural significance that might be too vast or too niche for an LLM's pre-trained knowledge. [13]\n",
    "\n",
    "### 4. Multilingual Support\n",
    "By configuring different document collections (or multilingual embeddings) for different languages, your museum guide can serve international visitors in their preferred language while maintaining factual accuracy across all stored information. [14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-complements",
   "metadata": {},
   "source": [
    "## How RAG Complements LLM and Guardrails: A Multi-Layered Approach\n",
    "\n",
    "A truly robust digital human system relies on the synergy of multiple AI components. RAG doesn't replace LLMs or guardrails; it enhances them, forming a powerful, multi-layered defense and capability system.\n",
    "\n",
    "### The Three-Layer Defense (Revisited with RAG)\n",
    "\n",
    "1.  **GuardrailProcessor (Keyword-Based)**: Acts as the first line of defense, quickly blocking explicit inappropriate queries before they consume resources or reach more complex systems. [10, 15]\n",
    "2.  **NeMo Guardrails NIMs (Semantic Safety & Topical)**: Provides a deeper, semantic layer of safety and topical control, ensuring queries align with acceptable conversation boundaries. [1, 10, 15]\n",
    "3.  **RAG Knowledge Filtering**: Ensures responses are grounded in authoritative museum content rather than potentially unreliable general knowledge from the LLM, directly addressing the hallucination problem. [16]\n",
    "4.  **LLM Safety**: The underlying language model (e.g., Llama 3.1) still provides inherent safety measures and maintains conversational appropriateness based on its training, acting as a final check. [1]\n",
    "\n",
    "### Accuracy and Trustworthiness\n",
    "\n",
    "RAG fundamentally addresses one of the most critical challenges in deploying AI systems in educational and informational settings: **hallucination**. By grounding responses in your museum's verified content, RAG significantly reduces the risk of providing incorrect historical facts or misattributing artwork. [16]\n",
    "\n",
    "The citation system provides transparency, allowing visitors to verify information and museum staff to trace the source of any responses that need correction, building greater user trust. [9, 16]\n",
    "\n",
    "### Scalability and Maintenance\n",
    "\n",
    "Unlike fine-tuning approaches (which require retraining the entire model for knowledge updates), RAG allows you to update your digital human's knowledge simply by updating the document collection in your vector database. New acquisitions, updated attributions, or revised interpretations can be incorporated without retraining the entire system, making it highly scalable and easier to maintain. [17]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animation-integration",
   "metadata": {},
   "source": [
    "## Animation Integration for Dynamic Digital Humans (Building on Module 3.1)\n",
    "\n",
    "While RAG provides factual depth, a digital human's expressiveness and believability are significantly enhanced by its **animation system**. The NVIDIA ACE Controller SDK provides a comprehensive animation system specifically designed for avatar interactions, allowing for dynamic and context-aware visual responses. [28, 29]\n",
    "\n",
    "How RAG interacts with animation is fascinating: the *retrieved content* can directly influence the avatar's non-verbal communication. For instance:\n",
    "\n",
    "-   If RAG retrieves information about a specific location, the avatar could use **pointing gestures** to direct attention (e.g., \"The sculpture you asked about is in Gallery 3, over there.\"). [30]\n",
    "-   When presenting detailed historical facts retrieved via RAG, the avatar might employ **presentation gestures** to emphasize key points, adding to the informative experience. [31]\n",
    "-   The avatar's overall state (e.g., \"Attentive\" when listening, \"Thinking\" during retrieval, \"Talking\" when responding) enhances the natural interaction, providing visual cues that complement the verbal and factual information. [34]\n",
    "\n",
    "These animated states and gestures are crucial for creating an immersive and natural user experience, making the digital human truly come alive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-considerations",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "The RAG service includes configurable parameters for balancing response quality with latency. The `vdb_top_k` (number of initial documents retrieved) and `reranker_top_k` (number of documents selected after re-ranking) parameters allow you to tune the trade-off between comprehensive knowledge retrieval and response speed. Higher values for `top_k` parameters can lead to more accurate answers but also increased latency. [11]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-exploration",
   "metadata": {},
   "source": [
    "## Assignment: Designing a RAG-Enhanced Digital Human for a Specific Domain\n",
    "\n",
    "This assignment challenges you to design a digital human application that leverages RAG to provide expert, factual information. You'll need to define a specific domain, outline the RAG components, and explain how it integrates into the broader digital human pipeline, considering LLM authoring, guardrails, and animation.\n",
    "\n",
    "### Brief\n",
    "1.  **Select a Domain:** Choose a domain (e.g., a specialized medical assistant, a technical support agent for a complex product, a historical tour guide for a specific landmark, a legal information bot).\n",
    "2.  **Define the Knowledge Base:** What kind of information would your digital human need access to, and how would it be structured for RAG?\n",
    "3.  **Propose the RAG-Enabled Solution:** Describe your digital human and its interaction flow, emphasizing the role of RAG.\n",
    "\n",
    "### Deliverable\n",
    "Write a **400-500 word proposal** covering:\n",
    "\n",
    "1.  **Application and Core Problem (approx. 75 words):**\n",
    "    *   Briefly describe your chosen digital human application and its primary goal within the selected domain.\n",
    "    *   Why is RAG essential for this application, particularly in overcoming LLM limitations like hallucinations or outdated knowledge?\n",
    "\n",
    "2.  **RAG-Enhanced Digital Human Architecture and Capabilities (approx. 300 words):**\n",
    "    *   **Knowledge Base Design:** What specific types of documents or data would constitute your knowledge base (e.g., product manuals, research papers, legal precedents)? How would they be processed for the vector database (e.g., chunking, embedding)?\n",
    "    *   **RAG Workflow:** Describe how a typical user query would flow through your RAG pipeline. Mention the roles of retrieval (e.g., `vdb_top_k`), ranking (e.g., `reranker_top_k`), and context injection into the LLM prompt. How would `NvidiaRAGService` (conceptually) facilitate this?\n",
    "    *   **Complementary AI:**\n",
    "        *   How would this RAG system work alongside **LLM Authoring** (e.g., a specific system prompt for summarization of retrieved content)?\n",
    "        *   How would **Guardrails** (both `GuardrailProcessor` and NeMo Guardrails NIMs) protect against inappropriate queries that might bypass RAG, or filter unsafe content in RAG's retrieved results?\n",
    "        *   How would **Animation** enhance the experience, perhaps by visualizing retrieved data (e.g., pointing to a diagram, using gestures for emphasis when citing sources)?\n",
    "    *   **Citation Strategy:** How would your digital human provide citations to users to build trust and allow for verification?\n",
    "    *   **High-level Data Flow:** Briefly sketch (in text) the order of operations in your Pipecat pipeline, integrating RAG, LLM, Guardrails, and Animation.\n",
    "\n",
    "3.  **Anticipated Benefits and Future Considerations (approx. 75 words):**\n",
    "    *   What are the expected improvements in accuracy, reliability, and user experience for your specific application due to RAG?\n",
    "    *   Briefly discuss one future enhancement for your RAG system (e.g., supporting multimodal document retrieval, incorporating real-time data feeds, or enabling conversational RAG with follow-up questions).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-conclusion",
   "metadata": {},
   "source": [
    "## Next Steps & Conclusion\n",
    "\n",
    "Congratulations! You've now gained a deep understanding of Retrieval-Augmented Generation (RAG) and its crucial role in building knowledgeable and trustworthy digital humans. You've explored its core stages, how it integrates into the `nvidia-pipecat` architecture, and its powerful synergy with LLM authoring, guardrails, and animation.\n",
    "\n",
    "RAG empowers your digital humans to move beyond static knowledge, providing dynamic, factual, and citable responses that significantly enhance their utility and credibility in specialized domains.\n",
    "\n",
    "This module concludes our exploration of the LLM-RAG fundamentals. You now have the conceptual tools to design sophisticated conversational AI systems. Keep experimenting, and apply these powerful techniques to your own innovative digital human projects!\n",
    "\n",
    "**To Prepare:**\n",
    "- Complete the assignment, focusing on a clear, well-reasoned design for your RAG-enabled digital human.\n",
    "- Review the NVIDIA RAG Blueprint documentation (linked in the introduction) to understand the practical aspects of deploying a RAG server.\n",
    "- Reflect on how all the modules so far (Pipecat basics, LLM authoring, guardrails, and RAG) come together to form a complete digital human system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
