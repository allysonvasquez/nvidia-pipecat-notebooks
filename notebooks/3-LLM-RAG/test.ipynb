{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "module-title-welcome",
   "metadata": {},
   "source": [
    "# Module 3.3: Retrieval-Augmented Generation (RAG) for Knowledgeable Digital Humans\n",
    "\n",
    "Welcome to Module 3.3 of the Digital Human Teaching Kit! In previous modules, you've mastered the fundamentals of LLM integration and refined digital human behavior through prompt engineering (Module 3.1), and established safety boundaries with various guardrails (Module 3.2). While powerful, even the most advanced LLMs have limitations: their knowledge is static (limited to their training data) and they can sometimes \"hallucinate\" or provide incorrect information. [16, 25]\n",
    "\n",
    "This module introduces **Retrieval-Augmented Generation (RAG)**, a transformative technique that empowers your digital human to access, interpret, and cite up-to-date, factual information from external knowledge bases. We will explore how RAG bridges the gap between general AI and domain-specific expertise, enabling your digital human to become a truly knowledgeable and trustworthy agent, such as a museum guide. We'll examine the core RAG pipeline, implement a **simple RAG example using prompt stuffing with NVIDIA NIMs**, and understand how NVIDIA's specialized RAG services and blueprints fit into this powerful paradigm. [1, 5]\n",
    "\n",
    "## Learning Objectives\n",
    "- Explain the core concepts of Retrieval-Augmented Generation (RAG) and its importance for digital humans.\n",
    "- Understand the three main stages of a RAG pipeline: retrieval, ranking, and generation.\n",
    "- Implement a simple, runnable RAG pipeline using a prompt-stuffing approach with NVIDIA NIMs and tool calling.\n",
    "- Identify how `NvidiaRAGService` (from the ACE Controller SDK) integrates with deployed RAG servers for production use cases.\n",
    "- Conceptualize the flow of data and context injection within a RAG-enabled digital human pipeline.\n",
    "- Explore practical use cases for RAG in domain-specific applications like a museum guide.\n",
    "- Discuss how RAG complements LLM authoring and guardrails for a multi-layered, robust AI system.\n",
    "\n",
    "## Prerequisites\n",
    "- Strong Python programming skills.\n",
    "- Familiarity with Pipecat core concepts: `Frames`, `Processors`, and `Pipelines` (Module 1.1).\n",
    "- Understanding of LLM integration, context management, and prompt engineering (Module 3.1).\n",
    "- Familiarity with guardrails for conversational AI (Module 3.2).\n",
    "- An active NVIDIA API Key for accessing NVIDIA NIMs.\n",
    "- **Additional Python packages:** For the live demo, you will need to install `loguru` and `openai` (if not already installed).\n",
    "\n",
    "    ```bash\n",
    "    pip install loguru openai\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "code-imports-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA API key loaded from .env file.\n",
      "\u001b[32m15:54:32\u001b[0m \u001b[1m\n",
      "--- Starting Simple Prompt Stuffing RAG Demo with NVIDIA NIMs ---\u001b[0m\n",
      "\u001b[32m15:54:32\u001b[0m \u001b[34m\u001b[1mLinking PipelineSource#0 -> OpenAIUserContextAggregator#0\u001b[0m\n",
      "\u001b[32m15:54:32\u001b[0m \u001b[34m\u001b[1mLinking OpenAIUserContextAggregator#0 -> NvidiaLLMService#0\u001b[0m\n",
      "\u001b[32m15:54:32\u001b[0m \u001b[34m\u001b[1mLinking NvidiaLLMService#0 -> OpenAIAssistantContextAggregator#1\u001b[0m\n",
      "\u001b[32m15:54:32\u001b[0m \u001b[34m\u001b[1mLinking OpenAIAssistantContextAggregator#1 -> PipelineSink#0\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SimpleChatObserver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 261\u001b[39m\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_task\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Run the RAG demo\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_rag_demo()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 231\u001b[39m, in \u001b[36mrun_rag_demo\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    220\u001b[39m rag_response_observer = RAGResponseObserver(target_llm_service=main_llm_service)\n\u001b[32m    222\u001b[39m pipeline = Pipeline([\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# Context aggregator to feed user input into the main LLM's context\u001b[39;00m\n\u001b[32m    224\u001b[39m     main_llm_service.create_context_aggregator(main_llm_context).user(),\n\u001b[32m    225\u001b[39m     main_llm_service,\n\u001b[32m    226\u001b[39m     main_llm_service.create_context_aggregator(main_llm_context).assistant()\n\u001b[32m    227\u001b[39m ])\n\u001b[32m    229\u001b[39m task = PipelineTask(\n\u001b[32m    230\u001b[39m     pipeline,\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     params=PipelineParams(observers=[\u001b[43mSimpleChatObserver\u001b[49m(), rag_response_observer])\n\u001b[32m    232\u001b[39m )\n\u001b[32m    234\u001b[39m runner = PipelineRunner()\n\u001b[32m    235\u001b[39m run_task = asyncio.create_task(runner.run(task))\n",
      "\u001b[31mNameError\u001b[39m: name 'SimpleChatObserver' is not defined"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from pipecat.frames.frames import Frame, TextFrame, EndFrame, StartFrame, TranscriptionFrame, TTSSpeakFrame\n",
    "from pipecat.observers.base_observer import BaseObserver\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineTask, PipelineParams, FrameDirection\n",
    "from pipecat.processors.frame_processor import FrameProcessor\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "# NVIDIA specific services from nvidia-pipecat\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "from nvidia_pipecat.frames.nvidia_rag import NvidiaRAGCitation, NvidiaRAGCitationsFrame, NvidiaRAGSettingsFrame\n",
    "# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService # Uncomment if you have a RAG server running\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # For running asyncio in Jupyter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"NVIDIA API key not found or invalid in .env file.\")\n",
    "    api_key = getpass.getpass(\"üîê Enter your NVIDIA API key: \").strip()\n",
    "    assert api_key.startswith(\"nvapi-\"), f\"{api_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = api_key\n",
    "else:\n",
    "    print(\"NVIDIA API key loaded from .env file.\")\n",
    "\n",
    "logger.remove()\n",
    "logger.add(lambda msg: print(msg, end=''), colorize=True, format=\"<green>{time:HH:mm:ss}</green> <level>{message}</level>\")\n",
    "\n",
    "\n",
    "# --- Global RAG Content for the Prompt Stuffing Demo ---\n",
    "# In a real application, this would be loaded from a vector DB via a RAG service.\n",
    "# For simplicity, we'll embed a small knowledge base directly.\n",
    "RAG_CONTENT = (\n",
    "    \"The Modern Art & Technology Museum features a groundbreaking exhibit on AI-generated art in the West Gallery, exploring how machine learning reshapes artistic expression.\\n\"\n",
    "    \"Vincent van Gogh\\'s \\'The Starry Night\\' is a masterpiece from 1889, showcasing his unique post-impressionist style. It depicts a dramatic night sky over a tranquil village and is currently on loan in Gallery 4.\\n\"\n",
    "    \"The museum is open from 10 AM to 5 PM, Tuesday through Sunday. Admission is $20 for adults, and free for members. The museum is closed on Mondays.\\n\"\n",
    "    \"Our gift shop, located near the main entrance, offers a wide array of art books, unique souvenirs, and reproductions of famous artworks. It closes 15 minutes before the museum.\\n\"\n",
    "    \"The ancient Egyptian artifacts collection is located on the second floor, featuring sarcophagi, hieroglyphs, and sculptures dating back to 2000 BCE. This exhibit is a permanent display.\\n\"\n",
    "    \"Photography without flash is permitted in all galleries, except where explicitly stated. Tripods are not allowed.\\n\"\n",
    "    \"Large bags and backpacks must be checked at the cloakroom, located on the ground floor next to the information desk. Food and drinks are not allowed inside the galleries.\\n\"\n",
    ")\n",
    "\n",
    "# --- RAG Prompt for the dedicated RAG LLM (prompt stuffing) ---\n",
    "RAG_PROMPT = (\n",
    "    \"You are a helpful assistant designed to answer user questions based solely on the provided knowledge base. If the answer is not found, respond with \\\"I don't know.\\\" Do not guess or make up an answer. Keep your response in 50 words or fewer and avoid introducing your response. Just provide the answer. You must follow all instructions. Use plain, natural language. \\n\\n\"\n",
    "    \"**Knowledge Base:**\\n\"\n",
    "    f\"{RAG_CONTENT}\"\n",
    ")\n",
    "\n",
    "class RAGResponseObserver(BaseObserver):\n",
    "    \"\"\"A special observer to handle responses from the RAG tool and print them.\"\"\"\n",
    "    def __init__(self, target_llm_service: NvidiaLLMService):\n",
    "        super().__init__()\n",
    "        self.target_llm_service = target_llm_service\n",
    "        self.current_tool_call_id = None\n",
    "\n",
    "    async def on_push_frame(self, src: FrameProcessor, dst: FrameProcessor, frame: Frame, direction: FrameDirection, timestamp: int):\n",
    "        if isinstance(frame, TextFrame) and getattr(src, \"_tool_call_id\", None) is not None:\n",
    "            # This assumes TextFrame coming from the RAG LLM carries a tool_call_id\n",
    "            # This is a simplification; in a real pipeline, the tool result would be properly aggregated\n",
    "            # and then passed back to the main LLM as a tool_result message.\n",
    "            logger.info(f\"[Observer] Received RAG TextFrame for tool_call_id: {getattr(src, '_tool_call_id', None)}\")\n",
    "            # Here, we simulate sending the tool_result back to the main LLM\n",
    "            if self.current_tool_call_id:\n",
    "                tool_result_message = {\n",
    "                    \"tool_call_id\": self.current_tool_call_id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": frame.text\n",
    "                }\n",
    "                await self.target_llm_service._context.add_message(tool_result_message) # Directly add tool result\n",
    "                await self.target_llm_service._context.process_context(self.target_llm_service) # Tell LLM to re-process context\n",
    "\n",
    "        elif isinstance(frame, NvidiaRAGCitationsFrame):\n",
    "            logger.info(\"\\n--- Citations (from NvidiaRAGCitationsFrame) ---\")\n",
    "            for i, citation in enumerate(frame.citations):\n",
    "                logger.info(f\"[{i+1}] Document: {citation.document_name}, Score: {citation.score:.2f}\")\n",
    "                logger.info(f\"    Content Snippet: {citation.content.decode('utf-8')[:100]}...\") # Decode and truncate for display\n",
    "            logger.info(\"--------------------------------------------------\")\n",
    "\n",
    "        elif isinstance(frame, EndFrame) and getattr(src, \"_tool_call_id\", None) is not None:\n",
    "            # Tool call ended, clear ID\n",
    "            self.current_tool_call_id = None\n",
    "\n",
    "\n",
    "async def query_knowledge_base_nim(question: str, rag_llm_service: NvidiaLLMService, rag_context: OpenAILLMContext, tool_call_id: str = None):\n",
    "    \"\"\"\n",
    "    Queries the RAG knowledge base (prompt stuffing) using a dedicated RAG LLM (NIM).\n",
    "    This function simulates the retrieval stage by placing the entire RAG_CONTENT into the prompt.\n",
    "    \"\"\"\n",
    "    logger.info(f\"[Tool Function] Querying knowledge base for question: '{question}' (tool_call_id: {tool_call_id})\")\n",
    "\n",
    "    # Prepare the context for the RAG LLM\n",
    "    # The RAG_PROMPT is pre-defined with the RAG_CONTENT\n",
    "    # We'll use a fresh context for the RAG LLM to ensure only the relevant prompt and question are sent.\n",
    "    rag_context.messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    # Set a temporary attribute on the RAG LLM service to pass the tool_call_id\n",
    "    # This is a simplified way to link back the response to the original tool call\n",
    "    rag_llm_service._tool_call_id = tool_call_id\n",
    "\n",
    "    try:\n",
    "        # Send the context to the RAG LLM and stream the response\n",
    "        start_time = time.perf_counter()\n",
    "        stream = await rag_llm_service.get_chat_completions(rag_context, rag_context.get_messages())\n",
    "        \n",
    "        full_rag_response = \"\"\n",
    "        async for chunk in stream:\n",
    "            if chunk.text():\n",
    "                full_rag_response += chunk.text()\n",
    "                # In a full pipeline, this chunk would be processed by the RAG LLM's output handler\n",
    "                # and then formatted as a tool result for the main LLM.\n",
    "                # For this demo, the observer handles direct injection.\n",
    "        end_time = time.perf_counter()\n",
    "        logger.info(f\"[Tool Function] RAG LLM response received in {end_time - start_time:.2f}s: '{full_rag_response}'\")\n",
    "        \n",
    "        # The RAGResponseObserver is expected to pick this up and inject it as a tool result\n",
    "        return full_rag_response # This return value is passed to the next stage if it were a direct tool\n",
    "    except Exception as e:\n",
    "        logger.error(f\"[Tool Function] Error querying knowledge base: {e}\")\n",
    "        return f\"An error occurred while retrieving information: {e}\"\n",
    "\n",
    "\n",
    "async def run_rag_demo():\n",
    "    logger.info(\"\\n--- Starting Simple Prompt Stuffing RAG Demo with NVIDIA NIMs ---\")\n",
    "\n",
    "    # 1. Initialize the main Conversational LLM (Voice Model)\n",
    "    # This LLM will handle the main conversation flow and decide when to call the RAG tool.\n",
    "    # Using a capable instruction-tuned model from NIMs.\n",
    "    main_llm_service = NvidiaLLMService(\n",
    "        model=\"meta/llama-3.1-8b-instruct\", # Main conversational LLM\n",
    "        api_key=api_key,\n",
    "        base_url=None,\n",
    "        temperature=0.7,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    # 2. Initialize the dedicated RAG LLM (RAG Model)\n",
    "    # This LLM will receive the RAG_PROMPT with the knowledge base and answer questions concisely.\n",
    "    rag_llm_service = NvidiaLLMService(\n",
    "        model=\"meta/llama-3.1-8b-instruct\", # Smaller, faster model for RAG queries if available, or same as main\n",
    "        api_key=api_key,\n",
    "        base_url=None,\n",
    "        temperature=0.1, # Keep RAG responses factual and less creative\n",
    "        max_tokens=64 # Limit RAG response length as per prompt instructions\n",
    "    )\n",
    "    # We need a separate context for the RAG LLM, which will be populated by the tool function\n",
    "    rag_context_manager = OpenAILLMContext([])\n",
    "\n",
    "    # 3. Define the tool for the main LLM to call\n",
    "    # The main LLM will see this tool and decide when to use it.\n",
    "    tools = [\n",
    "        {\n",
    "            \"function_declarations\": [\n",
    "                {\n",
    "                    \"name\": \"query_knowledge_base\",\n",
    "                    \"description\": \"Query the museum's knowledge base for information about exhibits, artifacts, hours, or amenities. Use this tool for any factual questions about the museum.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"question\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The specific question to query the knowledge base with.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"question\"],\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 4. System prompt for the main LLM (the Museum Guide persona)\n",
    "    main_llm_system_prompt = (\n",
    "        \"You are a friendly and helpful museum guide for the Modern Art & Technology Museum. \"\n",
    "        \"Your primary goal is to assist visitors with information about exhibits, logistics, and art history. \"\n",
    "        \"You have access to a `query_knowledge_base` tool to find factual answers about the museum. \"\n",
    "        \"Always use the tool for factual questions. Be concise and polite.\" \n",
    "        \"If a factual question can be answered by the knowledge base, use the tool. Otherwise, say you don't know or redirect.\"\n",
    "    )\n",
    "\n",
    "    # 5. Initialize the main LLM's context manager\n",
    "    main_llm_context = OpenAILLMContext(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": main_llm_system_prompt}\n",
    "        ],\n",
    "        tools=tools # Register the tools here\n",
    "    )\n",
    "\n",
    "    # 6. Register the tool function with the main LLM service\n",
    "    # When the main LLM decides to call 'query_knowledge_base', this function will be executed.\n",
    "    main_llm_service.register_function(\n",
    "        \"query_knowledge_base\",\n",
    "        lambda fn_name, tc_id, args, llm_ctx, cb:\n",
    "            query_knowledge_base_nim(args[\"question\"], rag_llm_service, rag_context_manager, tool_call_id=tc_id)\n",
    "    )\n",
    "\n",
    "    # 7. Create the pipeline and task\n",
    "    # This is a simplified pipeline for demonstration; in a real scenario, you'd have STT/TTS/etc.\n",
    "    # The main_llm_service acts as the primary processor in this simplified flow.\n",
    "\n",
    "    # The RAGResponseObserver needs access to the main_llm_service to inject tool results back.\n",
    "    rag_response_observer = RAGResponseObserver(target_llm_service=main_llm_service)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        # Context aggregator to feed user input into the main LLM's context\n",
    "        main_llm_service.create_context_aggregator(main_llm_context).user(),\n",
    "        main_llm_service,\n",
    "        main_llm_service.create_context_aggregator(main_llm_context).assistant()\n",
    "    ])\n",
    "\n",
    "    task = PipelineTask(\n",
    "        pipeline,\n",
    "        params=PipelineParams(observers=[SimpleChatObserver(), rag_response_observer])\n",
    "    )\n",
    "\n",
    "    runner = PipelineRunner()\n",
    "    run_task = asyncio.create_task(runner.run(task))\n",
    "    await asyncio.sleep(0.1) # Give pipeline time to initialize\n",
    "\n",
    "    logger.info(\"\\n--- Ready for museum guide queries! (Type 'exit' to quit) ---\")\n",
    "    logger.info(\"Try asking: 'What is the AI art exhibit?' or 'What are the museum hours?'\")\n",
    "    logger.info(\"Or a question not in the knowledge base: 'What is the capital of France?'\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            logger.info(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        logger.info(\"Assistant: \")\n",
    "        await task.queue_frame(TextFrame(user_input)) # Push user input\n",
    "        await task.queue_frame(EndFrame()) # Signal end of turn\n",
    "\n",
    "        # In a full pipeline, you'd have a more robust mechanism to wait for the LLM's full response\n",
    "        # For this simplified demo, a fixed sleep is used.\n",
    "        await asyncio.sleep(8) \n",
    "\n",
    "    logger.info(\"--- Demo Ended ---\")\n",
    "    await task.cancel()\n",
    "    await run_task\n",
    "\n",
    "# Run the RAG demo\n",
    "await run_rag_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-integration-architecture",
   "metadata": {},
   "source": [
    "## RAG Integration in the `nvidia-pipecat` Architecture (Production-Ready)\n",
    "\n",
    "The `NvidiaRAGService` (part of the ACE Controller SDK's `nvidia-pipecat` library) is designed for seamless integration with NVIDIA's full RAG server deployments, which can be deployed as NVIDIA NIMs or on your own infrastructure. This service simplifies complex RAG workflows into a manageable Pipecat component. [4, 5, 17]\n",
    "\n",
    "The `NvidiaRAGService` itself doesn't host the knowledge base; it communicates with a separate RAG server that handles the heavy lifting of document indexing and retrieval. This RAG server typically follows architectures defined by NVIDIA's Generative AI Examples (like the [NVIDIA RAG Blueprint](https://github.com/NVIDIA-AI-Blueprints/rag)). [5, 17]\n",
    "\n",
    "### Frame Processing Flow (Conceptual for `NvidiaRAGService`)\n",
    "Here's a conceptual flow demonstrating where `NvidiaRAGService` fits into a production-grade digital human pipeline:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"User Query (Voice/Text)\"] --> B[\"ASR (TranscriptionFrame)\"]\n",
    "    B --> C[\"GuardRailProcessor (Keyword/Semantic)\"]\n",
    "    C --> D[\"OpenAILLMContext (Manage History)\"]\n",
    "    D -- User Question --> E[\"NvidiaRAGService (Retrieval/Ranking)\"]\n",
    "    E -- Query to RAG Server --> F[\"RAG Server (Vector DB, Documents)\"]\n",
    "    F --> E -- Retrieved Chunks + Citations --> G[\"LLM (NvidiaLLMService) with Augmented Context\"]\n",
    "    G --> H[\"TTS (TTSSpeakFrame)\"]\n",
    "    H --> I[\"Animation (Avatar Motion)\"]\n",
    "    I --> J[\"Digital Human Response\"]\n",
    "```\n",
    "\n",
    "The `NvidiaRAGService` processes `OpenAILLMContext` objects, extracts the conversation history, and enriches the context with retrieved documents before sending it to the LLM for final generation. This process can also yield `NvidiaRAGCitationsFrame`s for traceability. [6, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep-dive-nvidiaragservice",
   "metadata": {},
   "source": [
    "## Deep Dive: `NvidiaRAGService` (from ACE Controller SDK)\n",
    "\n",
    "The `NvidiaRAGService` class, derived from `OpenAILLMService`, provides the official interface to NVIDIA's RAG server. It abstracts away the complexities of interacting with the RAG backend, allowing you to focus on building your digital human's logic. [1]\n",
    "\n",
    "### Basic Configuration and Parameters\n",
    "\n",
    "When initializing `NvidiaRAGService`, you'll set key parameters that control its interaction with the RAG backend and the LLM: [7]\n",
    "\n",
    "-   **`collection_name`**: A string identifier for your specific document collection within the RAG server (e.g., \"modern_art_museum_collection\"). This tells the RAG server which knowledge base to query.\n",
    "-   **`rag_server_url`**: The HTTP/S endpoint URL of your deployed NVIDIA RAG server (e.g., `\"http://localhost:8081\"` for a local deployment, or a cloud NIM endpoint). [1]\n",
    "-   **`use_knowledge_base`**: A boolean flag (`True`/`False`) to enable or disable RAG retrieval for a given query. This allows you to toggle RAG functionality at runtime.\n",
    "-   **`temperature`, `top_p`, `max_tokens`**: These parameters control the LLM's generation behavior (randomness, diversity, length) *after* the RAG context has been injected. They function identically to the parameters in `NvidiaLLMService`. [21]\n",
    "-   **`vdb_top_k`**: An integer indicating the number of initial document chunks to retrieve from the vector database. A higher value means more documents are considered by the reranker. [2, 11]\n",
    "-   **`reranker_top_k`**: An integer specifying how many top-ranked documents to select after the reranking stage. These are the chunks that will actually be sent to the LLM as context. A lower value here prioritizes precision. [3, 11]\n",
    "-   **`enable_citations`**: A boolean (`True`/`False`) to control whether the RAG service should return detailed source references (`NvidiaRAGCitationsFrame`) along with the LLM's generated response. [9]\n",
    "-   **`suffix_prompt`**: An optional string appended to the last user message before sending it to the RAG server. This can be used for subtle prompt injection specific to RAG queries.\n",
    "-   **`stop_words`**: Words that stop LLM generation. [1]\n",
    "\n",
    "```python\n",
    "# Conceptual instantiation of NvidiaRAGService\n",
    "# (Requires a running NVIDIA RAG server, which is external to this notebook's execution environment)\n",
    "\n",
    "# from nvidia_pipecat.services.nvidia_rag import NvidiaRAGService\n",
    "\n",
    "# rag_service = NvidiaRAGService(\n",
    "#     collection_name=\"my_museum_artifacts\",\n",
    "#     rag_server_url=\"http://your-rag-server-ip:port\", # IMPORTANT: Replace with your actual RAG server URL\n",
    "#     use_knowledge_base=True,\n",
    "#     enable_citations=True,\n",
    "#     vdb_top_k=20,       # Initial retrieval of 20 document chunks\n",
    "#     reranker_top_k=4,   # Select top 4 after re-ranking for LLM context\n",
    "#     temperature=0.2,    # Control LLM creativity\n",
    "#     max_tokens=500,     # Limit LLM response length\n",
    "#     stop_words=[\"thank you\"]\n",
    "# )\n",
    "```\n",
    "\n",
    "### Dynamic Configuration with `NvidiaRAGSettingsFrame`\n",
    "\n",
    "The `NvidiaRAGService` supports runtime configuration changes through the `NvidiaRAGSettingsFrame`. When this frame is pushed into the pipeline, the service's internal settings are updated. This allows you to dynamically adjust parameters‚Äîfor instance, switching between different museum collections (e.g., \"ancient_history\" vs. \"renaissance_art\") or modifying retrieval settings based on the user's current context or explicit preferences. The `_update_settings` method handles these changes internally. [8]\n",
    "\n",
    "```python\n",
    "# Conceptual example of dynamically updating RAG settings in a pipeline\n",
    "# await pipeline_task.queue_frame(NvidiaRAGSettingsFrame({\n",
    "#     \"collection_name\": \"new_exhibit_documents\",\n",
    "#     \"vdb_top_k\": 10, # Adjust retrieval for new context\n",
    "#     \"enable_citations\": False\n",
    "# }))\n",
    "```\n",
    "\n",
    "### Citation Handling (`NvidiaRAGCitationsFrame`)\n",
    "\n",
    "One of RAG's most valuable features for museum applications is its robust citation support. When `enable_citations` is `True`, the `NvidiaRAGService` can return `NvidiaRAGCitationsFrame` objects along with the generated `TextFrame`s. These citation frames contain detailed source information derived from the retrieved documents. [9]\n",
    "\n",
    "As seen in the `SimpleChatObserver` at the beginning of this notebook, each citation typically includes: [9]\n",
    "-   `document_type`: The type of source (e.g., \"Exhibit Label\", \"Research Paper\", \"Artifact Record\").\n",
    "-   `document_id`: A unique identifier for the source document.\n",
    "-   `document_name`: A human-readable name for the source (e.g., \"The Starry Night Exhibit Description\").\n",
    "-   `content`: The exact snippet of text from the source that was used to generate the response.\n",
    "-   `metadata`: Any additional, structured information about the source.\n",
    "-   `score`: A relevance score from the retrieval/ranking process.\n",
    "\n",
    "This allows your museum guide to not only provide accurate information but also direct visitors to specific artifacts, display cases, or additional online resources, enhancing the visitor experience and trustworthiness. [9, 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "museum-domain-use-cases",
   "metadata": {},
   "source": [
    "## Museum Domain Use Cases: RAG in Action\n",
    "\n",
    "Let's explore practical scenarios where RAG transforms a generic digital human into an expert museum guide:\n",
    "\n",
    "### 1. Artifact Information Queries\n",
    "When visitors ask about specific pieces (e.g., \"Tell me about the blue vase in Gallery 7\"), RAG retrieves detailed curatorial descriptions, historical context, acquisition details, and related artifacts from your collection management system. The LLM then synthesizes this information into a natural, conversational response. [13]\n",
    "\n",
    "### 2. Contextual Recommendations\n",
    "A visitor might ask, \"What else should I see if I'm interested in Roman sculpture?\" RAG can access thematic connections across your collection, identifying related artists, periods, or geographical origins, providing personalized tour suggestions that a general LLM would struggle with. [13]\n",
    "\n",
    "### 3. Historical Context\n",
    "RAG enables your guide to draw from extensive historical databases, academic papers, and curatorial research to provide rich contextual information about time periods, artistic movements, or cultural significance that might be too vast or too niche for an LLM's pre-trained knowledge. [13]\n",
    "\n",
    "### 4. Multilingual Support\n",
    "By configuring different document collections (or multilingual embeddings) for different languages, your museum guide can serve international visitors in their preferred language while maintaining factual accuracy across all stored information. [14]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-complements",
   "metadata": {},
   "source": [
    "## How RAG Complements LLM and Guardrails: A Multi-Layered Approach\n",
    "\n",
    "A truly robust digital human system relies on the synergy of multiple AI components. RAG doesn't replace LLMs or guardrails; it enhances them, forming a powerful, multi-layered defense and capability system.\n",
    "\n",
    "### The Three-Layer Defense (Revisited with RAG)\n",
    "\n",
    "1.  **GuardrailProcessor (Keyword-Based)**: Acts as the first line of defense, quickly blocking explicit inappropriate queries before they consume resources or reach more complex systems. [10, 15]\n",
    "2.  **NeMo Guardrails NIMs (Semantic Safety & Topical)**: Provides a deeper, semantic layer of safety and topical control, ensuring queries align with acceptable conversation boundaries. [1, 10, 15]\n",
    "3.  **RAG Knowledge Filtering**: Ensures responses are grounded in authoritative museum content rather than potentially unreliable general knowledge from the LLM, directly addressing the hallucination problem. [16]\n",
    "4.  **LLM Safety**: The underlying language model (e.g., Llama 3.1) still provides inherent safety measures and maintains conversational appropriateness based on its training, acting as a final check. [1]\n",
    "\n",
    "### Accuracy and Trustworthiness\n",
    "\n",
    "RAG fundamentally addresses one of the most critical challenges in deploying AI systems in educational and informational settings: **hallucination**. By grounding responses in your museum's verified content, RAG significantly reduces the risk of providing incorrect historical facts or misattributing artwork. [16]\n",
    "\n",
    "The citation system provides transparency, allowing visitors to verify information and museum staff to trace the source of any responses that need correction, building greater user trust. [9, 16]\n",
    "\n",
    "### Scalability and Maintenance\n",
    "\n",
    "Unlike fine-tuning approaches (which require retraining the entire model for knowledge updates), RAG allows you to update your digital human's knowledge simply by updating the document collection in your vector database. New acquisitions, updated attributions, or revised interpretations can be incorporated without retraining the entire system, making it highly scalable and easier to maintain. [17]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animation-integration",
   "metadata": {},
   "source": [
    "## Animation Integration for Dynamic Digital Humans (Building on Module 3.1)\n",
    "\n",
    "While RAG provides factual depth, a digital human's expressiveness and believability are significantly enhanced by its **animation system**. The NVIDIA ACE Controller SDK provides a comprehensive animation system specifically designed for avatar interactions, allowing for dynamic and context-aware visual responses. [28, 29]\n",
    "\n",
    "How RAG interacts with animation is fascinating: the *retrieved content* can directly influence the avatar's non-verbal communication. For instance:\n",
    "\n",
    "-   If RAG retrieves information about a specific location, the avatar could use **pointing gestures** to direct attention (e.g., \"The sculpture you asked about is in Gallery 3, over there.\"). [30]\n",
    "-   When presenting detailed historical facts retrieved via RAG, the avatar might employ **presentation gestures** to emphasize key points, adding to the informative experience. [31]\n",
    "-   The avatar's overall state (e.g., \"Attentive\" when listening, \"Thinking\" during retrieval, \"Talking\" when responding) enhances the natural interaction, providing visual cues that complement the verbal and factual information. [34]\n",
    "\n",
    "These animated states and gestures are crucial for creating an immersive and natural user experience, making the digital human truly come alive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-considerations",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "The RAG service includes configurable parameters for balancing response quality with latency. The `vdb_top_k` (number of initial documents retrieved) and `reranker_top_k` (number of documents selected after re-ranking) parameters allow you to tune the trade-off between comprehensive knowledge retrieval and response speed. Higher values for `top_k` parameters can lead to more accurate answers but also increased latency. [11]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-exploration",
   "metadata": {},
   "source": [
    "## Assignment: Designing a RAG-Enhanced Digital Human for a Specific Domain\n",
    "\n",
    "This assignment challenges you to design a digital human application that leverages RAG to provide expert, factual information. You'll need to define a specific domain, outline the RAG components, and explain how it integrates into the broader digital human pipeline, considering LLM authoring, guardrails, and animation.\n",
    "\n",
    "### Brief\n",
    "1.  **Select a Domain:** Choose a domain (e.g., a specialized medical assistant, a technical support agent for a complex product, a historical tour guide for a specific landmark, a legal information bot).\n",
    "2.  **Define the Knowledge Base:** What kind of information would your digital human need access to, and how would it be structured for RAG?\n",
    "3.  **Propose the RAG-Enabled Solution:** Describe your digital human and its interaction flow, emphasizing the role of RAG.\n",
    "\n",
    "### Deliverable\n",
    "Write a **400-500 word proposal** covering:\n",
    "\n",
    "1.  **Application and Core Problem (approx. 75 words):**\n",
    "    *   Briefly describe your chosen digital human application and its primary goal within the selected domain.\n",
    "    *   Why is RAG essential for this application, particularly in overcoming LLM limitations like hallucinations or outdated knowledge?\n",
    "\n",
    "2.  **RAG-Enhanced Digital Human Architecture and Capabilities (approx. 300 words):**\n",
    "    *   **Knowledge Base Design:** What specific types of documents or data would constitute your knowledge base (e.g., product manuals, research papers, legal precedents)? How would they be processed for the vector database (e.g., chunking, embedding)?\n",
    "    *   **RAG Workflow:** Describe how a typical user query would flow through your RAG pipeline. Mention the roles of retrieval (e.g., `vdb_top_k`), ranking (e.g., `reranker_top_k`), and context injection into the LLM prompt. How would `NvidiaRAGService` (conceptually) facilitate this?\n",
    "    *   **Complementary AI:**\n",
    "        *   How would this RAG system work alongside **LLM Authoring** (e.g., a specific system prompt for summarization of retrieved content)?\n",
    "        *   How would **Guardrails** (both `GuardrailProcessor` and NeMo Guardrails NIMs) protect against inappropriate queries that might bypass RAG, or filter unsafe content in RAG's retrieved results?\n",
    "        *   How would **Animation** enhance the experience, perhaps by visualizing retrieved data (e.g., pointing to a diagram, using gestures for emphasis when citing sources)?\n",
    "    *   **Citation Strategy:** How would your digital human provide citations to users to build trust and allow for verification?\n",
    "    *   **High-level Data Flow:** Briefly sketch (in text) the order of operations in your Pipecat pipeline, integrating RAG, LLM, Guardrails, and Animation.\n",
    "\n",
    "3.  **Anticipated Benefits and Future Considerations (approx. 75 words):**\n",
    "    *   What are the expected improvements in accuracy, reliability, and user experience for your specific application due to RAG?\n",
    "    *   Briefly discuss one future enhancement for your RAG system (e.g., supporting multimodal document retrieval, incorporating real-time data feeds, or enabling conversational RAG with follow-up questions).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-conclusion",
   "metadata": {},
   "source": [
    "## Next Steps & Conclusion\n",
    "\n",
    "Congratulations! You've now gained a deep understanding of Retrieval-Augmented Generation (RAG) and its crucial role in building knowledgeable and trustworthy digital humans. You've explored its core stages, how it integrates into the `nvidia-pipecat` architecture, and its powerful synergy with LLM authoring, guardrails, and animation.\n",
    "\n",
    "RAG empowers your digital humans to move beyond static knowledge, providing dynamic, factual, and citable responses that significantly enhance their utility and credibility in specialized domains.\n",
    "\n",
    "This module concludes our exploration of the LLM-RAG fundamentals. You now have the conceptual tools to design sophisticated conversational AI systems. Keep experimenting, and apply these powerful techniques to your own innovative digital human projects!\n",
    "\n",
    "**To Prepare:**\n",
    "- Complete the assignment, focusing on a clear, well-reasoned design for your RAG-enabled digital human.\n",
    "- Review the NVIDIA RAG Blueprint documentation (linked in the introduction) to understand the practical aspects of deploying a RAG server.\n",
    "- Reflect on how all the modules so far (Pipecat basics, LLM authoring, guardrails, and RAG) come together to form a complete digital human system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
