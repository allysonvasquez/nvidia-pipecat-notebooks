{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "module-title-welcome",
   "metadata": {},
   "source": [
    "# Module 3.0: Multimodal Digital Humans: Integrating Vision, Multimodal LLMs, and RAG\n",
    "\n",
    "Welcome to Module 3.0 of the Digital Human Teaching Kit! In our previous modules, we established the conceptual framework for digital humans and dove into the core mechanics of the Pipecat framework for real-time streaming, focusing primarily on text and speech interactions. Now, we expand our horizons to truly embody the \"human-like\" aspect of digital characters by introducing **multimodality**.\n",
    "\n",
    "This module will guide you through integrating visual perception into your digital human pipelines, allowing them to \"see\" and interpret images. We will explore how to leverage powerful **Multimodal Large Language Models (MLLMs)** to process both textual and visual information, and then enhance these capabilities with **Retrieval-Augmented Generation (RAG)** to ground responses in external knowledge. By the end of this module, you'll understand how to build more intelligent, context-aware, and impactful digital human applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the importance of multimodal inputs for advanced digital human interaction.\n",
    "- Process image and visual information within a real-time Pipecat pipeline.\n",
    "- Integrate and utilize NVIDIA NIM-powered Multimodal LLMs to interpret combined text and image inputs.\n",
    "- Implement fundamental concepts of Retrieval-Augmented Generation (RAG) to enhance LLM responses with external knowledge.\n",
    "- Design and conceptualize a multimodal, RAG-enabled digital human pipeline.\n",
    "\n",
    "## Prerequisites\n",
    "- Strong Python programming skills.\n",
    "- Familiarity with fundamental AI concepts: LLMs, ASR, TTS, basic computer vision.\n",
    "- Completion of Module 1.0 (Introduction to Digital Humans & NVIDIA ACE) and Module 1.1 (Pipecat Core Concepts & Your First Pipeline).\n",
    "- An active NVIDIA API Key for accessing NVIDIA NIM microservices (as set up in Module 1.1).\n",
    "\n",
    "**Note**: While this module introduces RAG concepts, building a full, production-grade RAG system is a complex topic beyond the scope of this single notebook. Our focus will be on understanding the integration points and demonstrating how RAG fits into the digital human pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multimodality-intro",
   "metadata": {},
   "source": [
    "# The Imperative for Multimodality in Digital Humans\n",
    "\n",
    "Imagine interacting with a human. You don't just listen to their words; you observe their facial expressions, gestures, and what they're looking at. You also reference shared knowledge or external information to fully understand and respond. Similarly, for digital humans to achieve truly natural and intelligent interactions, they must go beyond text and audio to embrace **multimodal inputs**.\n",
    "\n",
    "Multimodality in digital humans allows them to:\n",
    "- **Perceive visual context:** Understand charts, identify objects, interpret scenes, or analyze user expressions and gestures.\n",
    "- **Enhance understanding:** Combine spoken language with visual cues for richer comprehension.\n",
    "- **Provide more relevant responses:** Tailor answers based on what is seen, not just what is heard or read.\n",
    "- **Access external knowledge:** Integrate with databases, documents, or real-time information to provide accurate and specific details (the core of RAG).\n",
    "\n",
    "NVIDIA ACE, with its comprehensive suite of AI microservices, is designed to facilitate these complex multimodal workflows, and `nvidia-pipecat` provides the orchestration layer to bring it all together. [1, 29]\n",
    "\n",
    "## Image and Vision Processing in Pipecat\n",
    "\n",
    "Just as audio is streamed as `AudioFrame`s and text as `TextFrame`s, visual information, such as images, can be represented as `Frame`s within the Pipecat architecture. This allows images to flow through the pipeline and be processed by specialized components. [2, 29]\n",
    "\n",
    "### Representing Images as Frames\n",
    "\n",
    "In Pipecat, an image can be encapsulated within a `Frame` (often a `DataFrame` or a custom `ImageFrame` from `nvidia-pipecat` or `pipecat-ai` libraries) containing the image data (e.g., bytes, a PIL Image object, or a path/URL to an image). This frame can then be pushed into the pipeline, making it available to downstream processors. [2, 38, 43]\n",
    "\n",
    "### Integrating Vision Models\n",
    "\n",
    "To \"understand\" an image, a digital human pipeline integrates **vision models**. These models can perform tasks like object detection, image classification, or more advanced visual question answering. NVIDIA offers several vision-language models (VLMs) as NVIDIA NIM microservices, such as `Nemovision-4B-Instruct` or `llama-3.2-90b-vision-instruct`, which can interpret visual imagery and generate contextually accurate responses. [15, 18, 20, 27]\n",
    "\n",
    "Pipecat provides services that can wrap these vision models, allowing you to incorporate them into your real-time data flow. For example, a dedicated `ImageProcessor` (or a multimodal LLM service capable of handling image inputs) would receive `ImageFrame`s, process them, and potentially output `TextFrame`s containing descriptions or analyses of the image content, or enrich the context for an LLM. [2, 7]\n",
    "\n",
    "**Challenges in Vision Processing:**\n",
    "-   **Data Volume:** Images and video generate significantly more data than text or audio, requiring efficient processing and memory management.\n",
    "-   **Latency:** Real-time visual analysis is critical for responsive interactions, demanding optimized models and hardware.\n",
    "-   **Synchronization:** Combining visual input with simultaneous audio and text streams requires careful temporal alignment.\n",
    "\n",
    "## Multimodal LLM Integration\n",
    "\n",
    "The true power of multimodality comes from **Multimodal Large Language Models (MLLMs)**, also known as Vision-Language Models (VLMs). These models are trained on diverse datasets containing both text and images, enabling them to understand and generate responses based on a combination of these modalities. [13, 16, 25]\n",
    "\n",
    "NVIDIA NIM microservices provide access to powerful MLLMs like `Mistral Small 3.1` (which features enhanced multimodal comprehension) and specialized VLMs like `llama-3.2-90b-vision-instruct`. [10, 12, 13, 16, 17, 21, 22, 25]\n",
    "\n",
    "### Working with NVIDIA Multimodal LLMs via `nvidia-pipecat`\n",
    "\n",
    "The `nvidia-pipecat` library extends Pipecat with services specifically designed to interface with NVIDIA NIMs. The `NvidiaLLMService` (introduced in Module 1.1) is capable of handling not just text-based chat completions but also multimodal inputs when connected to a compatible NVIDIA VLM NIM. [3, 22]\n",
    "\n",
    "When a user interacts with a digital human using both voice and visual input (e.g., showing an image while speaking), the pipeline would flow as follows:\n",
    "\n",
    "1.  **Audio Input:** Captured by an `ASRService` (e.g., NVIDIA Riva ASR) and converted into `TranscriptionFrame`s and then `TextFrame`s. [1, 6, 7]\n",
    "2.  **Image Input:** An `ImageFrame` is generated from a camera feed or uploaded image. [2, 7]\n",
    "3.  **Multimodal Context:** The `TextFrame` and `ImageFrame` are combined and fed into the `OpenAILLMContext` (or similar context manager). This context now maintains a history of both text and visual inputs, preparing it for the MLLM. [1-1-Introduction-ACE-Controller-Pipecat.ipynb, 35, 39, 45, 46]\n",
    "4.  **MLLM Processing:** The `NvidiaLLMService`, configured to use a VLM NIM, receives the multimodal context. It then reasons over both the text and the image to generate a coherent and contextually relevant textual response. [3, 22, 27]\n",
    "\n",
    "This streamlined integration allows for richer conversations where the digital human can refer to elements within an image, answer questions about visual content, or generate responses that acknowledge both spoken words and presented visuals.\n",
    "\n",
    "```python\n",
    "from pipecat.frames.frames import TextFrame, ImageFrame\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "\n",
    "# Assuming 'api_key' is set as in Module 1.1\n",
    "multimodal_llm = NvidiaLLMService(\n",
    "    model=\"meta/llama-3.2-90b-vision-instruct\", # Example VLM NIM\n",
    "    api_key=api_key,\n",
    "    base_url=None\n",
    ")\n",
    "\n",
    "# The context will now store both text and image messages\n",
    "multimodal_context = OpenAILLMContext([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can analyze images.\"}\n",
    "])\n",
    "\n",
    "# Later, in your pipeline, you would add image messages like this:\n",
    "# import base64\n",
    "# with open(\"path/to/your/image.jpg\", \"rb\") as f:\n",
    "#     image_bytes = f.read()\n",
    "# multimodal_context.add_message({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is in this image?\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64.b64encode(image_bytes).decode('utf-8')}\"}}]})\n",
    "\n",
    "# ... and then call the multimodal_llm.get_chat_completions(multimodal_context, ...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Retrieval-Augmented Generation (RAG) Implementation\n",
    "\n",
    "While MLLMs are powerful, they have limitations: they can hallucinate, their knowledge is capped at their training data, and they may lack access to real-time or proprietary information. This is where **Retrieval-Augmented Generation (RAG)** becomes indispensable for digital humans. [6, 11, 19]\n",
    "\n",
    "RAG enhances LLM capabilities by giving them access to an external, up-to-date, and domain-specific knowledge base. Instead of generating responses solely from its internal parameters, a RAG system first *retrieves* relevant information from a knowledge base and then *augments* the LLM's prompt with this retrieved context, leading to more accurate, factual, and relevant responses. [5, 6, 11, 19, 32, 33]\n",
    "\n",
    "### The RAG Workflow in a Digital Human Pipeline\n",
    "\n",
    "1.  **User Query:** The digital human receives a multimodal input (e.g., spoken question + image).\n",
    "2.  **Query Transformation:** The user's query might be re-phrased or processed to generate effective search terms for the knowledge base.\n",
    "3.  **Retrieval:** A retriever component searches a knowledge base (e.g., a vector database containing company documents, product catalogs, or image descriptions) for information relevant to the query. NVIDIA provides `NVIDIA NeMo Retriever` NIM microservices designed for this purpose. [5, 9, 11, 19, 32, 33, 34]\n",
    "4.  **Context Augmentation:** The retrieved snippets of text, image descriptions, or other data are then combined with the original user query and inserted into the prompt for the MLLM. This provides the MLLM with factual, external context.\n",
    "5.  **Generation:** The MLLM uses this augmented prompt to generate a grounded response, drawing on both its inherent knowledge and the provided retrieved information.\n",
    "6.  **Expression:** The generated response is then converted into speech and animation. [6]\n",
    "\n",
    "NVIDIA offers **NIM Agent Blueprints** that streamline the development of RAG-enabled applications, including those for digital humans. These blueprints demonstrate how to connect LLMs with knowledge bases for richer interactions. [1, 9, 13, 23, 31, 34]\n",
    "\n",
    "### Integrating Knowledge Bases & Generating Dynamic Visual Outputs\n",
    "\n",
    "A key aspect of RAG in multimodal digital humans is the ability to retrieve *multimodal* information. For instance, if a user asks about a product, the RAG system might retrieve both textual product details and relevant product images. This retrieved image could then be passed to the digital human's rendering engine (e.g., NVIDIA Omniverse RTX or Unreal Engine) to be displayed alongside the spoken response, creating a dynamic visual output. [3, 4, 6, 8, 13, 17, 24, 26, 28, 30]\n",
    "\n",
    "Pipecat, as an orchestration framework, allows you to insert a RAG processing stage into your pipeline. This could be a custom `FrameProcessor` that interacts with your `NeMo Retriever` or a vector database, or it could be handled by a specialized `RAGService` from `nvidia-pipecat` if one becomes available. [2, 14, 36, 37, 40, 42]\n",
    "\n",
    "![Multimodal Digital Human Pipeline with RAG](../../docs/images/multimodal-rag-pipeline.png)\n",
    "*<p align=\"center\">Conceptual diagram of a Multimodal Digital Human Pipeline incorporating RAG.</p>*\n",
    "\n",
    "| Component               | Responsibility                                                         | Example NVIDIA Technology/Concept       |\n",
    "|-------------------------|------------------------------------------------------------------------|-----------------------------------------|\n",
    "| **Perception Layer**    |                                                                        |                                         |\n",
    "| Image/Video Input       | Captures visual data (e.g., camera feed, uploaded image)               | `ImageFrame` (Pipecat) [2, 38, 43]                  |\n",
    "| **Cognition Layer**     |                                                                        |                                         |\n",
    "| Multimodal LLM          | Interprets combined text & image, generates core response              | NVIDIA NIM (e.g., `llama-3.2-90b-vision-instruct`, `Mistral Small 3.1`) [10, 12, 13, 16, 17, 21, 22, 25] |\n",
    "| RAG Retriever           | Searches external knowledge base for relevant text/image info          | NVIDIA NeMo Retriever NIM [5, 11, 19, 32, 33, 34]       |\n",
    "| Context Augmentation    | Combines retrieved info with user query for LLM prompt                 | `OpenAILLMContext` (extended for RAG) [1-1-Introduction-ACE-Controller-Pipecat.ipynb, 35, 39, 45, 46]   |\n",
    "| **Generation Layer**    |                                                                        |                                         |\n",
    "| Dynamic Visual Output   | Displays retrieved images, charts, or contextual visuals                | NVIDIA Omniverse RTX, Unreal Engine [3, 4, 6, 8, 13, 17, 24, 26, 28, 30] |\n",
    "\n",
    "This integrated approach allows your digital human to not only understand complex multimodal queries but also to respond with factual, rich, and visually supported information, moving closer to truly human-like intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "## Lab: Building a Basic Multimodal Pipeline with NVIDIA Pipecat\n",
    "\n",
    "In this lab, we'll demonstrate a conceptual pipeline that takes a textual prompt and a *placeholder* for an image (as we cannot dynamically load images into a Jupyter notebook for a live demo in the same way as a full application) and passes them to a `NvidiaLLMService` capable of handling multimodal input. You will see how an `ImageFrame` could theoretically flow through the system and be part of the LLM's context.\n",
    "\n",
    "For this example, we'll simulate an `ImageFrame` by encoding a small placeholder image as a Base64 string. In a real application, this `ImageFrame` would come from a camera feed or an uploaded file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-lab-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import os\n",
    "import base64\n",
    "\n",
    "from pipecat.frames.frames import Frame, TextFrame, EndFrame, StartFrame\n",
    "from pipecat.observers.base_observer import BaseObserver\n",
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "from pipecat.pipeline.task import PipelineTask, PipelineParams, FrameDirection\n",
    "from pipecat.processors.aggregators.sentence import SentenceAggregator\n",
    "from pipecat.processors.frame_processor import FrameProcessor\n",
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "from nvidia_pipecat.services.nvidia_llm import NvidiaLLMService\n",
    "\n",
    "nest_asyncio.apply() # For running asyncio in Jupyter\n",
    "\n",
    "# Ensure NVIDIA_API_KEY is set (from Module 1.1 setup)\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "if not api_key or not api_key.startswith(\"nvapi-\"):\n",
    "    raise ValueError(\"NVIDIA API key not found or invalid. Please ensure it's set in your environment or .env file.\")\n",
    "\n",
    "# --- Placeholder for a small transparent image (1x1 pixel PNG) for demonstration ---\n",
    "# In a real scenario, this would be actual image data from a file or camera.\n",
    "PLACEHOLDER_IMAGE_BASE64 = \"iVBORw0KGgoAAAABQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\"\n",
    "\n",
    "class ImageFrame(Frame):\n",
    "    \"\"\"A custom frame to carry image data.\"\"\"\n",
    "    def __init__(self, image_data_base64: str, mime_type: str = \"image/png\"):\n",
    "        super().__init__()\n",
    "        self.image_data_base64 = image_data_base64\n",
    "        self.mime_type = mime_type\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ImageFrame(mime_type='{self.mime_type}', size={len(self.image_data_base64)} bytes)\"\n",
    "\n",
    "class MultimodalLLMProcessor(FrameProcessor):\n",
    "    \"\"\"A processor that sends multimodal (text+image) context to an LLM and streams responses.\"\"\"\n",
    "\n",
    "    def __init__(self, llm_service: NvidiaLLMService, context_manager: OpenAILLMContext, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.llm_service = llm_service\n",
    "        self.context_manager = context_manager\n",
    "        self.full_response_text = \"\"\n",
    "\n",
    "    async def process_frame(self, frame: Frame):\n",
    "        if isinstance(frame, StartFrame):\n",
    "            print(f\"[MultimodalLLMProcessor] Received StartFrame.\")\n",
    "            self.full_response_text = \"\"\n",
    "            yield frame # Pass StartFrame downstream\n",
    "            return\n",
    "\n",
    "        if isinstance(frame, TextFrame):\n",
    "            print(f\"[MultimodalLLMProcessor] Processing TextFrame: {frame.text}\")\n",
    "            self.context_manager.add_message({\"role\": \"user\", \"content\": frame.text})\n",
    "\n",
    "        elif isinstance(frame, ImageFrame):\n",
    "            print(f\"[MultimodalLLMProcessor] Processing ImageFrame: {frame.mime_type}, size: {len(frame.image_data_base64)} bytes.\")\n",
    "            # Add image to LLM context in OpenAI-compatible format\n",
    "            self.context_manager.add_message({\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"(User provided an image for context.)\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{frame.mime_type};base64,{frame.image_data_base64}\"}}\n",
    "            ]})\n",
    "\n",
    "        elif isinstance(frame, EndFrame):\n",
    "            print(f\"[MultimodalLLMProcessor] Received EndFrame. Sending context to LLM...\")\n",
    "            # Trigger LLM completion here after all inputs for this turn are received\n",
    "            stream = await self.llm_service.get_chat_completions(self.context_manager, self.context_manager.get_messages())\n",
    "\n",
    "            print(\"Assistant (streaming): \", end=\"\", flush=True)\n",
    "            llm_response_content = \"\"\n",
    "            async for chunk in stream:\n",
    "                if chunk.text():\n",
    "                    print(chunk.text(), end=\"\", flush=True)\n",
    "                    llm_response_content += chunk.text()\n",
    "                    yield TextFrame(chunk.text()) # Stream LLM chunks downstream\n",
    "            print() # Newline after streaming\n",
    "            self.context_manager.add_message({\"role\": \"assistant\", \"content\": llm_response_content})\n",
    "            yield EndFrame() # Pass EndFrame downstream to signal completion\n",
    "            return\n",
    "\n",
    "        # If a frame is not processed above, yield it to pass it along\n",
    "        yield frame\n",
    "\n",
    "\n",
    "class ResponsePrinter(BaseObserver):\n",
    "    async def on_push_frame(self, src: FrameProcessor, dst: FrameProcessor, frame: Frame, direction: FrameDirection, timestamp: int):\n",
    "        if isinstance(frame, TextFrame) and isinstance(src, MultimodalLLMProcessor):\n",
    "            # This observer specifically prints output from the LLM Processor\n",
    "            pass # Already printed by MultimodalLLMProcessor for streaming effect\n",
    "        elif isinstance(frame, EndFrame) and isinstance(src, MultimodalLLMProcessor):\n",
    "            print(\"[ResponsePrinter] LLM response turn complete.\")\n",
    "\n",
    "async def run_multimodal_pipeline_example():\n",
    "    print(\"\\n--- Running Multimodal LLM Pipeline Example ---\")\n",
    "\n",
    "    llm_service = NvidiaLLMService(\n",
    "        model=\"meta/llama-3.2-90b-vision-instruct\", # Ensure this NIM is available for multimodal\n",
    "        api_key=api_key,\n",
    "        base_url=None\n",
    "    )\n",
    "\n",
    "    # Initialize context with a system message that acknowledges image capability\n",
    "    context_manager = OpenAILLMContext([\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. You can also analyze images if provided. Keep responses concise.\"}\n",
    "    ])\n",
    "\n",
    "    multimodal_processor = MultimodalLLMProcessor(llm_service, context_manager)\n",
    "\n",
    "    pipeline = Pipeline([multimodal_processor])\n",
    "\n",
    "    task = PipelineTask(\n",
    "        pipeline,\n",
    "        params=PipelineParams(observers=[ResponsePrinter()])\n",
    "    )\n",
    "\n",
    "    runner = PipelineRunner()\n",
    "    run_task = asyncio.create_task(runner.run(task))\n",
    "\n",
    "    await asyncio.sleep(0.1)\n",
    "\n",
    "    print(\"\\nSimulating user input with an image and text:\")\n",
    "\n",
    "    # First turn: Send an image and a question about it\n",
    "    await task.queue_frame(ImageFrame(PLACEHOLDER_IMAGE_BASE64, mime_type=\"image/png\"))\n",
    "    await task.queue_frame(TextFrame(\"What do you see in this image, and what is its purpose?\"))\n",
    "    await task.queue_frame(EndFrame()) # Signal end of user turn\n",
    "    await asyncio.sleep(5) # Give LLM time to respond (adjust as needed)\n",
    "\n",
    "    print(\"\\nSimulating next user input (text only, for context):\")\n",
    "\n",
    "    # Second turn: Pure text input, continuing the conversation\n",
    "    await task.queue_frame(TextFrame(\"Can you tell me more about it?\"))\n",
    "    await task.queue_frame(EndFrame()) # Signal end of user turn\n",
    "    await asyncio.sleep(5) # Give LLM time to respond (adjust as needed)\n",
    "\n",
    "    print(\"\\nTerminating pipeline...\")\n",
    "    await task.queue_frame(EndFrame()) # Final EndFrame to gracefully stop the task\n",
    "    await run_task # Wait for the runner to complete\n",
    "    print(\"Pipeline execution finished.\")\n",
    "\n",
    "# Execute the pipeline\n",
    "await run_multimodal_pipeline_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment-planning",
   "metadata": {},
   "source": [
    "## Assignment: Designing a RAG-Enhanced Multimodal Digital Human Application\n",
    "\n",
    "Building upon the concepts of vision processing, multimodal LLMs, and RAG, propose a digital human application that leverages these advanced capabilities. This exercise will help you bridge theoretical knowledge with practical application design.\n",
    "\n",
    "### Brief\n",
    "1.  **Select a Domain:** Choose an industry or scenario (e.g., medical diagnostics, retail customer support, technical assistance, interactive gaming NPC) where multimodal RAG would provide significant value.\n",
    "2.  **Define the Problem:** Clearly articulate a problem or limitation in the current human-computer interaction within this domain that your multimodal, RAG-enabled digital human would solve.\n",
    "3.  **Propose the Solution:** Describe your digital human and its core interaction flow, emphasizing how vision and RAG are integrated.\n",
    "\n",
    "### Deliverable\n",
    "Write a **400-500 word proposal** covering:\n",
    "\n",
    "1.  **Problem and Current Limitations (approx. 100 words):**\n",
    "    *   Identify the chosen domain and the specific challenge. Why are existing solutions (text-only chatbots, static interfaces) insufficient?\n",
    "    *   How does the lack of visual understanding or access to dynamic external knowledge limit the current experience?\n",
    "\n",
    "2.  **Your Multimodal RAG Digital Human (approx. 250 words):**\n",
    "    *   **Persona and Role:** Describe the digital human's persona (e.g., a virtual medical assistant, a smart retail concierge) and its primary function.\n",
    "    *   **Input Modalities:** How will the digital human receive inputs (e.g., voice, images from a user's camera, screen shares)? Provide a concrete example of a multimodal user query.\n",
    "    *   **Pipeline Flow (Simplified):** Sketch a high-level data flow, highlighting where image processing, multimodal LLM inference, and RAG retrieval occur. Mention specific NVIDIA ACE/NIM technologies you would envision using (e.g., `llama-3.2-90b-vision-instruct` for MLLM, `NeMo Retriever` for RAG, `Omniverse RTX` for visual output).\n",
    "    *   **Knowledge Base:** What kind of external knowledge base would be critical for your RAG system (e.g., medical journals, product databases, technical manuals)? How might it include visual data?\n",
    "    *   **Output Expression:** How will the digital human communicate its responses, especially visually (e.g., displaying retrieved images, highlighting parts of a diagram, animating based on retrieved context)?\n",
    "\n",
    "3.  **Anticipated Impact & Metrics (approx. 100 words):**\n",
    "    *   What unique advantages does your multimodal RAG digital human offer over the existing system?\n",
    "    *   How will it improve user experience, efficiency, or accuracy?\n",
    "    *   Suggest 1-2 key metrics you would use to measure its success (e.g., accuracy of responses, task completion rate, user satisfaction score, reduction in human agent interaction).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-conclusion",
   "metadata": {},
   "source": [
    "## Next Steps & Conclusion\n",
    "\n",
    "This module has expanded your understanding of digital humans into the exciting realm of multimodality and knowledge augmentation. You've learned how to conceptualize pipelines that integrate vision, powerful multimodal LLMs from NVIDIA NIMs, and the critical role of RAG in grounding AI responses.\n",
    "\n",
    "The provided lab demonstrated a basic setup for processing multimodal input. In subsequent modules, we will continue to build upon these foundations, exploring more advanced integration patterns, real-time audio-visual synchronization, and deployment considerations for your digital human applications.\n",
    "\n",
    "**To Prepare:**\n",
    "- Review the example pipeline in this notebook and ensure you understand how `ImageFrame`s are handled and how the multimodal context is built.\n",
    "- Start outlining your assignment proposal. Think creatively about how multimodal RAG can truly transform a user experience.\n",
    "- Continue familiarizing yourself with Pipecat and `nvidia-pipecat` documentation to deepen your understanding of their capabilities for complex AI workflows.\n",
    "\n",
    "You are now better equipped to design and build sophisticated digital human interfaces that can perceive, understand, and respond to the world in a richer, more human-like way. Keep exploring and experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
