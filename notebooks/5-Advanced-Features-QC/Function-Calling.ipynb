{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e60317-5e62-4e0c-81bf-780772e37259",
   "metadata": {},
   "source": [
    "# Tool Calling Overview\n",
    "Tool calling (also known as function calling) lets LLMs interact with your code and external systems in real time. Tool calling powers the classic agent loop of: **Observe ‚Üí Decide ‚Üí Act ‚Üí Respond**\n",
    "\n",
    "Instead of just generating text from internal knowledge, the model can:\n",
    "- Fetch up-to-date data (weather, documents, search results)\n",
    "- Take actions (call APIs, run calculations, trigger backend workflows)\n",
    "\n",
    "This is essential for building agents and digital humans that:\n",
    "- Respond accurately using real-world data\n",
    "- Perform tasks through APIs or services\n",
    "- Act in context, not just chat\n",
    "\n",
    "You define available tools using structured JSON, and the model decides when and how to call them. This notebook will explore implementing a simple tool calling agent that you can expand upon in your digital human project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1095a66-66bb-40d8-9390-e2f494963da9",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Prior to getting started, you will need an NVIDIA API Key from the NVIDIA API Catalog to access the models used in this notebook.  \n",
    "\n",
    "Need an API Key? It's Free!\n",
    "  1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**.\n",
    "  2. Select any model, such as `llama-3.3-70b-instruct`.\n",
    "  3. On the right panel above the sample code snippet, click on \"Get API Key\". This will prompt you to log in if you have not already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c68f94-e464-4d95-929a-3185d7eaa265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0de1f-148c-430b-a5cd-ed2b0d924659",
   "metadata": {},
   "source": [
    "## Tool Calling with NIM: A Simple Weather Agent\n",
    "\n",
    "In this walkthrough, we'll build a simple tool-using agent using NVIDIA's NIM endpoint with OpenAI-compatible syntax. Our Digital Human will:\n",
    "1. Process a user query about weather conditions\n",
    "2. Determine whether external weather data is needed\n",
    "3. Call the appropriate weather service API\n",
    "4. Generate a natural language response integrating the retrieved data\n",
    "\n",
    "This demonstration highlights the complete agent loop, showing both the decision-making process and response generation phases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89f9305-9bfc-4099-a554-a3b597b9b5f1",
   "metadata": {},
   "source": [
    "First, let's define a function to retrieve weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac16392-2310-4b28-8da1-70fd9411031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def get_weather(latitude, longitude):\n",
    "    response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\")\n",
    "    data = response.json()\n",
    "    return data['current']['temperature_2m']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d1388-4b37-48fa-a031-d5374b1711c6",
   "metadata": {},
   "source": [
    "Next, we'll initialize our client to connect to the NIM endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3831af-19d3-4a74-8c65-819b534f0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client setup for NIM endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ.get(\"NVIDIA_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042dd46d-b1fa-482f-873a-d2dfa9c977aa",
   "metadata": {},
   "source": [
    "#### üìÑ Function Calling Schema Overview\n",
    "For an agent  to effectively use tools, we need to provide detailed specifications about available functions it can use. This **schema** defines what the function does and what parameters it requires.\n",
    "\n",
    "Functions can be set in the `tools` parameter of an API request. It is made up of the following fields:\n",
    "| Parameter   | Description                                                                                   | Required |\n",
    "|-------------|-----------------------------------------------------------------------------------------------|----------|\n",
    "| `name`      | The name of the function the model may call. Must be a valid function name (a-z, A-Z, 0-9, underscores). | Yes   |\n",
    "| `description` | A short description of what the function does. Helps the model decide when to use it.      | No    |\n",
    "| `parameters` | The parameters the function accepts, written in JSON Schema format.                         | Yes   |\n",
    "| `type`      | Should always be `\"object\"` at the top level of parameters.                                  | Yes   |\n",
    "| `properties` | Defines each input field with a name, description, and type.                                | Yes   |\n",
    "| `required`  | A list of required fields inside properties.                                                 | Yes   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e8d88b3-e68d-4317-acc9-80e5070bae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how we define a weather function:\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"latitude\": {\"type\": \"number\"},\n",
    "                \"longitude\": {\"type\": \"number\"}\n",
    "            },\n",
    "            \"required\": [\"latitude\", \"longitude\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d9935-6c47-422c-8227-528b03a71ab0",
   "metadata": {},
   "source": [
    "Let's process a weather-related query.  \n",
    "First, we'll define our user input, then let the model evaluate the query and decide whether to use our weather tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d133bf2-07ed-4be2-a7ef-f348710a0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input\n",
    "user_question = \"What‚Äôs the weather like in San Francisco?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff508d21-82d6-4fd4-bb7e-875d6173bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Let the model decide whether to call a tool\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.3-70b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": user_question}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "message = response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809be1af-3b5a-437b-9cd8-e178f4884f66",
   "metadata": {},
   "source": [
    "Let's examine the model's decision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "394299a3-a7f7-475d-a279-fa90a874f025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": null,\n",
      "  \"refusal\": null,\n",
      "  \"role\": \"assistant\",\n",
      "  \"audio\": null,\n",
      "  \"function_call\": null,\n",
      "  \"tool_calls\": [\n",
      "    {\n",
      "      \"id\": \"chatcmpl-tool-d41214755ffd4b26b8ea5aa94db133e7\",\n",
      "      \"function\": {\n",
      "        \"arguments\": \"{\\\"latitude\\\": 37.7749, \\\"longitude\\\": -122.4194}\",\n",
      "        \"name\": \"get_weather\"\n",
      "      },\n",
      "      \"type\": \"function\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# lets view the output - we see the model responds with a tool call!\n",
    "print(json.dumps(message.model_dump(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6057db92-b1de-4308-9c14-c4037fb1aaf5",
   "metadata": {},
   "source": [
    "Now we'll execute the weather function with the parameters the model provided above, then send the results back to the LLM for the final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03397054-2266-4013-8b44-9ca3aeccac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in San Francisco is 21.4¬∞C (70.5¬∞F)."
     ]
    }
   ],
   "source": [
    "# Step 2: If the model decides to call a tool, execute it\n",
    "if message.tool_calls:\n",
    "    for call in message.tool_calls:\n",
    "        \n",
    "        if call.function.name == \"get_weather\":\n",
    "            args = json.loads(call.function.arguments)\n",
    "            tool_result = get_weather(args[\"latitude\"], args[\"longitude\"])\n",
    "            \n",
    "            # Step 3: Send tool result back to the model for final response\n",
    "            followup = client.chat.completions.create(\n",
    "                model=\"meta/llama-3.3-70b-instruct\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": user_question},\n",
    "                    message,\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": call.id,\n",
    "                        \"name\": \"get_weather\",\n",
    "                        \"content\": json.dumps(tool_result),\n",
    "                    },\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                top_p=0.7,\n",
    "                max_tokens=1024,\n",
    "                stream=True\n",
    "            )\n",
    "\n",
    "            for chunk in followup:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    print(chunk.choices[0].delta.content, end=\"\")\n",
    "else:\n",
    "    # Fallback: no tool call, just print model response\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b66cb-f32d-47bb-ab37-eeef5d3a6f4c",
   "metadata": {},
   "source": [
    "#### Key Takeaways\n",
    "\n",
    "**Tool Integration**: Digital Humans can seamlessly incorporate external data and services into their responses\n",
    "\n",
    "**Autonomous Decision-Making**: The model independently determines when to use tools based on user queries\n",
    "Complete Reasoning Loop: We've demonstrated the full observe ‚Üí decide ‚Üí act ‚Üí respond cycle\n",
    "Natural Interaction: The final response integrates technical data into a natural, human-like answer\n",
    "\n",
    "This pattern can be extended to create sophisticated Digital Humans that leverage multiple tools and services to provide rich, contextual interactions across diverse domains and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe43e3a-ed31-479d-b4a8-ab0624454a9e",
   "metadata": {},
   "source": [
    "> ‚ÑπÔ∏è **Note: This notebook demonstrates the a direct API approach to tool calling.**  \n",
    "It uses manual API calls to simulate function (tool) calling behavior in a simple, script-based way.\n",
    "\n",
    "We start with this method because it helps illustrate the core concepts behind tool calling without introducing the complexity of full pipelines or real-time components.\n",
    "\n",
    "In the **larger nvidia-pipecat notebook**, you'll see how these same principles are extended using a production-ready, asynchronous pipeline that supports speech, streaming, and multi-modal integration.\n",
    "\n",
    "Here's a comparison of both approaches for context:\n",
    "\n",
    "| **Aspect**         | **Direct API Approach**          | **Pipecat Approach**                      |\n",
    "|--------------------|----------------------------------|-------------------------------------------|\n",
    "| Architecture        | Script-based, synchronous        | Pipeline-based, asynchronous              |\n",
    "| Integration         | Manual API client setup          | Modular component connections             |\n",
    "| Speech Support      | Text-only                        | Full speech I/O (STT and TTS)             |\n",
    "| Streaming           | Basic chunking                   | Full bi-directional streaming             |\n",
    "| Interruptibility    | Limited                          | Built-in support                          |\n",
    "| Scalability         | Limited                          | Designed for complex applications         |\n",
    "| Deployment          | Simple scripts                   | Production-ready architecture             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c88b47-6bee-4924-b638-31bf313073bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv-pipecat-env",
   "language": "python",
   "name": "nv-pipecat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
